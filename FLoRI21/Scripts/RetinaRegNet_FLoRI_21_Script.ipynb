{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a794e5-1426-49e3-9f1e-7a82bb4b2532",
   "metadata": {
    "id": "90a794e5-1426-49e3-9f1e-7a82bb4b2532",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from random import sample\n",
    "from pyunpack import Archive\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.ndimage import map_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371a0b2-22f9-4cd9-8c16-386359eef172",
   "metadata": {
    "id": "d371a0b2-22f9-4cd9-8c16-386359eef172",
    "outputId": "69a46991-f2fb-4d19-dddf-372d5fe4f08d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.modules.utils as nn_utils\n",
    "from torchvision.transforms import PILToTensor\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "from diffusers.models.unet_2d_condition import UNet2DConditionModel\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa36872-e4ce-40bb-bc76-d78dbe79e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Archive('FLoRI21_DataPort.zip').extractall(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08f320-69a8-4a80-931f-adae8d3e5774",
   "metadata": {
    "id": "4c08f320-69a8-4a80-931f-adae8d3e5774"
   },
   "outputs": [],
   "source": [
    "#shutil.rmtree(os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9321b34-4953-4f1d-9900-be2bd396823a",
   "metadata": {
    "id": "b9321b34-4953-4f1d-9900-be2bd396823a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results/Homography_Results')\n",
    "os.makedirs(path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1df1ad-a4fe-4a28-8a27-311c1021dfd8",
   "metadata": {
    "id": "dd1df1ad-a4fe-4a28-8a27-311c1021dfd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results/Polynomial_Results')\n",
    "os.makedirs(path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5cfb19-d74e-4e51-9c51-75c9c937c56f",
   "metadata": {
    "id": "8f5cfb19-d74e-4e51-9c51-75c9c937c56f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results/Homography_Polynomial_Results')\n",
    "os.makedirs(path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8ac95-d175-44ff-b771-f4a64d739580",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Some of the code cells were referenced from the paper titled \"Emergent Correspondence from Image Diffusion.\" Please cite their paper as follows:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{tang2023emergent,\n",
    "  title={Emergent Correspondence from Image Diffusion},\n",
    "  author={Luming Tang and Menglin Jia and Qianqian Wang and Cheng Perng Phoo and Bharath Hariharan},\n",
    "  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n",
    "  year={2023},\n",
    "  url={https://openreview.net/forum?id=ypOiXjdfnU}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39bb2c-86a7-4d50-ae72-f7e5e7204e28",
   "metadata": {
    "id": "fa39bb2c-86a7-4d50-ae72-f7e5e7204e28",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyUNet2DConditionModel(UNet2DConditionModel):\n",
    "    \"\"\"\n",
    "    Customized 2D U-Net conditioned model inherited from `UNet2DConditionModel`.\n",
    "\n",
    "    This model extends the original `UNet2DConditionModel` to incorporate additional conditioning mechanisms\n",
    "    such as encoder hidden states, attention mask, and cross-attention keyword arguments.\n",
    "    \"\"\"\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        up_ft_indices,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        class_labels: Optional[torch.Tensor] = None,\n",
    "        timestep_cond: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Forward method for `MyUNet2DConditionModel`.\n",
    "\n",
    "        Args:\n",
    "            sample (torch.FloatTensor): Noisy inputs tensor with shape (batch, channel, height, width).\n",
    "            timestep (torch.FloatTensor or float or int): Timesteps for each batch.\n",
    "            up_ft_indices (list): List of upsampling indices.\n",
    "            encoder_hidden_states (torch.FloatTensor): Encoder hidden states with shape (batch, sequence_length, feature_dim).\n",
    "            class_labels (Optional[torch.Tensor], default=None): Class labels tensor.\n",
    "            timestep_cond (Optional[torch.Tensor], default=None): Timestep condition tensor.\n",
    "            attention_mask (Optional[torch.Tensor], default=None): Mask to avoid attention to certain positions.\n",
    "            cross_attention_kwargs (Optional[dict], default=None): Keyword arguments passed along to the `AttnProcessor`.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing upsampled features (`up_ft`).\n",
    "        \"\"\"\n",
    "\n",
    "        # By default samples have to be AT least a multiple of the overall upsampling factor.\n",
    "        # The overall upsampling factor is equal to 2 ** (# num of upsampling layears).\n",
    "        # However, the upsampling interpolation output size can be forced to fit any upsampling size\n",
    "        # on the fly if necessary.\n",
    "        default_overall_up_factor = 2**self.num_upsamplers\n",
    "\n",
    "        # upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`\n",
    "        forward_upsample_size = False\n",
    "        upsample_size = None\n",
    "\n",
    "        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n",
    "            # logger.info(\"Forward upsample size to force interpolation output size.\")\n",
    "            forward_upsample_size = True\n",
    "\n",
    "        # prepare attention_mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1 - attention_mask.to(sample.dtype)) * -10000.0\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb, timestep_cond)\n",
    "\n",
    "        if self.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\"class_labels should be provided when num_class_embeds > 0\")\n",
    "\n",
    "            if self.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "            emb = emb + class_emb\n",
    "\n",
    "        # 2. pre-process\n",
    "        sample = self.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(\n",
    "                sample,\n",
    "                emb,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                cross_attention_kwargs=cross_attention_kwargs,\n",
    "            )\n",
    "\n",
    "        # 5. up\n",
    "        up_ft = {}\n",
    "        for i, upsample_block in enumerate(self.up_blocks):\n",
    "\n",
    "            if i > np.max(up_ft_indices):\n",
    "                break\n",
    "\n",
    "            is_final_block = i == len(self.up_blocks) - 1\n",
    "\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            # if we have not reached the final block and need to forward the\n",
    "            # upsample size, we do it here\n",
    "            if not is_final_block and forward_upsample_size:\n",
    "                upsample_size = down_block_res_samples[-1].shape[2:]\n",
    "\n",
    "            if hasattr(upsample_block, \"has_cross_attention\") and upsample_block.has_cross_attention:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    res_hidden_states_tuple=res_samples,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    upsample_size=upsample_size,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=upsample_size\n",
    "                )\n",
    "\n",
    "            if i in up_ft_indices:\n",
    "                up_ft[i] = sample.detach()\n",
    "\n",
    "        output = {}\n",
    "        output['up_ft'] = up_ft\n",
    "        return output\n",
    "\n",
    "class OneStepSDPipeline(StableDiffusionPipeline):\n",
    "    \"\"\"\n",
    "    One-step Stable Diffusion Pipeline.\n",
    "\n",
    "    Provides a one-step stable diffusion process, integrating the VAE encoding and U-Net based sampling.\n",
    "    \"\"\"\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        img_tensor,\n",
    "        t,\n",
    "        up_ft_indices,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Call method for `OneStepSDPipeline`.\n",
    "\n",
    "        Args:\n",
    "            img_tensor (torch.Tensor): Image tensor.\n",
    "            t (torch.Tensor or int): Timesteps tensor.\n",
    "            up_ft_indices (list): List of upsampling indices.\n",
    "            negative_prompt (Optional[str or list], default=None): Negative prompts.\n",
    "            generator (Optional[torch.Generator or list], default=None): Torch generator for random sampling.\n",
    "            prompt_embeds (Optional[torch.FloatTensor], default=None): Precomputed prompt embeddings.\n",
    "            callback (Optional[Callable], default=None): Callback function invoked during diffusion.\n",
    "            callback_steps (int, default=1): Frequency of invoking the callback.\n",
    "            cross_attention_kwargs (Optional[dict], default=None): Keyword arguments for cross-attention.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing output from U-Net.\n",
    "        \"\"\"\n",
    "        device = self._execution_device\n",
    "        latents = self.vae.encode(img_tensor).latent_dist.sample() * self.vae.config.scaling_factor\n",
    "        t = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        noise = torch.randn_like(latents).to(device)\n",
    "        latents_noisy = self.scheduler.add_noise(latents, noise, t)\n",
    "        unet_output = self.unet(latents_noisy,\n",
    "                               t,\n",
    "                               up_ft_indices,\n",
    "                               encoder_hidden_states=prompt_embeds,\n",
    "                               cross_attention_kwargs=cross_attention_kwargs)\n",
    "        return unet_output\n",
    "\n",
    "\n",
    "class SDFeaturizer:\n",
    "    \"\"\"\n",
    "    Stable Diffusion Featurizer.\n",
    "\n",
    "    Provides a mechanism to compute stable diffusion based features from an input image, conditioned on a given prompt.\n",
    "    \"\"\"\n",
    "    def __init__(self, sd_id='stabilityai/stable-diffusion-2-1'):\n",
    "        \"\"\"\n",
    "        Initializes `SDFeaturizer` with a given stable diffusion model ID.\n",
    "\n",
    "        Args:\n",
    "            sd_id (str, default='stabilityai/stable-diffusion-2-1'): Stable diffusion model ID to be used for featurization.\n",
    "        \"\"\"\n",
    "        unet = MyUNet2DConditionModel.from_pretrained(sd_id, subfolder=\"unet\")\n",
    "        onestep_pipe = OneStepSDPipeline.from_pretrained(sd_id, unet=unet, safety_checker=None)\n",
    "        onestep_pipe.vae.decoder = None\n",
    "        onestep_pipe.scheduler = DDIMScheduler.from_pretrained(sd_id, subfolder=\"scheduler\")\n",
    "        gc.collect()\n",
    "        onestep_pipe = onestep_pipe.to(\"cuda\")\n",
    "        onestep_pipe.enable_attention_slicing()\n",
    "        onestep_pipe.enable_xformers_memory_efficient_attention()\n",
    "        self.pipe = onestep_pipe\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self,\n",
    "                img_tensor, # single image, [1,c,h,w]\n",
    "                t,\n",
    "                up_ft_index,\n",
    "                prompt,\n",
    "                ensemble_size=8):\n",
    "        \"\"\"\n",
    "        Forward method for `SDFeaturizer`.\n",
    "\n",
    "        Args:\n",
    "            img_tensor (torch.Tensor): Single input image tensor with shape [1, c, h, w].\n",
    "            t (torch.Tensor or int): Timesteps tensor.\n",
    "            up_ft_index (int): Index for upsampling.\n",
    "            prompt (str): Textual prompt for conditioning.\n",
    "            ensemble_size (int, default=8): Size of the ensemble for feature averaging.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Stable diffusion based features with shape [1, c, h, w].\n",
    "        \"\"\"\n",
    "        img_tensor = img_tensor.repeat(ensemble_size, 1, 1, 1).cuda() # ensem, c, h, w\n",
    "        prompt_embeds = self.pipe._encode_prompt(\n",
    "            prompt=prompt,\n",
    "            device='cuda',\n",
    "            num_images_per_prompt=1,\n",
    "            do_classifier_free_guidance=False) # [1, 77, dim]\n",
    "        prompt_embeds = prompt_embeds.repeat(ensemble_size, 1, 1)\n",
    "        unet_ft_all = self.pipe(\n",
    "            img_tensor=img_tensor,\n",
    "            t=t,\n",
    "            up_ft_indices=[up_ft_index],\n",
    "            prompt_embeds=prompt_embeds)\n",
    "        unet_ft = unet_ft_all['up_ft'][up_ft_index] # ensem, c, h, w\n",
    "        unet_ft = unet_ft.mean(0, keepdim=True) # 1,c,h,w\n",
    "        return unet_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b7f17-8778-42a7-8bdd-25a92228b5be",
   "metadata": {
    "id": "914b7f17-8778-42a7-8bdd-25a92228b5be",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DFT:\n",
    "    \"\"\"\n",
    "    RetinaRegNet (RetinaRegNetwork) utilizes DFT (Diffusion Features) for identifying vital key feature correlations\n",
    "    and locations between images.\n",
    "    \"\"\"\n",
    "    def __init__(self, imgs,img_size,pts):\n",
    "        \"\"\"\n",
    "        Initialize the DFT object.\n",
    "\n",
    "        Parameters:\n",
    "        - imgs (list): List of input image tensors.\n",
    "        - img_size (int): Expected size of the image for processing.\n",
    "        - pts (list): List of point tuples specifying coordinates.\n",
    "        \"\"\"\n",
    "        self.pts = pts\n",
    "        self.imgs = imgs\n",
    "        self.num_imgs = len(imgs)\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def unravel_index(self,index, shape):\n",
    "        \"\"\"\n",
    "        Converts a flat index into a tuple of coordinate indices in a tensor of the specified shape.\n",
    "\n",
    "        This function mimics numpy's `unravel_index` functionality, which is used to convert a flat index\n",
    "        into a tuple of coordinate indices for an array of given shape. This is useful for finding the original\n",
    "        multi-dimensional indices of a position in a flattened array.\n",
    "\n",
    "        Parameters:\n",
    "            index (int): The flat index into the array.\n",
    "            shape (tuple of ints): The shape of the array from which the index is derived.\n",
    "\n",
    "        Returns:\n",
    "            tuple of ints: A tuple representing the coordinates of the index in an array of the specified shape.\n",
    "\n",
    "        Example:\n",
    "            >>> obj = MyClass()\n",
    "            >>> obj.unravel_index(22, (5, 5))\n",
    "            (4, 2)\n",
    "            >>> obj.unravel_index(52, (7, 8))\n",
    "            (6, 4)\n",
    "\n",
    "        Note:\n",
    "            This function operates under the assumption that indexing starts from 0, which is standard in Python.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for dim in reversed(shape):\n",
    "            out.append(index % dim)\n",
    "            index = index // dim\n",
    "        return tuple(reversed(out))\n",
    "\n",
    "    def compute_pooled_and_combining_feature_maps(self,feature_map, hierarchy_range=1, stride=1):\n",
    "        \"\"\"\n",
    "        Compute pooled and stacked feature maps.\n",
    "\n",
    "        Parameters:\n",
    "        - feature_map (torch.Tensor): Input feature map.\n",
    "        - hierarchy_range (int, optional): Depth of hierarchical pooling. Defaults to 3.\n",
    "        - stride (int, optional): Stride for pooling. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Pooled and stacked feature map.\n",
    "        \"\"\"\n",
    "        # List to store the pooled feature maps\n",
    "        pooled_feature_maps = feature_map\n",
    "        # Loop through the specified hierarchy range\n",
    "        for hierarchy in range(1,hierarchy_range):\n",
    "            # Average pooling with kernel size 3^k x 3^k\n",
    "            win_size = 3 ** hierarchy\n",
    "            avg_pool = torch.nn.AvgPool2d(win_size, stride=1, padding=win_size // 2, count_include_pad=False)\n",
    "            pooled_map = avg_pool(feature_map)\n",
    "            # Append the pooled feature map to the list\n",
    "            pooled_feature_maps=+pooled_map\n",
    "        return pooled_feature_maps\n",
    "\n",
    "\n",
    "    def compute_batched_2d_correlation_maps(self, pts_list, feature_map1, feature_map2):\n",
    "        \"\"\"\n",
    "        Computes 2D correlation maps between selected points in one feature map and another feature map.\n",
    "\n",
    "        This method takes two feature maps and a list of points. It extracts features from the first feature map\n",
    "        at specified points, normalizes them, and then computes a batched 2D correlation with the second feature map.\n",
    "        The output is a set of correlation maps, each corresponding to a point in `pts_list`, showing how that point's\n",
    "        feature vector correlates across the spatial dimensions of the second feature map.\n",
    "\n",
    "        Parameters:\n",
    "            pts_list (list of tuples): List of points (y, x) for which the correlation map is to be computed.\n",
    "            feature_map1 (torch.Tensor): The first feature map tensor of shape (1, C, H1, W1) where C is the number of channels.\n",
    "            feature_map2 (torch.Tensor): The second feature map tensor of shape (1, C, H2, W2) where C is the number of channels\n",
    "                                         and H2, W2 do not necessarily need to be equal to H1, W1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (NumPoints, H2, W2) where each slice corresponds to the correlation map\n",
    "                          for each point in `pts_list`.\n",
    "        Notes:\n",
    "            The function assumes that the first dimension of feature_map1 and feature_map2 is 1 (batch size of 1).\n",
    "            This method uses batch matrix multiplication and vector normalization for efficient computation.\n",
    "            Running this method on a GPU is recommended due to its computational and memory intensity.\n",
    "        \"\"\"\n",
    "        # Convert the input tensors to float16\n",
    "        feature_map1 = feature_map1.to(dtype=torch.float16)\n",
    "        feature_map2 = feature_map2.to(dtype=torch.float16)\n",
    "        _, C, H, W = feature_map2.shape\n",
    "\n",
    "        # Flatten feature_map2 for batch matrix multiplication\n",
    "        feature_map2_flat = feature_map2.view(C, H*W)\n",
    "\n",
    "        # Prepare a batch of point features\n",
    "        points_indices = torch.tensor(pts_list)\n",
    "        point_features = feature_map1[0, :, points_indices[:, 0], points_indices[:, 1]].transpose(0, 1)  # Shape: (NumPoints, Channels)  # Shape: (NumPoints, Channels)\n",
    "\n",
    "        # Normalize the point features and feature_map2_flat\n",
    "        point_features_norm = torch.norm(point_features, dim=1, keepdim=True)\n",
    "        normalized_point_features = point_features / point_features_norm\n",
    "\n",
    "        feature_map2_norm = torch.norm(feature_map2_flat, dim=0, keepdim=True)\n",
    "        normalized_feature_map2 = feature_map2_flat / feature_map2_norm\n",
    "\n",
    "        # Compute the correlation map for each point\n",
    "        correlation_maps = torch.mm(normalized_point_features, normalized_feature_map2)\n",
    "\n",
    "        # Reshape the correlation maps to the desired output shape (NumPoints, H, W)\n",
    "        correlation_maps = correlation_maps.view(-1, H, W)\n",
    "\n",
    "        # Cleanup if needed\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return correlation_maps\n",
    "\n",
    "\n",
    "\n",
    "    def compute_correlation_map_max_locations(self, pts_list, feature_map1, feature_map2): # heirachy range - hpo\n",
    "        \"\"\"\n",
    "        Compute the maximum locations in the batched correlation maps between two feature maps.\n",
    "\n",
    "        Parameters:\n",
    "        - pts_list (list of tuples): List of points for which the correlation maps were computed.\n",
    "        - feature_map1, feature_map2 (torch.Tensor): The input feature maps.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tensor of maximum locations for each point.\n",
    "        - torch.Tensor: Tensor of maximum values for each point.\n",
    "        \"\"\"\n",
    "        enhanced_feature_map1 = self.compute_pooled_and_combining_feature_maps(feature_map1, hierarchy_range=1)\n",
    "        enhanced_feature_map2 = self.compute_pooled_and_combining_feature_maps(feature_map2, hierarchy_range=1)\n",
    "        # Compute the batched correlation maps\n",
    "        batched_correlation_maps = self.compute_batched_2d_correlation_maps(pts_list, enhanced_feature_map1, enhanced_feature_map2)\n",
    "\n",
    "        M,H2, W2 = batched_correlation_maps.shape\n",
    "        #print(batched_correlation_maps.shape)\n",
    "\n",
    "        # Find the maximum values and their locations along the last two dimensions for each map\n",
    "        max_values, max_indices_flat = torch.max(batched_correlation_maps.view(len(pts_list), -1), dim=-1)\n",
    "\n",
    "        x, y = zip(*[self.unravel_index(idx.item(), (H2, W2)) for idx in max_indices_flat.view(-1)])\n",
    "        x = torch.tensor(x, device = 'cuda').view(M)\n",
    "        y = torch.tensor(y, device = 'cuda').view(M)\n",
    "\n",
    "        # Stack the coordinates to get a 2xHxW tensor\n",
    "        max_locations = torch.stack((x, y)).t()\n",
    "\n",
    "        return max_locations, max_values\n",
    "\n",
    "    def feature_upsampling(self,ft):\n",
    "        \"\"\"\n",
    "        Upsample the feature to match the specified image size.\n",
    "\n",
    "        Parameters:\n",
    "        - ft (torch.Tensor): Feature tensor to be upsampled.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Upsampled source and target feature maps.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            num_channel = ft.size(1)\n",
    "            src_ft = ft[0].unsqueeze(0)\n",
    "            src_ft = nn.Upsample(size=(self.img_size, self.img_size), mode='bilinear')(src_ft)  # (1, C, H, W)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            trg_ft = nn.Upsample(size=(self.img_size, self.img_size), mode='bilinear')(ft[1:])  # (1, C, H, W)\n",
    "        return src_ft,trg_ft\n",
    "\n",
    "\n",
    "    def feature_maps(self,feature_map1,feature_map2,iccl):\n",
    "        \"\"\"\n",
    "        Processes feature maps to extract points that meet the inverse consistency criteria between two images.\n",
    "\n",
    "        This method computes the maximum locations of correlation between feature maps of two images and \n",
    "        checks for inverse consistency between the mapped points. It filters these points based on the \n",
    "        specified inverse consistency criteria limit (iccl), keeping only those pairs where the \n",
    "        distance between the original point and its double-mapped location is within the threshold.\n",
    "\n",
    "        Parameters:\n",
    "            feature_map1 (torch.Tensor): The first feature map, used as the base for initial correlations.\n",
    "            feature_map2 (torch.Tensor): The second feature map, used for reverse correlations to check consistency.\n",
    "            iccl (float): The maximum allowed distance (inverse consistency criteria limit) for a point and \n",
    "                          its double-mapped location to be considered consistent.\n",
    "\n",
    "        Returns:\n",
    "            tuple of (list, list, list):\n",
    "            - pnts (list of tuples): The points from the original feature map that meet the inverse consistency criteria.\n",
    "            - rmaxs (list of floats): The maximum correlation values at these points.\n",
    "            - rspts (list of tuples): The corresponding points in the second feature map that have the highest correlation \n",
    "                                      with the points in `pnts`.\n",
    "        \"\"\"\n",
    "        pnts,rmaxs,rspts=[],[],[]\n",
    "        pts = [(int(y), int(x)) for x, y in self.pts]\n",
    "        max_indices_ST, max_values_ST = self.compute_correlation_map_max_locations(pts,feature_map1,feature_map2)\n",
    "        x_prime_y_prime = max_indices_ST\n",
    "        max_indices_TS, max_values_TS = self.compute_correlation_map_max_locations(max_indices_ST,feature_map2,feature_map1)\n",
    "        x_prime_prime_y_prime_prime = max_indices_TS\n",
    "        for i, (pt, max_idx) in enumerate(zip(self.pts, x_prime_prime_y_prime_prime)):\n",
    "            # Calculate the distance between the point and the max correlation index\n",
    "            if np.sqrt((pt[1] - max_idx.cpu()[0]) ** 2 + (pt[0] - max_idx.cpu()[1]) ** 2) <=iccl: ### inverse consistency criteria\n",
    "                pnts.append((int(pt[0]), int(pt[1])))\n",
    "                rmaxs.append(max_values_ST[i].cpu().item())  # Assuming max_values_ST is a tensor with corresponding max values\n",
    "                rspts.append((x_prime_y_prime[i][1].cpu().item(), x_prime_y_prime[i][0].cpu().item()))  # Assuming x_prime_y_prime has corresponding max index locations\n",
    "        return pnts, rmaxs, rspts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189cfab-28f4-4c99-a98b-2df7e8c82f0a",
   "metadata": {
    "id": "4189cfab-28f4-4c99-a98b-2df7e8c82f0a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_boundary(image, mean_intensity):\n",
    "    \"\"\"\n",
    "    Compute the boundary of an image based on its mean intensity.\n",
    "\n",
    "    Parameters:\n",
    "    - image (numpy.array): The input grayscale image.\n",
    "    - mean_intensity (float): Average intensity of the image to define boundaries.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: upper, lower, left, and right boundaries of the image region with intensities above mean_intensity.\n",
    "    \"\"\"\n",
    "    # Compute the upper, lower, left, and right boundary\n",
    "    upper_boundary = next((i for i, row in enumerate(image) if np.mean(row) > mean_intensity), 0)\n",
    "    lower_boundary = next((i for i, row in enumerate(image[::-1]) if np.mean(row) > mean_intensity), 0)\n",
    "\n",
    "    left_boundary = next((i for i, col in enumerate(image.T) if np.mean(col) > mean_intensity), 0)\n",
    "    right_boundary = next((i for i, col in enumerate(image.T[::-1]) if np.mean(col) > mean_intensity), 0)\n",
    "\n",
    "    return upper_boundary, image.shape[0]-lower_boundary, left_boundary, image.shape[1]-right_boundary\n",
    "\n",
    "def is_within_boundary(kp, boundaries):\n",
    "    \"\"\"\n",
    "    Check if a keypoint is within the specified boundaries.\n",
    "\n",
    "    Parameters:\n",
    "    - kp (cv2.KeyPoint): The keypoint to check.\n",
    "    - boundaries (tuple): Tuple of (upper, lower, left, right) boundaries.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the keypoint is within the boundaries, False otherwise.\n",
    "    \"\"\"\n",
    "    upper, lower, left, right = boundaries\n",
    "    return left <= kp.pt[0] <= right and upper <= kp.pt[1] <= lower\n",
    "\n",
    "def SIFT_top_n_keypoints(image_path, N=250, img_shape=256, max_dist=25):\n",
    "    \"\"\"\n",
    "    Detect top N keypoints in the given image using SIFT, considering constraints on distance, boundary, and collinearity.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - N (int): Number of keypoints to select. Defaults to 250.\n",
    "    - img_shape (int): The size to which the image should be resized. Defaults to 256.\n",
    "    - max_dist (int): Minimum distance between selected keypoints. Defaults to 25.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of selected keypoints (cv2.KeyPoint objects).\n",
    "    - list: List of keypoints' positions in the form (x, y).\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image1 = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image1 = cv2.resize(image1, (img_shape, img_shape))\n",
    "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(8, 8))\n",
    "    image = clahe.apply(image1)\n",
    "\n",
    "    # Initialize SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "\n",
    "    # Sort keypoints based on response (strength of the keypoint)\n",
    "    keypoints = sorted(keypoints, key=lambda x: -x.response)\n",
    "\n",
    "    # Determine the intensity threshold\n",
    "    mean_intensity = np.mean(image)\n",
    "    boundaries = compute_boundary(image, mean_intensity)\n",
    "\n",
    "    # Select top N keypoints\n",
    "    selected_keypoints = []\n",
    "    for keypoint in keypoints:\n",
    "        # Check if the keypoint is within the boundary\n",
    "        if is_within_boundary(keypoint, boundaries):\n",
    "            # Check if the pixel intensity at the keypoint is greater than the threshold (not black)\n",
    "            if image[int(keypoint.pt[1]), int(keypoint.pt[0])] > mean_intensity:\n",
    "                # Check if the keypoint is far from existing selected keypoints\n",
    "                if all(cv2.norm(np.array(keypoint.pt) - np.array(kp.pt)) > max_dist for kp in selected_keypoints):\n",
    "                    selected_keypoints.append(keypoint)\n",
    "\n",
    "            # Break if N keypoints are selected\n",
    "            if len(selected_keypoints) == N:\n",
    "                break\n",
    "\n",
    "    # Draw keypoints on the color image\n",
    "    image_with_keypoints = cv2.drawKeypoints(image1, selected_keypoints, None)\n",
    "    return selected_keypoints, [kp.pt for kp in selected_keypoints]\n",
    "\n",
    "def select_random_points(img, num_points=100, img_size=1200,offset=0.01,window_size = 51,max_attempts_per_point=50):\n",
    "    \"\"\"\n",
    "    Selects a specified number of random points from an image, ensuring that each point is centered in a region \n",
    "    meeting a defined intensity threshold within the image. The image is resized to a specified size, and points \n",
    "    are chosen randomly, with each potential point undergoing validation against criteria before being accepted.\n",
    "\n",
    "    Parameters:\n",
    "        img (str): Path to the image file.\n",
    "        num_points (int, optional): The number of random points to select. Defaults to 100.\n",
    "        img_size (int, optional): The size to which the image is resized (assumed square). Defaults to 1200.\n",
    "        offset (float, optional): Proportional offset to exclude points near the edges, represented as a fraction of \n",
    "                                  the image dimensions. Defaults to 0.01.\n",
    "        window_size (int, optional): Size of the square window used to check pixel intensity around each point. \n",
    "                                     Defaults to 51.\n",
    "        max_attempts_per_point (int, optional): The maximum number of attempts allowed to find a suitable point \n",
    "                                                that meets the criteria. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: A list where each tuple represents the (y, x) coordinates of a selected point.\n",
    "\n",
    "    Notes:\n",
    "        The function converts the image to grayscale and resizes it to img_size x img_size. It avoids selecting \n",
    "        points near the image boundary by applying a boundary offset calculated from the 'offset' parameter. \n",
    "        Each point must be centered in a window (defined by 'window_size') where all pixels have an intensity \n",
    "        greater than or equal to 5. If the function fails to find a suitable point after 'max_attempts_per_point' \n",
    "        for any location, it stops and returns the points found up to that moment.\n",
    "    \"\"\"\n",
    "\n",
    "    image = cv2.resize(cv2.imread(img, cv2.IMREAD_GRAYSCALE), (img_size, img_size))\n",
    "    h, w = image.shape\n",
    "    boundary_offset = int(offset * h)\n",
    "    pts = []\n",
    "    window_offset = window_size // 2  # Calculate the offset from the center of the window\n",
    "\n",
    "    while len(pts) < num_points:\n",
    "        attempts = 0\n",
    "        while attempts < max_attempts_per_point:\n",
    "            x = random.randint(boundary_offset + window_offset, h - boundary_offset - window_offset - 1)\n",
    "            y = random.randint(boundary_offset + window_offset, w - boundary_offset - window_offset - 1)\n",
    "\n",
    "            # Define the window boundaries\n",
    "            x_lower = x - window_offset\n",
    "            x_upper = x + window_offset + 1\n",
    "            y_lower = y - window_offset\n",
    "            y_upper = y + window_offset + 1\n",
    "\n",
    "            # Check that no pixel in the window has an intensity less than 10\n",
    "            if np.all(image[x_lower:x_upper, y_lower:y_upper] >= 5):\n",
    "                pts.append((y, x))\n",
    "                break  # Successfully found a point, break the inner loop\n",
    "            attempts += 1  # Increment attempts\n",
    "\n",
    "        if attempts == max_attempts_per_point:\n",
    "            print(\"Maximum attempts reached, unable to find sufficient points with the specified criteria.\")\n",
    "            break  # Break outer loop if max attempts is reached without finding a point\n",
    "\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8mwdIHS8RIZY",
   "metadata": {
    "id": "8mwdIHS8RIZY"
   },
   "outputs": [],
   "source": [
    "def clahe(imag, clip):\n",
    "    \"\"\"\n",
    "    Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) to an image.\n",
    "\n",
    "    This function converts an image to grayscale, applies CLAHE to enhance the image contrast,\n",
    "    and then converts it back to RGB. It uses OpenCV for the CLAHE operation and PIL for image\n",
    "    conversions.\n",
    "\n",
    "    Parameters:\n",
    "        imag (np.array): The input image array. Expected to be in format suitable for OpenCV.\n",
    "        clip (float): The clipping limit for the CLAHE algorithm, which controls the contrast limit.\n",
    "                      Higher values increase contrast.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The contrast-enhanced image in RGB format.\n",
    "\n",
    "    Notes:\n",
    "        - The tile grid size for CLAHE is set to (8, 8). Adjustments to this parameter may affect\n",
    "          the granularity of the histogram equalization.\n",
    "    \"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(8, 8))\n",
    "    imag = Image.fromarray(np.uint8(imag))\n",
    "    imag = imag.convert('L')\n",
    "    img = np.asarray(imag)\n",
    "    image_equalized = clahe.apply(img)\n",
    "    image_equalized_img = Image.fromarray(np.uint8(image_equalized))\n",
    "    image_equalized = image_equalized_img.convert('RGB')\n",
    "    image_equalized = np.asarray(image_equalized)\n",
    "    return image_equalized\n",
    "\n",
    "def compute_plot_Flori21_AUC(landmark_errors):\n",
    "    \"\"\"\n",
    "    Function to compute and plot the success rate curve and calculate the AUC for the dataset titled Flori21.\n",
    "    \n",
    "    Parameters:\n",
    "    - landmark_errors: List of landmark errors including outliers.\n",
    "    \"\"\"\n",
    "    landmark_errors_sorted = sorted(landmark_errors) # includes all outliers as well\n",
    "    # Initialize lists for thresholds and success rates\n",
    "    thresholds = list(range(101)) # 0 to 100\n",
    "    success_rates = []\n",
    "    # Calculate success rate for each threshold\n",
    "    for threshold in thresholds:\n",
    "        successful_count = sum([1 for error in landmark_errors_sorted if error <= threshold])\n",
    "        success_rate = successful_count / len(landmark_errors_sorted)\n",
    "        success_rates.append(success_rate * 100) # convert to percentage\n",
    "    # Plot the curve\n",
    "    plt.plot(thresholds, success_rates, label=\"Success Rate Curve\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Success Rate (%)\")\n",
    "    plt.title(\"Success Rate vs. Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # Compute AUC\n",
    "    auc = np.sum(success_rates) / 10000 # normalize to 0-1\n",
    "    print(\"AUC:\", auc)\n",
    "\n",
    "def plot_landmark_errors(landmark_errors,rpth,chr='All'):\n",
    "    \"\"\"\n",
    "    Plots a graph of landmark errors over successive iterations to provide a visual analysis of registration accuracy\n",
    "    across samples. This function is designed to help in the assessment of registration processes in image processing\n",
    "    or computer vision tasks by plotting each landmark error against its iteration number. It also calculates and \n",
    "    displays the average landmark error across all iterations.\n",
    "\n",
    "    Parameters:\n",
    "        landmark_errors (list of float): A list containing numerical errors for each landmark across multiple iterations.\n",
    "                                         Outliers (e.g., errors set to 10000) are automatically excluded from the plot.\n",
    "        rpth (str): Path where the resulting plot image will be saved.\n",
    "        chr (str, optional): Characteristic or description to include in the plot title, indicating the dataset or model\n",
    "                             used. Defaults to 'All'.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value but saves the plot to the specified path and displays it.\n",
    "\n",
    "    Notes:\n",
    "        - This plot is useful for tracking improvements or deteriorations in landmark detection algorithms over time.\n",
    "        - It automatically filters out error values set to 10000, considering them as outliers.\n",
    "        - The function saves the plot in the directory specified by `rpth` and names it 'Landmark_Error_Plot.png'.\n",
    "    \"\"\"\n",
    "    #excluding outliers\n",
    "    landmark_errors =[x for x in landmark_errors if x!=10000]\n",
    "    # Generate sample numbers (or indices) based on the number of errors recorded\n",
    "    samples = list(range(1, len(landmark_errors) + 1))\n",
    "    # Calculate the average landmark error\n",
    "    avg_error = sum(landmark_errors) / len(landmark_errors)\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(samples, landmark_errors, marker='o', linestyle='-', color='#2C3E50', label=\"Landmark Error\")\n",
    "    plt.axhline(y=avg_error, color='#E74C3C', linestyle='--', label=f\"Average Error: {avg_error:.3f}\")\n",
    "    plt.title(\"Landmark Error vs. Iteration for Database Housing model {} images\".format(chr), fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Iteration Number\", fontsize=14)\n",
    "    plt.ylabel(\"Landmark Error\", fontsize=14)\n",
    "    plt.xticks(samples, [f\"Iteration {i}\" for i in samples], rotation=45)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(rpth,'Landmark_Error_Plot.png'))\n",
    "    plt.show();\n",
    "\n",
    "def keypoints_visualization(image, landmarks, img_size, rpth, num):\n",
    "    \"\"\"\n",
    "    Visualize keypoints on a given image.\n",
    "\n",
    "    Parameters:\n",
    "    - image (ndarray): The input image on which the keypoints are to be visualized.\n",
    "    - landmarks (list or array-like): Collection of keypoints to be drawn on the image.\n",
    "    - img_size (tuple): The size (width, height) of the image.\n",
    "    - rpth (str): The path where the image with keypoints will be saved.\n",
    "    - num (int or str): An identifier for the saved image file.\n",
    "\n",
    "    Notes:\n",
    "    - The function assumes the existence of a coordinates_rescaling function.\n",
    "    - The displayed image has the title 'Chosen Keypoints' and does not have any axis markings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Rescale the coordinates of the landmarks\n",
    "    rescaled_landmarks = coordinates_rescaling(landmarks, img_size, img_size, 256)\n",
    "\n",
    "    # Define color map based on the number of points\n",
    "    num_points = len(rescaled_landmarks)\n",
    "    if num_points > 15:\n",
    "        cmap = plt.get_cmap('tab20')  # Changed from 'tab10' to 'tab20' for more colors\n",
    "    else:\n",
    "        cmap = ListedColormap([\"red\", \"yellow\", \"blue\", \"lime\", \"magenta\", \"indigo\", \"orange\", \"cyan\",\n",
    "                               \"darkgreen\", \"maroon\", \"black\", \"white\", \"chocolate\", \"gray\", \"blueviolet\"])\n",
    "    colors = np.array([cmap(i) for i in range(num_points)])\n",
    "\n",
    "    # Create the figure and axis\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    # Set title and turn off the axis\n",
    "    ax1.set_title('Chosen Keypoints')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Show the image\n",
    "    ax1.imshow(image, cmap='gray')  # Assuming image is grayscale; remove cmap if it's color\n",
    "\n",
    "    # Define the radii for the circles\n",
    "    radius1, radius2 = 4, 1\n",
    "\n",
    "    # Plot each rescaled landmark on the image\n",
    "    for point, color in zip(rescaled_landmarks, colors):\n",
    "        x, y = point\n",
    "        # Create two circles at each point\n",
    "        circ1 = plt.Circle((x, y), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ2 = plt.Circle((x, y), radius2, facecolor=color, edgecolor='white')\n",
    "        # Add the circles to the axis\n",
    "        ax1.add_patch(circ1)\n",
    "        ax1.add_patch(circ2)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(rpth, 'Keypoints_Visualization_' + str(num) + '.png'))\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show();\n",
    "\n",
    "def image_point_correspondences(images,img_size,landmarks1,landmarks2,landmarks3,rpth,num):\n",
    "    \"\"\"\n",
    "    Displays and compares point correspondences between three images using given landmarks.\n",
    "\n",
    "    This function visualizes three images side-by-side with their respective landmarks. Each pair\n",
    "    of corresponding landmarks across the images is marked with the same color for easy identification\n",
    "    of correspondences. The function is designed to handle visualization for studies involving image\n",
    "    registration or similar tasks where landmark matching is crucial.\n",
    "\n",
    "    Parameters:\n",
    "        images (list of str): File paths to the three images (source, target, and result images).\n",
    "        img_size (int): The size to which images should be resized, specified as width and height (assumed square).\n",
    "        landmarks1 (list of tuples): Landmark points on the first image (source image).\n",
    "        landmarks2 (list of tuples): Corresponding landmark points on the second image (target image).\n",
    "        landmarks3 (list of tuples): Corresponding landmark points on the third image (result after some transformation).\n",
    "        rpth (str): Path where the resultant visualization should be saved.\n",
    "        num (int): An identifier number used to differentiate the output file name.\n",
    "\n",
    "    Returns:\n",
    "        None: The function directly displays the image using matplotlib and saves the output visualization to disk.\n",
    "\n",
    "    Notes:\n",
    "        - The function uses OpenCV for reading and resizing images, and the 'clahe' function to enhance image contrast.\n",
    "        - Matplotlib is used for visualizing the images and landmarks. The colormap used switches based on the number\n",
    "          of landmarks; if there are more than 15 landmarks, a cyclic colormap is used.\n",
    "        - This function is particularly useful for visualizing transformations and registrations in medical imaging or\n",
    "          similar fields where point correspondence is critical.\n",
    "        - It is assumed that the order of landmarks in `landmarks1`, `landmarks2`, and `landmarks3` correspond to each other.\n",
    "    \"\"\"\n",
    "    image1 = cv2.resize(cv2.imread(images[0]),(256,256))\n",
    "    image2 = cv2.resize(cv2.imread(images[1]),(256,256))\n",
    "    #### enhance image contrast for visulaiztion\n",
    "    image1 = clahe(image1, 3)\n",
    "    image2 = clahe(image2, 3)\n",
    "    keypoints_visualization(image1,landmarks1,img_size,rpth,num)\n",
    "    landmarks2 = coordinates_rescaling(landmarks2,img_size,img_size,256)\n",
    "    landmarks3 = coordinates_rescaling(landmarks3,img_size,img_size,256)\n",
    "    assert len(landmarks2) == len(landmarks3), f\"points lengths are incompatible: {len(landmarks2)} != {len(landmarks3)}.\"\n",
    "    num_points = len(landmarks2)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax1.set_title('Source Image')\n",
    "    ax2.set_title('Target Image')\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    ax1.imshow(image1,cmap='gray')\n",
    "    ax2.imshow(image2,cmap='gray')\n",
    "    if num_points > 15:\n",
    "        cmap = plt.get_cmap('tab20')\n",
    "    else:\n",
    "        cmap = ListedColormap([\"red\", \"yellow\", \"blue\", \"lime\", \"magenta\", \"indigo\", \"orange\", \"cyan\", \"darkgreen\",\n",
    "                               \"maroon\", \"black\", \"white\", \"chocolate\", \"gray\", \"blueviolet\"])\n",
    "    colors = np.array([cmap(x) for x in range(num_points)])\n",
    "    radius1, radius2 = 4, 1\n",
    "    for point1, point2, color in zip(landmarks2, landmarks3, colors):\n",
    "        x1, y1 = point1\n",
    "        circ1_1 = plt.Circle((x1, y1), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ1_2 = plt.Circle((x1, y1), radius2, facecolor=color, edgecolor='white')\n",
    "        ax1.add_patch(circ1_1)\n",
    "        ax1.add_patch(circ1_2)\n",
    "        x2, y2 = point2\n",
    "        circ2_1 = plt.Circle((x2, y2), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ2_2 = plt.Circle((x2, y2), radius2, facecolor=color, edgecolor='white')\n",
    "        ax2.add_patch(circ2_1)\n",
    "        ax2.add_patch(circ2_2)\n",
    "    plt.savefig(os.path.join(rpth,'RetinaRegNet_Keypoints_Estimation_Results'+str(num)+'.png'))\n",
    "    plt.show();\n",
    "\n",
    "def original_image_point_correspondences(images, img_size, landmarks1, landmarks2, landmarks3, rpth, num):\n",
    "    \"\"\"\n",
    "    Visualize point correspondences across three images typically representing fixed, moving,\n",
    "    and deformed states in image processing tasks. This function plots and saves the visualizations\n",
    "    showing landmark points overlaid on each image. The images are enhanced using CLAHE for better visibility\n",
    "    and the landmarks are scaled according to a given image size.\n",
    "\n",
    "    Parameters:\n",
    "        images (list of np.array): A list containing three images (as numpy arrays) for fixed, moving,\n",
    "                                   and deformed image states.\n",
    "        img_size (tuple): The original size (width, height) of the images before resizing.\n",
    "        landmarks1 (list of tuples): Landmark coordinates for the first image.\n",
    "        landmarks2 (list of tuples): Landmark coordinates for the second image.\n",
    "        landmarks3 (list of tuples): Landmark coordinates for the third image.\n",
    "        rpth (str): Path to the directory where the result image will be saved.\n",
    "        num (int): A numeric label to differentiate the output file name.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the lengths of landmarks1, landmarks2, and landmarks3 do not match.\n",
    "\n",
    "    Notes:\n",
    "        - This function uses CLAHE for contrast enhancement of the images.\n",
    "        - The landmarks are resized to fit a 256x256 image scale for visualization.\n",
    "        - A colormap is used to differentiate points; if the number of points exceeds 15, a 20-color map is used,\n",
    "          otherwise a specific 15-color map is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(landmarks1) == len(landmarks2) == len(landmarks3), \"All landmarks lists must have the same length.\"\n",
    "    num_points = len(landmarks1)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    ax1.set_title('Fixed Image')\n",
    "    ax2.set_title('Moving Image')\n",
    "    ax3.set_title('Deformed Image')\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    ax3.axis('off')\n",
    "\n",
    "    ax1.imshow(clahe(cv2.resize(images[0],(256,256)),1.2),cmap='gray')\n",
    "    ax2.imshow(clahe(cv2.resize(images[1],(256,256)),1.2),cmap='gray')\n",
    "    ax3.imshow(clahe(cv2.resize(images[2],(256,256)),1.2),cmap='gray')\n",
    "\n",
    "    landmarks1 = coordinates_rescaling(landmarks1,img_size,img_size,256)\n",
    "    landmarks2 = coordinates_rescaling(landmarks2,img_size,img_size,256)\n",
    "    landmarks3 = coordinates_rescaling(landmarks3,img_size,img_size,256)\n",
    "\n",
    "    if num_points > 15:\n",
    "        cmap = plt.get_cmap('tab20')\n",
    "    else:\n",
    "        cmap = ListedColormap([\"red\", \"yellow\", \"blue\", \"lime\", \"magenta\", \"indigo\", \"orange\", \"cyan\", \"darkgreen\",\n",
    "                               \"maroon\", \"black\", \"white\", \"chocolate\", \"gray\", \"blueviolet\"])\n",
    "\n",
    "    colors = np.array([cmap(x) for x in range(num_points)])\n",
    "    radius1, radius2 = 4, 1\n",
    "\n",
    "    for point1, point2, point3, color in zip(landmarks1, landmarks2, landmarks3, colors):\n",
    "        # Landmarks for Image 1\n",
    "        x1, y1 = point1\n",
    "        circ1_1 = plt.Circle((x1, y1), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ1_2 = plt.Circle((x1, y1), radius2, facecolor=color, edgecolor='white')\n",
    "        ax1.add_patch(circ1_1)\n",
    "        ax1.add_patch(circ1_2)\n",
    "\n",
    "        # Landmarks for Image 2\n",
    "        x2, y2 = point2\n",
    "        circ2_1 = plt.Circle((x2, y2), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ2_2 = plt.Circle((x2, y2), radius2, facecolor=color, edgecolor='white')\n",
    "        ax2.add_patch(circ2_1)\n",
    "        ax2.add_patch(circ2_2)\n",
    "\n",
    "        # Landmarks for Image 3\n",
    "        x3, y3 = point3\n",
    "        circ3_1 = plt.Circle((x3, y3), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ3_2 = plt.Circle((x3, y3), radius2, facecolor=color, edgecolor='white')\n",
    "        ax3.add_patch(circ3_1)\n",
    "        ax3.add_patch(circ3_2)\n",
    "\n",
    "    plt.savefig(os.path.join(rpth, 'RetinaRegNet_Original_Keypoints_Estimation_Results' + str(num) + '.png'))\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f6b806-7ba0-4d70-8389-0c9520bfc022",
   "metadata": {
    "id": "88f6b806-7ba0-4d70-8389-0c9520bfc022",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_affine_transformation(points):\n",
    "    \"\"\"\n",
    "    Estimate affine transformation matrix using point correspondences.\n",
    "\n",
    "    Args:\n",
    "    points (np.array): Array of point correspondences.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Affine transformation matrix.\n",
    "    \"\"\"\n",
    "    src_pts = np.float32([point[0] for point in points])\n",
    "    dst_pts = np.float32([point[1] for point in points])\n",
    "    affine_matrix, _ = cv2.estimateAffinePartial2D(src_pts, dst_pts)\n",
    "    return affine_matrix\n",
    "\n",
    "def transform_points_affine(moving_points, affine_matrix):\n",
    "    \"\"\"\n",
    "    Transform the moving points using the given affine matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - moving_points: List of (x, y) tuples\n",
    "    - affine_matrix: (3x3) affine matrix\n",
    "    \n",
    "    Returns:\n",
    "    - List of (x, y) tuples representing transformed points\n",
    "    \"\"\"\n",
    "    points_array = np.array(moving_points)\n",
    "    homogeneous_points = np.hstack([points_array, np.ones((len(moving_points), 1))])\n",
    "    transformed_points = np.dot(homogeneous_points, affine_matrix.T)\n",
    "    return [tuple(point) for point in transformed_points[:, :2]]\n",
    "\n",
    "\n",
    "def transform_points_homography(moving_points, homography_matrix):\n",
    "    \"\"\"\n",
    "    Transform the moving points using the given homography matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - moving_points: List of (x, y) tuples\n",
    "    - homography_matrix: (3x3) homography matrix\n",
    "    \n",
    "    Returns:\n",
    "    - List of (x, y) tuples representing transformed points\n",
    "    \"\"\"\n",
    "    points_array = np.array(moving_points)\n",
    "    homogeneous_points = np.hstack([points_array, np.ones((len(moving_points), 1))])\n",
    "    transformed_points = np.dot(homogeneous_points, homography_matrix.T)\n",
    "    transformed_points /= transformed_points[:, 2][:, np.newaxis]  # Normalize by z-coordinate\n",
    "    return [tuple(point[:2]) for point in transformed_points]\n",
    "\n",
    "\n",
    "def transform_points_third_order_polynomial(moving_points, coefficients):\n",
    "    \"\"\"\n",
    "    Transform the moving points using the given third-order polynomial coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "    - moving_points: List of (x, y) tuples\n",
    "    - coefficients: Array of 20 coefficients for the third-order polynomial transformation\n",
    "    \n",
    "    Returns:\n",
    "    - List of (x, y) tuples representing transformed points\n",
    "    \"\"\"\n",
    "    if len(coefficients) != 20:\n",
    "        raise ValueError(\"Coefficients should have a shape of (20,).\")\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, \\\n",
    "    a11, a12, a13, a14, a15, a16, a17, a18, a19, a20 = coefficients\n",
    "\n",
    "    transformed_points = []\n",
    "    for x, y in moving_points:\n",
    "        # Compute new x' and y' for each point using third-order polynomial\n",
    "        x_prime = (a1*x**3 + a2*x**2*y + a3*x*y**2 + a4*y**3 + \n",
    "                   a5*x**2 + a6*x*y + a7*y**2 + a8*x + a9*y + a10)\n",
    "        y_prime = (a11*x**3 + a12*x**2*y + a13*x*y**2 + a14*y**3 + \n",
    "                   a15*x**2 + a16*x*y + a17*y**2 + a18*x + a19*y + a20)\n",
    "        transformed_points.append((x_prime, y_prime))\n",
    "\n",
    "    return transformed_points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_points_quadratic(points, coefficients):\n",
    "    \"\"\"\n",
    "    Deform a set of points using given quadratic coefficients.\n",
    "    \n",
    "    :param points: A list of points, where each point is a tuple (x, y).\n",
    "    :param coefficients: The coefficients to use for the deformation.\n",
    "    :return: A list of deformed points.\n",
    "    \"\"\"\n",
    "    if len(coefficients) != 12:\n",
    "        raise ValueError(\"Coefficients should have a shape of (12,).\")\n",
    "    \n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12 = coefficients\n",
    "    \n",
    "    deformed = []\n",
    "    for x, y in points:\n",
    "        x_prime = a1*x + a2*y + a3*x*y + a4*x**2 + a5*y**2 + a6\n",
    "        y_prime = a7*x + a8*y + a9*x*y + a10*x**2 + a11*y**2 + a12\n",
    "        deformed.append((x_prime, y_prime))\n",
    "    \n",
    "    return deformed\n",
    "\n",
    "def compute_landmark_error_fixed_space(polynomial_matrix,fixed_points,moving_points,image_size,new_image_size):\n",
    "    \"\"\"\n",
    "    Compute the landmark error between fixed points and transformed moving points.\n",
    "    \n",
    "    Parameters:\n",
    "    - fixed_points: List of (x, y) tuples in the fixed image.\n",
    "    - moving_points: List of (x, y) tuples in the moving image.\n",
    "    - polynomial_matrix: (3x3) matrix used to transform points using a third-order polynomial.\n",
    "    - image_size: The original size of the images.\n",
    "    - new_image_size: The size of the images after rescaling.\n",
    "    \n",
    "    Returns:\n",
    "    - mle: Mean Landmark Error.\n",
    "    \"\"\"\n",
    "    transformed_points = transform_points_third_order_polynomial(moving_points, polynomial_matrix)\n",
    "    transformed_points = coordinates_rescaling_high_scale(transformed_points,image_size,image_size, new_image_size)\n",
    "    errors = np.linalg.norm(np.array(fixed_points) - transformed_points, axis=1)\n",
    "    mle = np.mean(errors)\n",
    "    return mle\n",
    "    \n",
    "\n",
    "def compute_third_order_polynomial_matrix(landmarks1, landmarks2):\n",
    "    \"\"\"\n",
    "    Compute coefficients for the third-order polynomial transformation.\n",
    "\n",
    "    Args:\n",
    "    landmarks1 (list): List of (x, y) tuples of landmarks in the first image.\n",
    "    landmarks2 (list): List of (x, y) tuples of landmarks in the second image.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Coefficients of the third-order polynomial transformation.\n",
    "    \"\"\"\n",
    "    if len(landmarks1) != len(landmarks2) or len(landmarks1) < 10:\n",
    "        raise ValueError(\"Both landmarks should have the same number of points, and at least 10 points are required.\")\n",
    "\n",
    "    A = []\n",
    "    B = []\n",
    "\n",
    "    for (x, y), (x_prime, y_prime) in zip(landmarks1, landmarks2):\n",
    "        # For x'\n",
    "        A.append([x**3, x**2 * y, x * y**2, y**3, x**2, x * y, y**2, x, y, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        # For y'\n",
    "        A.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, x**3, x**2 * y, x * y**2, y**3, x**2, x * y, y**2, x, y, 1])\n",
    "        \n",
    "        B.extend([x_prime, y_prime])\n",
    "\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "\n",
    "    # Solve the linear system\n",
    "    coefficients, _, _, _ = np.linalg.lstsq(A, B, rcond=None)\n",
    "\n",
    "    return coefficients  # The shape of coefficients is (20,)\n",
    "    \n",
    "def compute_quadratic_matrix(landmarks1, landmarks2):\n",
    "    \"\"\"\n",
    "    Compute the quadratic matrix using provided landmarks.\n",
    "    \n",
    "    Parameters:\n",
    "    - landmarks1: List of (x, y) tuples from the source image.\n",
    "    - landmarks2: List of (x, y) tuples from the target image.\n",
    "\n",
    "    Returns:\n",
    "    - 3x3 homography matrix.\n",
    "    \"\"\"\n",
    "    if len(landmarks1) != len(landmarks2) or len(landmarks1) < 6:\n",
    "        raise ValueError(\"Both landmarks should have the same number of points, and at least 6 points are required.\")\n",
    "\n",
    "    A = []\n",
    "    B = []\n",
    "\n",
    "    for (x, y), (x_prime, y_prime) in zip(landmarks1, landmarks2):\n",
    "        A.append([x, y, x*y, x*x, y*y, 1, 0, 0, 0, 0, 0, 0])\n",
    "        A.append([0, 0, 0, 0, 0, 0, x, y, x*y, x*x, y*y, 1])\n",
    "        \n",
    "        B.append(x_prime)\n",
    "        B.append(y_prime)\n",
    "\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "\n",
    "    # Solve the linear system\n",
    "    coefficients, _, _, _ = np.linalg.lstsq(A, B, rcond=None)\n",
    "\n",
    "    return coefficients\n",
    "    \n",
    "def compute_homography_matrix(landmarks1, landmarks2):\n",
    "    \"\"\"\n",
    "    Compute the homography matrix using provided landmarks.\n",
    "    \n",
    "    Parameters:\n",
    "    - landmarks1: List of (x, y) tuples from the source image.\n",
    "    - landmarks2: List of (x, y) tuples from the target image.\n",
    "\n",
    "    Returns:\n",
    "    - 3x3 homography matrix.\n",
    "    \"\"\"\n",
    "    homography_matrix, _ = cv2.findHomography(np.array(landmarks1), np.array(landmarks2))\n",
    "    return homography_matrix\n",
    "\n",
    "def transform_points_third_order_polynomial_matrix(landmarks1, landmarks2,img_size,new_img_size):\n",
    "    \"\"\"\n",
    "    Computes a third-order polynomial transformation matrix based on rescaled landmark points from one image space\n",
    "    to another. This transformation is typically used for tasks like geometric transformation of images where precise \n",
    "    alignment or registration of image features is necessary.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks1 (list of tuples): List of original landmark points in the source image given as (x, y) tuples.\n",
    "        landmarks2 (list of tuples): List of corresponding landmark points in the target image given as (x, y) tuples.\n",
    "                                     The points in landmarks2 should correspond one-to-one with those in landmarks1.\n",
    "        img_size (int): Original size of the images from which the landmarks were extracted. This is used to help\n",
    "                        rescale points for accurate computation of the transformation matrix.\n",
    "        new_img_size (int): New size to which the points will be rescaled before computing the transformation matrix.\n",
    "                            This should reflect the size of the image space into which the points will be transformed.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A transformation matrix which can be used to map the points from the space defined by landmarks1\n",
    "                       to the space defined by landmarks2. The matrix is represented as a 10x1 array of coefficients,\n",
    "                       corresponding to the terms of a third-order polynomial.\n",
    "\n",
    "    Notes:\n",
    "        - Ensure that the number of points in landmarks1 and landmarks2 are equal and that they correspond to each other in order.\n",
    "        - This function involves rescaling coordinates, calculating a transformation matrix, and is typically used in image processing\n",
    "          tasks where geometric transformations are necessary for alignment and registration.\n",
    "    \"\"\"\n",
    "    landmarks1 = coordinates_rescaling(landmarks1,img_size,img_size,new_img_size)\n",
    "    landmarks2 = coordinates_rescaling(landmarks2,img_size,img_size,new_img_size)\n",
    "    third_order_polynomial_matrix  = compute_third_order_polynomial_matrix(landmarks1, landmarks2)\n",
    "    return third_order_polynomial_matrix \n",
    "\n",
    "def transform_points_quadratic_matrix(landmarks1, landmarks2,img_size,new_img_size):\n",
    "    \"\"\"\n",
    "    Computes a quadratic transformation matrix based on rescaled landmarks from one set of image coordinates to another.\n",
    "\n",
    "    This function rescales the input landmarks from their original dimensions (img_size) to new dimensions (new_img_size).\n",
    "    It then calculates a quadratic transformation matrix that describes how points from the first set of landmarks (landmarks1)\n",
    "    can be transformed to align with the second set (landmarks2). This matrix could be used to apply geometric transformations\n",
    "    to images or coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks1 (list of tuples): List of (x, y) tuples representing original landmarks in the source image.\n",
    "        landmarks2 (list of tuples): List of (x, y) tuples representing target landmarks in the target image, \n",
    "                                     corresponding to landmarks1.\n",
    "        img_size (int): The original size (width and height, assumed square) of the images from which the landmarks were extracted.\n",
    "        new_img_size (int): The new size (width and height, assumed square) to which the images and landmarks are rescaled\n",
    "                            before computing the transformation matrix.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A matrix that contains the coefficients of the quadratic transformation. This matrix is used\n",
    "                       to transform points from the source image to the target image based on the calculated polynomial.\n",
    "\n",
    "    Notes:\n",
    "        - Ensure that the number of points in landmarks1 and landmarks2 are equal and that they correspond to each other in order.\n",
    "        - This function is essential in image processing tasks where precise transformations are necessary for image alignment and registration.\n",
    "    \"\"\"\n",
    "    landmarks1 = coordinates_rescaling(landmarks1,img_size,img_size,new_img_size)\n",
    "    landmarks2 = coordinates_rescaling(landmarks2,img_size,img_size,new_img_size)\n",
    "    quadratic_matrix = compute_quadratic_matrix(landmarks1, landmarks2)\n",
    "    return quadratic_matrix \n",
    "\n",
    "\n",
    "def warp_image_third_order_polynomial(image, coefficients):\n",
    "    \"\"\"\n",
    "    Deform the image using given third-order polynomial coefficients.\n",
    "    \n",
    "    :param image: The image to deform, as a numpy array (height, width) or (height, width, channels).\n",
    "    :param coefficients: The coefficients to use for the deformation.\n",
    "    :return: The deformed image.\n",
    "    \"\"\"\n",
    "    if len(coefficients) != 20:\n",
    "        raise ValueError(\"Coefficients should have a shape of (20,).\")\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, \\\n",
    "    a11, a12, a13, a14, a15, a16, a17, a18, a19, a20 = coefficients\n",
    "    \n",
    "    # Check if the image is grayscale or colored\n",
    "    if len(image.shape) == 2:\n",
    "        height, width = image.shape\n",
    "        output = np.zeros((height, width))\n",
    "        channels = 1\n",
    "        image = image[:, :, np.newaxis]  # add an additional dimension for consistency\n",
    "    else:\n",
    "        height, width, channels = image.shape\n",
    "        output = np.zeros((height, width, channels))\n",
    "    \n",
    "    # Generate the coordinates\n",
    "    coordinates = np.indices((height, width))\n",
    "    x_coords = coordinates[1]\n",
    "    y_coords = coordinates[0]\n",
    "\n",
    "    # Compute new x' and y' for every x and y using third-order polynomial\n",
    "    x_prime = (a1*x_coords**3 + a2*x_coords**2*y_coords + a3*x_coords*y_coords**2 + a4*y_coords**3 + \n",
    "               a5*x_coords**2 + a6*x_coords*y_coords + a7*y_coords**2 + a8*x_coords + a9*y_coords + a10)\n",
    "    y_prime = (a11*x_coords**3 + a12*x_coords**2*y_coords + a13*x_coords*y_coords**2 + a14*y_coords**3 + \n",
    "               a15*x_coords**2 + a16*x_coords*y_coords + a17*y_coords**2 + a18*x_coords + a19*y_coords + a20)\n",
    "\n",
    "    # Map the old image pixels to the new deformed positions\n",
    "    for c in range(channels):  # for each channel\n",
    "        output[:, :, c] = map_coordinates(image[:, :, c], [y_prime, x_prime], order=1, mode='constant', cval=0.0)\n",
    "\n",
    "    if channels == 1:\n",
    "        return output[:, :, 0]  # return as 2D grayscale image\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def warp_image_quadratic_matrix(image, coefficients):\n",
    "    \"\"\"\n",
    "    Deform the image using given quadratic coefficients.\n",
    "    \n",
    "    :param image: The image to deform, as a numpy array (height, width) or (height, width, channels).\n",
    "    :param coefficients: The coefficients to use for the deformation.\n",
    "    :return: The deformed image.\n",
    "    \"\"\"\n",
    "    if len(coefficients) != 12:\n",
    "        raise ValueError(\"Coefficients should have a shape of (12,).\")\n",
    "    \n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12 = coefficients\n",
    "    \n",
    "    # Check if the image is grayscale or colored\n",
    "    if len(image.shape) == 2:\n",
    "        height, width = image.shape\n",
    "        output = np.zeros((height, width))\n",
    "        channels = 1\n",
    "        image = image[:, :, np.newaxis]  # add an additional dimension for consistency\n",
    "    else:\n",
    "        height, width, channels = image.shape\n",
    "        output = np.zeros((height, width, channels))\n",
    "    \n",
    "    # Generate the coordinates\n",
    "    coordinates = np.indices((height, width))\n",
    "    x_coords = coordinates[1]\n",
    "    y_coords = coordinates[0]\n",
    "\n",
    "    # Compute new x' and y' for every x and y\n",
    "    x_prime = a1*x_coords + a2*y_coords + a3*x_coords*y_coords + a4*x_coords**2 + a5*y_coords**2 + a6\n",
    "    y_prime = a7*x_coords + a8*y_coords + a9*x_coords*y_coords + a10*x_coords**2 + a11*y_coords**2 + a12\n",
    "\n",
    "    # Map the old image pixels to the new deformed positions\n",
    "    for c in range(channels):  # for each channel\n",
    "        output[:, :, c] = map_coordinates(image[:, :, c], [y_prime, x_prime], order=1, mode='constant', cval=0.0)\n",
    "\n",
    "    if channels == 1:\n",
    "        return output[:, :, 0]  # return as 2D grayscale image\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "\n",
    "def compute_third_order_polynomial_matrix_and_plot(images, img_size, landmarks1, landmarks2, rpth, num,cll=5):\n",
    "    \"\"\"\n",
    "    Computes the third-order polynomial transformation matrix between two sets of landmarks\n",
    "    and applies this transformation to warp one image to align with another. The function \n",
    "    displays and saves the fixed, moving, and deformed images with contrast enhancement.\n",
    "\n",
    "    Parameters:\n",
    "        images (list of str): List containing the file paths of two images.\n",
    "        img_size (int): The size (height and width) to which the images will be resized.\n",
    "        landmarks1 (list of tuples): Landmark points (x, y) on the first image.\n",
    "        landmarks2 (list of tuples): Corresponding landmark points (x, y) on the second image.\n",
    "        rpth (str): The directory path where the resultant images will be saved.\n",
    "        num (int): Identifier number used to differentiate the output file names.\n",
    "        cll (float, optional): The clipping limit for the CLAHE algorithm used in contrast enhancement. Default is 5.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the landmarks1 list is empty.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing three elements:\n",
    "            - imags (list of np.array): List containing the original fixed and moving images and the transformed image.\n",
    "            - imgs (list of str): File paths where the output images have been saved.\n",
    "            - coefficients (np.array): Coefficients of the third-order polynomial transformation.\n",
    "\n",
    "    Notes:\n",
    "        - The third-order polynomial transformation is a complex and computationally intensive operation typically used\n",
    "          for fine-grained image registration tasks where simple affine transformations are insufficient.\n",
    "        - The resultant 'Deformed Image' is the second image warped to align with the first image based on the computed\n",
    "          polynomial transformation matrix.\n",
    "    \"\"\"\n",
    "    imags,imgs = [],[]\n",
    "    img1 = cv2.resize(cv2.imread(images[0]), (img_size, img_size))\n",
    "    img2 = cv2.resize(cv2.imread(images[1]), (img_size, img_size))\n",
    "    \n",
    "    imags.append(img1)\n",
    "    imags.append(img2)\n",
    "\n",
    "    # Check if the list is not empty\n",
    "    if not landmarks1:\n",
    "        raise ValueError(\"Input list cannot be empty\")\n",
    "\n",
    "    # Compute the third-order polynomial transformation matrix\n",
    "    coefficients = compute_third_order_polynomial_matrix(landmarks1, landmarks2)\n",
    "    coefficients_for_transform = compute_third_order_polynomial_matrix(landmarks2, landmarks1)\n",
    "\n",
    "    # Apply the transformation using third-order polynomial\n",
    "    transformed_image = warp_image_third_order_polynomial(img2, coefficients_for_transform.flatten())\n",
    "    imags.append(transformed_image)\n",
    "\n",
    "    # Display and save the images\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(clahe(img1.astype(np.uint8),cll))\n",
    "    plt.title('Fixed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(clahe(img2.astype(np.uint8),cll))\n",
    "    plt.title('Moving Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(clahe(transformed_image.astype(np.uint8),cll))\n",
    "    plt.title('Deformed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    imgs.append(os.path.join(rpth, 'Deformed_Image_' + str(num) + '_.png'))\n",
    "    imgs.append(os.path.join(rpth, 'Target_' + str(num) + '_.png'))\n",
    "    cv2.imwrite(os.path.join(rpth, 'Target_' + str(num) + '_.png'), img1)\n",
    "    cv2.imwrite(os.path.join(rpth, 'Source_' + str(num) + '_.png'), img2)\n",
    "    cv2.imwrite(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'),transformed_image);\n",
    "    return imags,imgs,coefficients\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_affine_matrix(images,img_size,landmarks1, landmarks2,rpth,num,cll=5):\n",
    "    \"\"\"\n",
    "    Computes an affine transformation matrix based on provided landmarks from two images and applies \n",
    "    this transformation to visually compare the source, target, and transformed images.\n",
    "\n",
    "    This function takes pairs of landmarks from two images and computes the affine transformation matrix \n",
    "    that best maps the source image to align with the target image. It then applies this transformation to \n",
    "    the source image and displays both the original (source and target) and the transformed images side-by-side.\n",
    "    The images are displayed after contrast enhancement and are saved to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "        images (list of str): File paths for the source and target images.\n",
    "        img_size (int): The size (width and height) to which the images will be resized, defaulting to 256.\n",
    "        landmarks1 (list of tuples): Landmark points (x, y) on the source image.\n",
    "        landmarks2 (list of tuples): Corresponding landmark points (x, y) on the target image.\n",
    "        rpth (str): Directory path where the resultant images will be saved.\n",
    "        num (int): Identifier number used to differentiate the output file names.\n",
    "        cll (float, optional): Clipping limit for the CLAHE algorithm used in contrast enhancement. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains two items:\n",
    "            - imgs (list of str): File paths where the images are saved.\n",
    "            - affine_matrix (numpy.ndarray): The computed 3x3 affine transformation matrix.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the landmarks list is empty, indicating insufficient data to compute the matrix.\n",
    "\n",
    "    Notes:\n",
    "        - This function uses OpenCV for image processing tasks including reading, resizing, and warping images.\n",
    "        - The affine transformation matrix is computed using a least squares method based on provided landmarks.\n",
    "        - This function is useful for image registration tasks where visual comparison of alignment is required.\n",
    "    \"\"\"\n",
    "    imgs=[]\n",
    "    img1 = cv2.resize(cv2.imread(images[0]),(img_size,img_size))\n",
    "    img2 = cv2.resize(cv2.imread(images[1]),(img_size,img_size))\n",
    "    \n",
    "     # Check if the list is not empty\n",
    "    if not landmarks1:\n",
    "        raise ValueError(\"Input list cannot be empty\")\n",
    "\n",
    "    # Create the array with the specified format\n",
    "    A = np.array([[xs, ys, 1] for xs, ys in landmarks1])\n",
    "    \n",
    "    X = np.array([xt for xt, yt in landmarks2])\n",
    "    Y = np.array([yt for xt, yt in landmarks2])\n",
    "\n",
    "    # Solve for the variables x1, y1, and z1\n",
    "    sol1 = np.dot(np.dot(np.linalg.inv(np.dot(A.T,A)),A.T),X)\n",
    "    sol2 = np.dot(np.dot(np.linalg.inv(np.dot(A.T,A)),A.T),Y)\n",
    "    # Extract the variables\n",
    "    x1, y1, z1 = sol1\n",
    "    x2, y2, z2 = sol2\n",
    "    affine_matrix = np.array([[x1,y1,z1],\n",
    "                           [x2,y2,z2],\n",
    "                           [0, 0, 1]])\n",
    "    print(\"Affine Matrix:\")\n",
    "    print(affine_matrix)\n",
    "    \n",
    "    # Ensure the affine matrix is of type float32\n",
    "    affine_matrix = affine_matrix.astype(np.float32)\n",
    "\n",
    "    # Use only the top two rows for cv2.warpAffine\n",
    "    affine_for_warp = affine_matrix[:2]\n",
    "    \n",
    "    # Apply the affine transformation using cv2.warpAffine\n",
    "    transformed_image = cv2.warpAffine(img2, affine_for_warp, (img2.shape[1], img2.shape[0]))\n",
    "\n",
    "    # Display the original and transformed images\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(clahe(img1,cll))\n",
    "    \n",
    "    plt.title('Fixed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(clahe(img2,cll))\n",
    "    \n",
    "    plt.title('Moving Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(clahe(transformed_image,cll))\n",
    "    \n",
    "    plt.title('Deformed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show();\n",
    "    imgs.append(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'))\n",
    "    imgs.append(os.path.join(rpth,'Target_'+str(num)+'_.png'))\n",
    "    cv2.imwrite(os.path.join(rpth,'Target_'+str(num)+'_.png'),img1);\n",
    "    cv2.imwrite(os.path.join(rpth,'Source_'+str(num)+'_.png'),img2);\n",
    "    cv2.imwrite(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'),transformed_image);\n",
    "    \n",
    "    return imgs,affine_matrix\n",
    "    \n",
    "def warp_quadratic_matrix(images,img_size,landmarks1, landmarks2,rpth,num,cll=5):\n",
    "    \"\"\"\n",
    "    Computes a quadratic transformation matrix from source to target landmarks and applies this transformation\n",
    "    to the source image. The transformed source image is displayed alongside the original source and target images,\n",
    "    and all images are saved to disk.\n",
    "\n",
    "    This function takes pairs of corresponding landmarks from the source and target images to compute a quadratic\n",
    "    transformation matrix. This matrix is then used to warp the source image to match the target image. The\n",
    "    result, along with the original images, is displayed and saved for comparison.\n",
    "\n",
    "    Parameters:\n",
    "        images (list of str): File paths for the source and target images.\n",
    "        img_size (int): The size (width and height) to which the images will be resized, defaulting to 256.\n",
    "        landmarks1 (list of tuples): Landmark points (x, y) on the source image.\n",
    "        landmarks2 (list of tuples): Corresponding landmark points (x, y) on the target image.\n",
    "        rpth (str): Directory path where the resultant images will be saved.\n",
    "        num (int): Identifier number used to differentiate the output file names.\n",
    "        cll (float, optional): Clipping limit for the CLAHE algorithm used in contrast enhancement. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains three items:\n",
    "            - imgs (list of str): File paths where the output images are saved.\n",
    "            - imags (list of np.array): List containing the numpy arrays of the original and transformed images.\n",
    "            - quadratic_matrix (numpy.ndarray): The computed quadratic transformation matrix.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the number of points in `landmarks1` and `landmarks2` are not equal, since a matching\n",
    "                        number of points is required for matrix computation.\n",
    "\n",
    "    Notes:\n",
    "        - The function uses OpenCV for image processing tasks such as reading, resizing, transforming, and saving images.\n",
    "        - The quadratic transformation matrix is computed using a least squares method based on provided landmarks.\n",
    "        - Matplotlib is used for visualizing the before and after effects of the transformation.\n",
    "        - This function is particularly useful in applications such as image registration and geometric transformations.\n",
    "    \"\"\"\n",
    "    imgs,imags=[],[]\n",
    "    img1 = cv2.resize(cv2.imread(images[0]),(img_size,img_size))\n",
    "    img2 = cv2.resize(cv2.imread(images[1]),(img_size,img_size))\n",
    "    \n",
    "    imags.append(img1)\n",
    "    imags.append(img2)\n",
    "    \n",
    "    assert len(landmarks1) == len(landmarks2), \"landmarks lists must have the same length.\"\n",
    "\n",
    "    # Ensure the quadratic matrix is of type float32\n",
    "    quadratic_matrix = compute_quadratic_matrix(landmarks1, landmarks2)\n",
    "    \n",
    "    quadratic_matrix_for_image_deformed = compute_quadratic_matrix(landmarks2, landmarks1)\n",
    "    \n",
    "    print(\"quadratic Matrix:\")\n",
    "    print(quadratic_matrix)\n",
    "    \n",
    "    # Apply the quadratic transformation using cv2.warpquadratic\n",
    "    transformed_image =  warp_image_quadratic_matrix(img2, quadratic_matrix_for_image_deformed)\n",
    "    transformed_image = cv2.resize(transformed_image,  (img2.shape[1], img2.shape[0]))\n",
    "    \n",
    "    # Display the original and transformed images\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(clahe(img1,cll))\n",
    "    \n",
    "    plt.title('Fixed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(clahe(img2,cll))\n",
    "    \n",
    "    plt.title('Moving Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(clahe(transformed_image.astype(int),cll))\n",
    "    \n",
    "    plt.title('Deformed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show();\n",
    "    imags.append(transformed_image)\n",
    "    imgs.append(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'))\n",
    "    imgs.append(os.path.join(rpth,'Target_'+str(num)+'_.png'))\n",
    "    cv2.imwrite(os.path.join(rpth,'Target_'+str(num)+'_.png'),img1);\n",
    "    cv2.imwrite(os.path.join(rpth,'Source_'+str(num)+'_.png'),img2);\n",
    "    cv2.imwrite(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'),transformed_image);\n",
    "    return imgs,imags,quadratic_matrix\n",
    "\n",
    "def compute_and_apply_homography(images, img_size, landmarks1, landmarks2, rpth, num,cll=5):\n",
    "    \"\"\"\n",
    "    Computes the homography transformation matrix based on landmark correspondences between two images \n",
    "    and applies this transformation to the source image. The function displays the original source and \n",
    "    target images along with the transformed source image. It also saves these images to disk.\n",
    "\n",
    "    Parameters:\n",
    "        images (list of str): Paths to the source and target images.\n",
    "        img_size (int): The size to which both images will be resized (assumed square for simplicity).\n",
    "        landmarks1 (list of tuples): Landmark points (x, y) from the source image.\n",
    "        landmarks2 (list of tuples): Corresponding landmark points (x, y) from the target image.\n",
    "        rpth (str): The directory path where the resultant images will be saved.\n",
    "        num (int): An identifier number used to differentiate the output file names.\n",
    "        cll (float): The clipping limit for the CLAHE algorithm used in contrast enhancement. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the paths to the saved images, a list of image arrays including the transformed image,\n",
    "               and the computed homography matrix.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the list of landmarks is empty, indicating that there are not enough data points to compute the homography.\n",
    "    Notes:\n",
    "        - The function uses OpenCV for image reading, resizing, and applying the homography transformation.\n",
    "          Matplotlib is used for displaying the images.\n",
    "        - Ensure the landmarks are accurately defined as their correspondence directly affects the quality of the transformation.\n",
    "        - Homography transformations are particularly useful for applications in image registration, computer vision, and photogrammetry.\n",
    "    \"\"\"\n",
    "    imgs,imags=[],[]\n",
    "    img1 = cv2.resize(cv2.imread(images[0]),(img_size,img_size))\n",
    "    img2 = cv2.resize(cv2.imread(images[1]),(img_size,img_size))\n",
    "    \n",
    "    imags.append(img1)\n",
    "    imags.append(img2)\n",
    "    \n",
    "    # Check if the list is not empty\n",
    "    if not landmarks1:\n",
    "        raise ValueError(\"Input list cannot be empty\")\n",
    "        \n",
    "    # Compute homography matrix\n",
    "    homography_matrix = compute_homography_matrix(landmarks1, landmarks2)\n",
    "    \n",
    "    print(\"Homography Matrix:\")\n",
    "    print(homography_matrix)\n",
    "    \n",
    "    # Ensure the affine matrix is of type float32\n",
    "    homography_matrix = homography_matrix.astype(np.float32)\n",
    "\n",
    "    # Apply the homography transformation using cv2.warpPerspective\n",
    "    transformed_image=cv2.warpPerspective(img2, homography_matrix, (img2.shape[1], img2.shape[0]))\n",
    "    \n",
    "    # Display the original and transformed images\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(clahe(img1,cll))\n",
    "    plt.title('Fixed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(clahe(img2,cll))\n",
    "    plt.title('Moving Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(clahe(transformed_image.astype(np.uint8),cll))\n",
    "    plt.title('Deformed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show();\n",
    "\n",
    "    imags.append(transformed_image)\n",
    "    imgs.append(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'))\n",
    "    imgs.append(os.path.join(rpth,'Target_'+str(num)+'_.png'))\n",
    "    cv2.imwrite(os.path.join(rpth, 'Target_' + str(num) + '_.png'),img1)\n",
    "    cv2.imwrite(os.path.join(rpth, 'Source_' + str(num) + '_.png'),img2)\n",
    "    cv2.imwrite(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'),transformed_image);\n",
    "    return imgs,imags,homography_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qrsKQysjS74U",
   "metadata": {
    "id": "qrsKQysjS74U"
   },
   "outputs": [],
   "source": [
    "def landmark_error(point, transformed_point):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between the original point and the transformed point.\n",
    "\n",
    "    Args:\n",
    "    point (tuple): Original point (x, y).\n",
    "    transformed_point (tuple): Transformed point (x, y).\n",
    "\n",
    "    Returns:\n",
    "    float: Euclidean distance.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(np.array(point) - np.array(transformed_point))\n",
    "\n",
    "def estimate_affine_transformation(points):\n",
    "    \"\"\"\n",
    "    Estimate affine transformation matrix using point correspondences.\n",
    "\n",
    "    Args:\n",
    "    points (np.array): Array of point correspondences.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Affine transformation matrix.\n",
    "    \"\"\"\n",
    "    src_pts = np.float32([point[0] for point in points])\n",
    "    dst_pts = np.float32([point[1] for point in points])\n",
    "    affine_matrix, _ = cv2.estimateAffinePartial2D(src_pts, dst_pts)\n",
    "    return affine_matrix\n",
    "\n",
    "def estimate_homography_matrix(points):\n",
    "    \"\"\"\n",
    "    Estimate the homography matrix given a set of point correspondences.\n",
    "\n",
    "    Parameters:\n",
    "    - points: A list of tuples, where each tuple contains two (x, y) tuples.\n",
    "              The first tuple in each pair is from the first set of points (set1),\n",
    "              and the second tuple is the corresponding point in the second set (set2).\n",
    "\n",
    "    Returns:\n",
    "    - homography_matrix: The estimated (3x3) homography matrix.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "\n",
    "    # Separate the points into two sets\n",
    "    set1 = [point[0] for point in points]\n",
    "    set2 = [point[1] for point in points]\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    set1 = np.array(set1, dtype=np.float32)\n",
    "    set2 = np.array(set2, dtype=np.float32)\n",
    "\n",
    "    # Estimate the homography matrix\n",
    "    homography_matrix, _ = cv2.findHomography(set1, set2, cv2.RANSAC)\n",
    "\n",
    "    return homography_matrix\n",
    "\n",
    "\n",
    "def remove_outliers_based_on_error_affine(set1, set2, threshold=20):\n",
    "    \"\"\"\n",
    "    Filters out outlier point pairs from two sets of points by applying an affine transformation and \n",
    "    removing pairs that have an error greater than a specified threshold. The function first estimates\n",
    "    an affine transformation matrix based on all given point pairs. Each point in the first set is then \n",
    "    transformed using this matrix, and the error is calculated as the Euclidean distance between the \n",
    "    transformed point and the corresponding point in the second set. Points with an error exceeding the \n",
    "    threshold are considered outliers and are excluded from the results.\n",
    "\n",
    "    Parameters:\n",
    "        set1 (list of tuples): A list of (x, y) tuples representing coordinates of points in the first image.\n",
    "        set2 (list of tuples): A list of (x, y) tuples representing corresponding coordinates of points in the \n",
    "                               second image. The indices in `set1` and `set2` must correspond.\n",
    "        threshold (float, optional): The maximum allowed error distance between the original and transformed \n",
    "                                     points for them to be considered inliers. Default value is 20.\n",
    "\n",
    "    Returns:\n",
    "        tuple of lists: Returns two lists (updated_set1, updated_set2) containing the inlier points from\n",
    "                        `set1` and `set2` respectively.\n",
    "    Notes:\n",
    "        - It is critical that `set1` and `set2` are of equal length and that the points correspond correctly, \n",
    "          as any misalignment could result in incorrect calculations and poor results.\n",
    "        - This function is typically used in image processing and computer vision tasks where alignment and \n",
    "          transformation of point sets between images is required, particularly in stereo vision and motion tracking.\n",
    "    \"\"\"\n",
    "    points = list(zip(set1, set2))\n",
    "    affine_matrix = estimate_affine_transformation(points)\n",
    "    updated_set1 = []\n",
    "    updated_set2 = []\n",
    "\n",
    "    for point1, point2 in zip(set1, set2):\n",
    "        transformed_point = transform_points_affine([point1], affine_matrix)[0]\n",
    "        error = landmark_error(point2, transformed_point)\n",
    "\n",
    "        if error <= threshold:\n",
    "            updated_set1.append(point1)\n",
    "            updated_set2.append(point2)\n",
    "\n",
    "    return updated_set1, updated_set2\n",
    "\n",
    "def remove_outliers_based_on_error_homography(set1, set2, threshold=20):\n",
    "    \"\"\"\n",
    "    Filters out outlier point pairs from two sets of points by applying a homography transformation\n",
    "    and removing pairs that have an error greater than a specified threshold. The function first estimates\n",
    "    a homography transformation matrix based on all given point pairs. Each point in the first set is then\n",
    "    transformed using this matrix, and the error is calculated as the Euclidean distance between the\n",
    "    transformed point and the corresponding point in the second set. Points with an error exceeding the\n",
    "    threshold are considered outliers and are excluded from the results.\n",
    "\n",
    "    Parameters:\n",
    "        set1 (list of tuples): A list of (x, y) tuples representing coordinates of points in the first image.\n",
    "        set2 (list of tuples): A list of (x, y) tuples representing corresponding coordinates of points in the\n",
    "                               second image. The indices in `set1` and `set2` must correspond.\n",
    "        threshold (float, optional): The maximum allowed error distance between the original and transformed \n",
    "                                     points for them to be considered inliers. Default value is 20.\n",
    "\n",
    "    Returns:\n",
    "        tuple of lists: Returns two lists (updated_set1, updated_set2) containing the inlier points from\n",
    "                        `set1` and `set2` respectively.\n",
    "    Notes:\n",
    "        - Ensure that `set1` and `set2` are of equal length and that the points correspond correctly, \n",
    "          as any misalignment could result in incorrect calculations and poor results.\n",
    "        - This function is typically used in image processing and computer vision tasks where precise\n",
    "          alignment and transformation of point sets between images are required, especially in applications\n",
    "          like panorama stitching and object tracking.\n",
    "    \"\"\"\n",
    "    points = list(zip(set1, set2))\n",
    "    homography_matrix = estimate_homography_matrix(points)\n",
    "    updated_set1 = []\n",
    "    updated_set2 = []\n",
    "\n",
    "    for point1, point2 in zip(set1, set2):\n",
    "        transformed_point = transform_points_homography([point1], homography_matrix)[0]\n",
    "        error = landmark_error(point2, transformed_point)\n",
    "\n",
    "        if error <= threshold:\n",
    "            updated_set1.append(point1)\n",
    "            updated_set2.append(point2)\n",
    "\n",
    "    return updated_set1, updated_set2\n",
    "\n",
    "\n",
    "def filter_outlier_cond(computed,original,criteria='affine', thresh=20):\n",
    "    \"\"\"\n",
    "    Filter outliers based on a specified condition.\n",
    "\n",
    "    This function processes two sets of points (computed and original) and filters out outliers based on a specified criteria (either 'affine' or 'homography'). The function uses either homography matrix estimation or affine error-based methods to identify and remove outliers.\n",
    "\n",
    "    Parameters:\n",
    "    - computed (list of tuples): List of computed points as (x, y) coordinates.\n",
    "    - original (list of tuples): List of original points as (x, y) coordinates to compare against.\n",
    "    - criteria (str, optional): The criteria to use for filtering outliers. Options are 'affine' or 'homography'. Defaults to 'affine'.\n",
    "    - thresh (int, optional): Threshold value used in the outlier removal process. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list containing the filtered computed points after outlier removal.\n",
    "    - list: A list containing the filtered original points after outlier removal.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: If the length of the computed points is not 3.\n",
    "\n",
    "    Notes:\n",
    "    - If 'homography' is chosen as the criteria, the function estimates a homography matrix between the computed and original points and removes outliers based on the threshold.\n",
    "    - If 'affine' is chosen, it removes outliers based on affine transformation error exceeding the threshold.\n",
    "    \"\"\"\n",
    "    assert len(computed) >= 3\n",
    "    if criteria=='homography':\n",
    "        computed,original = estimate_homography_matrix(computed,original,thresh)\n",
    "    else:\n",
    "        computed,original = remove_outliers_based_on_error_affine(computed,original,thresh)\n",
    "    return computed,original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ujyb-RXOSTRT",
   "metadata": {
    "id": "Ujyb-RXOSTRT"
   },
   "outputs": [],
   "source": [
    "def coordinates_rescaling_high_scale(pnts,H,W,img_shape):\n",
    "    \"\"\"\n",
    "    Rescale a list of coordinates based on given height and width ratios.\n",
    "\n",
    "    Parameters:\n",
    "    - pnts (list of tuples): List of (x, y) coordinates to be rescaled.\n",
    "    - H (int): Original height.\n",
    "    - W (int): Original width.\n",
    "    - img_shape (int): Desired image dimension (assumes square shape).\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: List of rescaled (x, y) coordinates.\n",
    "    \"\"\"\n",
    "    scaled_points=[]\n",
    "    for row in pnts:\n",
    "        a = (row[0]/W)*img_shape[1]\n",
    "        b = (row[1]/H)*img_shape[0]\n",
    "        scaled_points.append((a,b))\n",
    "    return scaled_points\n",
    "\n",
    "\n",
    "def coordinates_rescaling(pnts,H,W,img_shape):\n",
    "    \"\"\"\n",
    "    Rescale a list of coordinates based on given height and width ratios.\n",
    "\n",
    "    Parameters:\n",
    "    - pnts (list of tuples): List of (x, y) coordinates to be rescaled.\n",
    "    - H (int): Original height.\n",
    "    - W (int): Original width.\n",
    "    - img_shape (int): Desired image dimension (assumes square shape).\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: List of rescaled (x, y) coordinates.\n",
    "    \"\"\"\n",
    "    scaled_points=[]\n",
    "    for row in pnts:\n",
    "        a = (row[0]/W)*img_shape\n",
    "        b = (row[1]/H)*img_shape\n",
    "        scaled_points.append((a,b))\n",
    "    return scaled_points\n",
    "\n",
    "def CLAHE_Images(imags, clip):\n",
    "    \"\"\"\n",
    "    Applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to a list of image files to enhance \n",
    "    their contrast. This method is particularly useful for improving the visibility of features in images \n",
    "    that suffer from poor contrast.\n",
    "\n",
    "    Parameters:\n",
    "        imags (list of str): List of paths to the image files that need contrast enhancement.\n",
    "        clip (float): Clip limit for the CLAHE algorithm, which sets the threshold for contrast limiting. \n",
    "                      The higher the clip limit, the more aggressive the contrast enhancement.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Returns a list of paths to the saved CLAHE-processed images. Each processed image is \n",
    "                     saved with a \"CLAHE_\" prefix in its filename to distinguish it from the original.\n",
    "\n",
    "    Notes:\n",
    "        - This function uses OpenCV's `createCLAHE` method to apply the CLAHE algorithm. Each image is \n",
    "          first converted to grayscale as CLAHE is typically applied to single-channel images for better \n",
    "          visualization of detail.\n",
    "        - The images are processed in-place and saved in the same directory as the original, with 'CLAHE_' \n",
    "          prefixed to their original filenames.\n",
    "        - It is recommended to adjust the `clip` parameter based on the specific requirements of the image \n",
    "          content and the desired level of contrast enhancement.\n",
    "    \"\"\"\n",
    "    imgs=[]\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(8, 8))\n",
    "    for img in imags:\n",
    "      fn,_ = os.path.splitext(os.path.basename(img))\n",
    "      ifn = 'CLAHE'+'_'+str(fn)+'.png'\n",
    "      imag = cv2.imread(img)\n",
    "      imag = Image.fromarray(np.uint8(imag))\n",
    "      imag = imag.convert('L')\n",
    "      img = np.asarray(imag)\n",
    "      image_equalized = clahe.apply(img)\n",
    "      image_equalized_img = Image.fromarray(np.uint8(image_equalized))\n",
    "      image_equalized = image_equalized_img.convert('RGB')\n",
    "      image_equalized = np.asarray(image_equalized)\n",
    "      cv2.imwrite(ifn,image_equalized);\n",
    "      imgs.append(ifn)\n",
    "    return imgs\n",
    "\n",
    "def Feature_padding(feature_maps, size):\n",
    "    \"\"\"\n",
    "    Pad feature maps to a uniform size using bilinear interpolation.\n",
    "\n",
    "    This function adjusts the size of each feature map in the input list to a specified uniform size using bilinear interpolation. This is typically used to standardize the size of feature maps obtained from different sources or processes.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_maps (list of tensors): A list of feature map tensors to be resized.\n",
    "    - size (tuple): The target size for the feature maps as (height, width).\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of uniformly sized feature maps.\n",
    "    \"\"\"\n",
    "    uniform_feature_maps=[]\n",
    "    for feature in feature_maps:\n",
    "        uniform_feature_maps.append(F.interpolate(feature, size=size, mode='bilinear', align_corners=False))\n",
    "    return uniform_feature_maps\n",
    "\n",
    "\n",
    "def multi_resolution_features(images,img_size,N,clip,offset,window_size,max_dist,timestep,up_ft_indices,multi_ch,multi_img_size,multi_iter):\n",
    "    \"\"\"\n",
    "    Generate multi-resolution features from images using SIFT, and Random Points.\n",
    "\n",
    "    This function processes images to generate feature maps at multiple resolutions. It combines techniques like SIFT,Random Points Sampler, and CLAHE to enhance and extract features from the images. The function can operate in either multi-channel or single-channel mode.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list of str): List of paths to the images to be processed.\n",
    "    - img_size (int): The size of the images for processing.\n",
    "    - N (int): The number of keypoints to be used in SIFT.\n",
    "    - clip (float): The clip limit for CLAHE.\n",
    "    - max_dist (float): Maximum distance for keypoint selection in SIFT.\n",
    "    - timestep (float): Timestep parameter for Diffusion Model initialization.\n",
    "    - up_ft_indices (list): Indices for feature upsampling in the Diffusion Model.\n",
    "    - multi_ch (bool): Flag to indicate multi-channel mode.\n",
    "    - multi_img_size (int): The size of the images for multi-resolution processing.\n",
    "    - multi_iter (int): Number of iterations for multi-resolution processing.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple of source and target feature tensors.\n",
    "    \"\"\"\n",
    "    if multi_ch:\n",
    "        src_fts,trg_fts =[],[]\n",
    "        for i in range(multi_iter):\n",
    "            sks,pts = SIFT_top_n_keypoints(images[0],N,multi_img_size*(i+1),max_dist)\n",
    "            pts = pts+select_random_points(images[0],N,multi_img_size*(i+1),offset,window_size)\n",
    "            if clip > 0:\n",
    "                images = CLAHE_Images(images, clip = clip)\n",
    "            dft = DFT(images,multi_img_size*(i+1),pts)\n",
    "            src_ft1,trg_ft1 = dft.feature_upsampling(RetinaRegNet_Intialization(images,multi_img_size*(i+1),timestep,up_ft_indices))\n",
    "            src_fts.append(src_ft1)\n",
    "            trg_fts.append(trg_ft1)\n",
    "        src_fts = Feature_padding(src_fts,(img_size,img_size))\n",
    "        trg_fts = Feature_padding(trg_fts,(img_size,img_size))\n",
    "        src_ft = torch.cat(src_fts, dim=1)\n",
    "        trg_ft = torch.cat(trg_fts, dim=1)\n",
    "    else:\n",
    "        sks,pts = SIFT_top_n_keypoints(images[0],N,img_size,max_dist)\n",
    "        pts = pts+select_random_points(images[0],N,img_size,offset,window_size)\n",
    "        if clip > 0:\n",
    "            images = CLAHE_Images(images, clip = clip)\n",
    "        dft = DFT(images,img_size,pts)\n",
    "        src_ft,trg_ft = dft.feature_upsampling(RetinaRegNet_Intialization(images,img_size,timestep,up_ft_indices))\n",
    "    return src_ft,trg_ft\n",
    "\n",
    "def landmarks_condition_check(orig_images, img_size, t, uft, landmarks1, landmarks2, max_tries=2, num=100, clip = 1.0, N = 100,offset=0.01,window_size=51,iccl=3,outlier_cond='affine',thresh=20):\n",
    "    \"\"\"\n",
    "    Iteratively attempts to improve image registration quality by enhancing image contrast and adjusting landmarks\n",
    "    until certain quality conditions are met or a maximum number of attempts is reached. This function applies CLAHE\n",
    "    for image contrast enhancement and uses various feature transformation and scaling techniques to improve the accuracy\n",
    "    of landmark correspondences between two images.\n",
    "\n",
    "    Parameters:\n",
    "        orig_images (list of str): Paths to the original images to be processed.\n",
    "        img_size (int): Size of the images to be processed, assumed to be square.\n",
    "        t (float): Threshold parameter for initializing the Diffusion Model.\n",
    "        uft (float): Parameter for extracting diffusion features from the diffusion model.\n",
    "        landmarks1 (list of tuples): Initial landmarks as (x, y) coordinates in the first image.\n",
    "        landmarks2 (list of tuples): Target landmarks as (x, y) coordinates in the second image.\n",
    "        max_tries (int, optional): Maximum number of attempts to improve image registration. Defaults to 2.\n",
    "        num (int, optional): Minimum required number of landmarks. Defaults to 100.\n",
    "        clip (float, optional): Clip limit for CLAHE. Defaults to 1.0.\n",
    "        N (int, optional): Number of points to be chosen at random for processing. Defaults to 100.\n",
    "        offset (float, optional): Offset used in point selection to avoid edge effects. Defaults to 0.01.\n",
    "        window_size (int, optional): Size of the window used in point selection. Defaults to 51.\n",
    "        iccl (float, optional): Inverse consistency criteria limit used in landmark filtering. Defaults to 3.\n",
    "        outlier_cond (str, optional): Condition used to determine outliers. Defaults to 'affine'.\n",
    "        thresh (float, optional): Threshold used for filtering outliers. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Depending on the success of the registration process, this function returns:\n",
    "            - The original images and the best set of landmarks found, or\n",
    "            - The original images and a set of default landmarks if conditions are not met.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the number of initial and target landmarks do not match.\n",
    "\n",
    "    Notes:\n",
    "        - This function is particularly useful in medical imaging or computer vision tasks where accurate image\n",
    "          registration is crucial for further analysis.\n",
    "        - The effectiveness of the registration process depends heavily on the quality and accuracy of the input landmarks.\n",
    "        - CLAHE and other image processing techniques may not always produce the desired results if the input images\n",
    "          are of poor quality or the initial landmarks are inaccurately defined.\n",
    "    \"\"\"\n",
    "    imgs,lim,land_marks1, land_marks2,list_landmarks_2, list_sim_scores,list_landmarks_1,temp = [], [], [], [], [],[],[],[]\n",
    "    tries, ch, = 0, 0\n",
    "    assert len(landmarks1) == len(landmarks2), f\"Points lengths are incompatible: {len(landmarks1)} != {len(landmarks2)}.\"\n",
    "    landmarks2,landmarks1 = filter_outlier_cond(landmarks2,landmarks1,outlier_cond,thresh)\n",
    "    print(len(landmarks2))\n",
    "    list_landmarks_1.append(landmarks1)\n",
    "    imgs.append(orig_images)\n",
    "    list_landmarks_2.append(landmarks2)\n",
    "    if len(landmarks2) < num:\n",
    "        print(\"Image Registration Unsuccessful for Original Set of Images\")\n",
    "        while len(land_marks2) < num and tries< max_tries:\n",
    "            print(\"Executing Trial\", tries + 1)\n",
    "            images = orig_images\n",
    "            pts = select_random_points(orig_images[0], N, img_size,offset,window_size)\n",
    "            dft = DFT(images, img_size, pts)\n",
    "            src_ft,trg_ft = dft.feature_upsampling(RetinaRegNet_Intialization(images,img_size,t + 75*tries,uft))\n",
    "            land_marks1,sim_score, land_marks2 = dft.feature_maps(src_ft,trg_ft,iccl)\n",
    "            del src_ft\n",
    "            del trg_ft\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            land_marks2,land_marks1 = filter_outlier_cond(land_marks2,land_marks1,outlier_cond,thresh)\n",
    "            list_landmarks_1.append(land_marks1)\n",
    "            imgs.append(images)\n",
    "            list_landmarks_2.append(land_marks2)\n",
    "            list_sim_scores.append(np.mean(sim_score))\n",
    "            tries += 1\n",
    "        for i in range(len(list_landmarks_2)):\n",
    "            lim.append(len(list_landmarks_2[i]))\n",
    "        idx = np.argmax(np.array(lim))\n",
    "        return orig_images,list_landmarks_1[idx],list_landmarks_2[idx]\n",
    "    else:\n",
    "        return orig_images, landmarks1, landmarks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70358021-0f5b-46b5-9cf6-84310ef5d006",
   "metadata": {
    "id": "70358021-0f5b-46b5-9cf6-84310ef5d006",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def folder_structure(path):\n",
    "    \"\"\"\n",
    "    Create a directory structure for storing image registration results.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): Base path for the directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.join(path+'_'+'Image_Registration_Results'), exist_ok=True)\n",
    "\n",
    "def subject_organization(nfn,fls):\n",
    "    \"\"\"\n",
    "    Organize subjects based on file naming conventions.\n",
    "\n",
    "    Parameters:\n",
    "    - nfn (list): List of subject names.\n",
    "    - fls (list): List of filenames.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary with subjects as keys and their corresponding files as values.\n",
    "    \"\"\"\n",
    "    result_lists = {f'Subject_{i + 1}': [] for i in range(len(nfn))}\n",
    "    for i in fls:\n",
    "        if '_'.join(map(str,i.split('.')[0].split('_')[-2:])) in nfn:\n",
    "            result_lists[str('_'.join(map(str,i.split('.')[0].split('_')[-2:])))] .append(i)\n",
    "        else:\n",
    "            continue\n",
    "    return result_lists\n",
    "\n",
    "def elements_replication(fixed, temp):\n",
    "    \"\"\"\n",
    "    Replicate elements of a list based on another list's elements.\n",
    "\n",
    "    Parameters:\n",
    "    - fixed (list): List containing elements to replicate.\n",
    "    - temp (list): List containing numbers indicating how many times to replicate each element.\n",
    "\n",
    "    Returns:\n",
    "    - list: List with replicated elements.\n",
    "    \"\"\"\n",
    "    fxd = []\n",
    "    for i in range(len(fixed)):\n",
    "        replicated_sublist = [fixed[i][0]] * temp[i]\n",
    "        fxd.append(replicated_sublist)\n",
    "    return fxd\n",
    "\n",
    "\n",
    "def data_organizing(pth,nfn,fnn,result_lists):\n",
    "    \"\"\"\n",
    "    Organize data based on filenames and subject names.\n",
    "\n",
    "    Parameters:\n",
    "    - pth (str): Base path to the dataset.\n",
    "    - nfn (list): List of subject names.\n",
    "    - fnn (list): List of file identifiers.\n",
    "    - result_lists (dict): Dictionary with subjects as keys and their corresponding files as values.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Lists containing paths to fixed images, moving images, and point files.\n",
    "    \"\"\"\n",
    "    fixed,moving,pnts,temp=[],[],[],[]\n",
    "    for i in nfn:\n",
    "        a,b,c=[],[],[]\n",
    "        for j in range(len(result_lists[i])):\n",
    "            if result_lists[i][j].split('_')[1].startswith(str(fnn[1][0])):\n",
    "                b.append(os.path.join(pth,str(i),fnn[1],result_lists[i][j]))\n",
    "            elif result_lists[i][j].startswith(str(fnn[2][0])):\n",
    "                #a += [os.path.join(pth,str(i),fnn[2],result_lists[i][j])] * N\n",
    "                a.append(os.path.join(pth,str(i),fnn[2],result_lists[i][j]))\n",
    "                #a.append(result_lists[i][j])\n",
    "            else:\n",
    "                c.append(os.path.join(pth,str(i),fnn[0],result_lists[i][j]))\n",
    "                #c.append(result_lists[i][j])\n",
    "        temp.append(len(b))\n",
    "        fixed.append(sorted(a))\n",
    "        moving.append(sorted(b))\n",
    "        pnts.append(sorted(c))\n",
    "    fixed = elements_replication(fixed,temp)\n",
    "    return fixed,moving,pnts\n",
    "\n",
    "def text_points_extraction(pnts):\n",
    "    \"\"\"\n",
    "    Extract point coordinates from a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - pnts (str): Path to the text file containing point coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Lists of fixed points and moving points.\n",
    "    \"\"\"\n",
    "    fixed_pnts = []\n",
    "    moving_pnts = []\n",
    "    with open(pnts, 'r') as file:\n",
    "        for line in file:\n",
    "            points = [float(coord) for coord in line.strip().split(',')]\n",
    "            fps = tuple(points[:2])\n",
    "            lps = tuple(points[2:])\n",
    "            fixed_pnts.append(fps)\n",
    "            moving_pnts.append(lps)\n",
    "    return fixed_pnts,moving_pnts\n",
    "\n",
    "def info_extraction(fixed,moving, pnts):\n",
    "    \"\"\"\n",
    "    Extract information from given lists to pair up images and their points.\n",
    "\n",
    "    Parameters:\n",
    "    - fixed (list): List of fixed image paths.\n",
    "    - moving (list): List of moving image paths.\n",
    "    - pnts (list): List of point file paths.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Lists of image pairs, fixed points, and moving points.\n",
    "    \"\"\"\n",
    "    images,fixed_points,moving_points=[],[],[]\n",
    "    assert len(fixed) == len(moving), f\"Some Imgaes do not have a pair: {len(fixed)} != {len(moving)}.\"\n",
    "    for i in range(len(fixed)):\n",
    "        for j in range(len(moving)):\n",
    "            if i==j:\n",
    "                for k in range(len(fixed[i])):\n",
    "                    for l in range(len(moving[j])):\n",
    "                        if k==l:\n",
    "                            images.append([moving[j][l],fixed[i][k]])\n",
    "                            p1,p2 = text_points_extraction(pnts[j][l])\n",
    "                            fixed_points.append(p1)\n",
    "                            moving_points.append(p2)\n",
    "                        else:\n",
    "                            continue\n",
    "            else:\n",
    "                continue\n",
    "    return images,fixed_points,moving_points\n",
    "\n",
    "def coordinates_processing(image1,image2,fpnts,mpnts,img_shape=256):\n",
    "    \"\"\"\n",
    "    Process and rescale coordinates for two images.\n",
    "\n",
    "    Parameters:\n",
    "    - image1 (str): Path to the first image.\n",
    "    - image2 (str): Path to the second image.\n",
    "    - fpnts (list of tuples): List of (x, y) coordinates related to the first image.\n",
    "    - mpnts (list of tuples): List of (x, y) coordinates related to the second image.\n",
    "    - img_shape (int, optional): Desired image dimension for rescaling. Default is 256.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Scaled coordinates for the first and second images.\n",
    "    \"\"\"\n",
    "    H1,W1,C1 = cv2.imread(image1).shape\n",
    "    H2,W2,C2 = cv2.imread(image2).shape\n",
    "    scaled_moving_points = coordinates_rescaling(mpnts,H1,W1,img_shape)\n",
    "    scaled_fixed_points = coordinates_rescaling(fpnts,H2,W2,img_shape)\n",
    "    scaled_original_moving_points = coordinates_rescaling(mpnts,H1,W1,max(max(H1,W1),max(H2,W2)))\n",
    "    return (H2,W2),max(max(H1,W1),max(H2,W2)),scaled_fixed_points,scaled_moving_points,scaled_original_moving_points\n",
    "\n",
    "\n",
    "\n",
    "def feature_scaling(images,fixed_points,moving_points,img_shape):\n",
    "    \"\"\"\n",
    "    Apply feature scaling to given images and their associated points.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list): List of image paths.\n",
    "    - fixed_points (list): List of fixed points.\n",
    "    - moving_points (list): List of moving points.\n",
    "    - img_shape (int): Desired image dimension for rescaling.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Scaled fixed and moving points.\n",
    "    \"\"\"\n",
    "    fixed_image_size,max_image_size,fixed_pointss,moving_pointss,scaled_moving_points =[],[],[],[],[]\n",
    "    for i in range(len(images)):\n",
    "        fhs,mhs,fpnts,mpnts,scmpnts = coordinates_processing(images[i][0],images[i][1],fixed_points[i],moving_points[i],img_shape)\n",
    "        fixed_image_size.append(fhs)\n",
    "        max_image_size.append(mhs)\n",
    "        fixed_pointss.append(fpnts)\n",
    "        moving_pointss.append(mpnts)\n",
    "        scaled_moving_points.append(scmpnts)\n",
    "    return fixed_image_size,max_image_size,fixed_points,fixed_pointss,moving_pointss,scaled_moving_points\n",
    "\n",
    "def data_preprocessing(path):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset from a given path by organizing and extracting relevant information.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): Path to the dataset directory.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Images, fixed points, and moving points extracted from the dataset.\n",
    "    \"\"\"\n",
    "    fls,nfn,fnn=[],[],[]\n",
    "    pth = os.path.join(os.getcwd(),path,'data')\n",
    "    for i in os.listdir(pth):\n",
    "        nfn.append(i)\n",
    "        for j in os.listdir(os.path.join(os.getcwd(),path,'data',i)):\n",
    "            fnn.append(j)\n",
    "            for k in os.listdir(os.path.join(os.getcwd(),path,'data',i,j)):\n",
    "                if k!='.ipynb_checkpoints':\n",
    "                    fls.append(k)\n",
    "                else:\n",
    "                    continue\n",
    "    folder_structure(path)\n",
    "    result= subject_organization(nfn,fls)\n",
    "    fixed,moving,pnts = data_organizing(pth,nfn,np.unique(fnn),result)\n",
    "    images,fixed_points,moving_points=info_extraction(fixed,moving,pnts)\n",
    "    return images,fixed_points,moving_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5586c68-cd9d-40c4-9d78-5def613ab482",
   "metadata": {
    "id": "b5586c68-cd9d-40c4-9d78-5def613ab482",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RetinaRegNet_Intialization(filelist,img_size = 256,timestep = 75,up_ft_index = 2):\n",
    "    \"\"\"\n",
    "    Initialize RetinaRegNet by processing a list of image files.\n",
    "\n",
    "    Parameters:\n",
    "    - filelist (list of str): List of paths to image files for feature extraction.\n",
    "    - img_size (int, optional): Desired size for resizing images. Default is 256.\n",
    "    - timestep (int, optional): Time step for the intializing the diffusion model. Default is 75.\n",
    "    - up_ft_index (int, optional): Index for the extracting diffusion features from the diffusion model . Default is 2\n",
    "\n",
    "    Returns:\n",
    "    - ft (torch.Tensor): A tensor containing the Diffusion features of the images in the list.\n",
    "\n",
    "    Notes:\n",
    "    The function uses the SDFeaturizer from the 'stabilityai/stable-diffusion-2-1' model to extract stable diffusion features\n",
    "    from each image. After processing all images, the extracted features are concatenated into a single tensor.\n",
    "    To avoid memory issues, the function cleans up resources after processing.\n",
    "    \"\"\"\n",
    "    ft = []\n",
    "    imglist = []\n",
    "    dfm = SDFeaturizer(sd_id='stabilityai/stable-diffusion-2-1')\n",
    "    for filename in filelist:\n",
    "        img = Image.open(filename).convert('RGB')\n",
    "        img = img.resize((img_size, img_size))\n",
    "        imglist.append(img)\n",
    "        img_tensor = (PILToTensor()(img) / 255.0 - 0.5) * 2\n",
    "        ft.append(dfm.forward(img_tensor,\n",
    "                               timestep,\n",
    "                               up_ft_index,\n",
    "                               prompt='FLoRI21',\n",
    "                               ensemble_size=8))\n",
    "    ft = torch.cat(ft, dim=0)\n",
    "\n",
    "    del dfm\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ccf60c-2abc-4ef2-9de2-87bf121691b9",
   "metadata": {
    "id": "03ccf60c-2abc-4ef2-9de2-87bf121691b9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(images,rpth,ifn,img_size=256,up_ft_indices = 1,timestep = 75,N=50,offset=0.01,window_size=51,max_dist =5,iccl=3,outlier_cond='affine',thresh=20,max_tries=3,num=50,clip = 1.0, multi_ch=True,multi_iter=3, multi_img_size=256):\n",
    "    \"\"\"\n",
    "    Perform image registration and point correspondence using a series of processing steps.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list): A list of input images for registration.\n",
    "    - rpth (str): Path to save the resulting registered images.\n",
    "    - ifn (str): File name prefix for the saved images.\n",
    "    - img_size (int, optional): Size of the input images (default is 256).\n",
    "    - up_ft_indices (int, optional): Up-sampling factor for feature indices (default is 1).\n",
    "    - timestep (int, optional): Time step for feature extraction (default is 75).\n",
    "    - N (int, optional): Number of keypoints to extract (default is 50).\n",
    "    - offset (float, optional): Offset parameter for feature extraction (default is 0.01).\n",
    "    - window_size (int, optional): Size of the window for feature extraction (default is 51).\n",
    "    - max_dist (int, optional): Maximum distance for feature matching (default is 5).\n",
    "    - iccl (int, optional): ICC level for feature matching (default is 3).\n",
    "    - outlier_cond (str, optional): Condition for outlier removal (default is 'affine').\n",
    "    - thresh (int, optional): Threshold value for outlier removal (default is 20).\n",
    "    - max_tries (int, optional): Maximum number of attempts for matching features (default is 3).\n",
    "    - num (int, optional): Number of iterations for matching features (default is 50).\n",
    "    - clip (float, optional): Clip parameter for image enhancement (default is 1.0).\n",
    "    - multi_ch (bool, optional): Flag indicating whether to use multi-channel processing (default is True).\n",
    "    - multi_iter (int, optional): Number of iterations for multi-channel processing (default is 3).\n",
    "    - multi_img_size (int, optional): Size of images for multi-channel processing (default is 256).\n",
    "\n",
    "    Returns:\n",
    "    - original (list): List of original image points.\n",
    "    - computed (list): List of computed image points after registration.\n",
    "\n",
    "    Note:\n",
    "    - This function performs various processing steps including feature extraction, feature matching,\n",
    "      outlier removal, and image registration.\n",
    "    - It saves the resulting registered images in the specified directory.\n",
    "    - If the image registration is unsuccessful, empty lists are returned for both original and computed points.\n",
    "    \"\"\"\n",
    "    sks,pts = SIFT_top_n_keypoints(images[0],N,img_size,max_dist)\n",
    "    pts = pts+select_random_points(images[0],N,img_size)\n",
    "    if clip > 0:\n",
    "            images = CLAHE_Images(images, clip = clip)\n",
    "    dft = DFT(images,img_size,pts)\n",
    "    src_ft,trg_ft = multi_resolution_features(images,img_size,N,clip,offset,window_size,max_dist,timestep,up_ft_indices,multi_ch,multi_img_size,multi_iter)\n",
    "    pnts,rmaxs, rspts = dft.feature_maps(src_ft,trg_ft,iccl)\n",
    "    del src_ft\n",
    "    del trg_ft\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    images,original,computed = landmarks_condition_check(images, img_size, timestep, up_ft_indices, pnts, rspts, max_tries, num, clip, N, offset, window_size, iccl, outlier_cond, thresh)\n",
    "    if len(computed)!=0:\n",
    "        image_point_correspondences(images,img_size,pts,original,computed,rpth,ifn)\n",
    "        return original,computed\n",
    "    else:\n",
    "        print(\"Image Registration is Unsuccessful for the presented Images due to unsufficent Matching Features\")\n",
    "        return [],[]\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27acd5-cc0b-4796-b7b7-b6d0de573c1c",
   "metadata": {
    "id": "6c27acd5-cc0b-4796-b7b7-b6d0de573c1c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_size= 1024\n",
    "images,fixed_points,moving_points = data_preprocessing('FLoRI21_DataPort')\n",
    "fixed_image_size,max_image_size,fixed_points,scaled_fixed_points,scaled_moving_points,scaled_original_moving_points  = feature_scaling(images,fixed_points,moving_points,img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc9bf50-7221-4fac-9f44-fd9b198490f8",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b7e5dbc433934952a015d69341d06a12",
      "d9be9753dd5d414da55ffcd4b51a9fdb",
      "8ce2686d2276426489f37b91cee6c1c2",
      "2a1339bea9a94ccf987b1a4fbd902fa5",
      "079f4326021e4dcf9beb340d2078b111",
      "9b302fbe38ed499382f0f18806424fe0",
      "b416f69eafa744b7a1d9f534078d37e1",
      "f9fcad6f6bbf4a60b0293269979c9474",
      "b672e79322d6473386d8058231b33fd4",
      "939df72d8ae14ad6acd30eb21a369162",
      "c18cee046dc84b039914150873514770",
      "4debb1ef087b4a538b20655907b39416",
      "76a73346af6f4faaa107c5930d80fea8",
      "73c22e5725ed430e9e7f571fec0a85e7",
      "be6c792d4b8e4a7f99898342a78c6055",
      "9a57288ff4544c6e9191aa2f2d4fb35f",
      "8e214ab4cf6a415faff5d6154b83f484",
      "2bc4fc18dc3946db9d33769aca99c317",
      "45d4e0efae0a49aea703c11ad73ebf97",
      "98e9ebe19d8c440d93a5a1f4f1b2adf6",
      "9b3f3fc59a0140ba94c71e951fc0193f",
      "53651df8e0d74ba489adfa64ceca2a83",
      "c91f70c7a09e46a39e04e87a4e68a3be",
      "187a0c0296b24986ae1701fd9f86bc4d",
      "e94af91fceea4d8cad96d3658774f479",
      "db062f9f2d634a95bc0e6dc268d1a7db",
      "160b9f691eca4fa2943eb5e4213fa933",
      "9b274208604f4983b6a7ff5451cc1403",
      "dbaa1d60efa548eeb0ac9c800d55a8dd",
      "384bb899d16345328f9eebe6a49d56bf"
     ]
    },
    "id": "5cc9bf50-7221-4fac-9f44-fd9b198490f8",
    "outputId": "fb7afe06-0147-4aa3-d59c-bb71e21f8e34",
    "tags": []
   },
   "outputs": [],
   "source": [
    "landmark_errors=[]\n",
    "for i in range(len(images)):\n",
    "    print(\"Iteration {}\".format(i+1))\n",
    "    print(\"Loading Source Images {0} ,Target Image{1} to the framework\".format(images[i][1],images[i][0]))\n",
    "    original_low_res,computed_low_res = main(images[i],os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results','Homography_Results'),str(i),img_size,up_ft_indices = 2,timestep = 1,N=1000,offset=0.01,window_size=51,max_dist = 5,iccl=3,outlier_cond='affine',thresh=40, max_tries=2,num=100,clip = 0.0,multi_ch=False,multi_iter=5, multi_img_size=256)\n",
    "    imags,imgs,homography_matrix_low_res = compute_and_apply_homography(images[i][::-1], img_size,original_low_res,computed_low_res,os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results','Homography_Results'),str(i))\n",
    "    if len(homography_matrix_low_res) !=0:\n",
    "        transformed_points_hom = transform_points_homography(scaled_moving_points[i],homography_matrix_low_res)\n",
    "        transformed_points_high_res_hom =  coordinates_rescaling(transformed_points_hom,img_size,img_size,max_image_size[i])\n",
    "        original_low_res,computed_low_res = main(imags,os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results','Polynomial_Results'),str(i),img_size,up_ft_indices = 2,timestep = 1,N=1000,offset=0.01,window_size=51,max_dist = 5,iccl=3,outlier_cond='affine',thresh=30, max_tries=2,num=100,clip = 0.0,multi_ch=False,multi_iter=5, multi_img_size=256)\n",
    "        imag,imgs,quadratic_matrix_low_res = compute_third_order_polynomial_matrix_and_plot(imags[::-1], img_size,original_low_res,computed_low_res,os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results','Polynomial_Results'),str(i))\n",
    "        if len(quadratic_matrix_low_res) !=0:\n",
    "            ## rescaled version for dispaly purposes\n",
    "            transformed_points_poly = transform_points_third_order_polynomial(transformed_points_hom, quadratic_matrix_low_res)\n",
    "            original_image_point_correspondences(imag,img_size, scaled_fixed_points[i], transformed_points_hom, transformed_points_poly,os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results','Homography_Polynomial_Results'), str(i))\n",
    "            ### Original Version for computation of errors\n",
    "            polynomial_matrix = transform_points_third_order_polynomial_matrix(original_low_res,computed_low_res,img_size,max_image_size[i])\n",
    "            error = compute_landmark_error_fixed_space(polynomial_matrix,fixed_points[i],transformed_points_high_res_hom,max_image_size[i],fixed_image_size[i])\n",
    "            print(\"Recorded Landmark Error for Iteration {0} is {1}\".format(i+1,error))\n",
    "            landmark_errors.append(error)\n",
    "        else:\n",
    "            landmark_errors.append(10000)\n",
    "    else:\n",
    "        landmark_errors.append(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a9a70-7255-43fd-8291-2d71e3371fb3",
   "metadata": {
    "id": "759a9a70-7255-43fd-8291-2d71e3371fb3",
    "outputId": "1b8c4fad-096f-434e-f427-729ce56343db"
   },
   "outputs": [],
   "source": [
    "plot_landmark_errors(landmark_errors,os.path.join(os.getcwd(),'FLoRI21_DataPort_Image_Registration_Results'),'All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d38b4-9a19-49aa-a76e-e4ee372c2f16",
   "metadata": {
    "id": "0a3d38b4-9a19-49aa-a76e-e4ee372c2f16",
    "outputId": "a786d177-eadb-4218-bd40-6ad8b89fb871",
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_plot_Flori21_AUC(landmark_errors)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "VBS_HRC",
   "language": "python",
   "name": "vbs_hrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
