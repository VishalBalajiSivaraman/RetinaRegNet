{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a794e5-1426-49e3-9f1e-7a82bb4b2532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from random import sample\n",
    "from pyunpack import Archive\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.ndimage import map_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371a0b2-22f9-4cd9-8c16-386359eef172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.modules.utils as nn_utils\n",
    "from torchvision.transforms import PILToTensor\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "from diffusers.models.unet_2d_condition import UNet2DConditionModel\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers import StableDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ff0ab-fb80-4a00-b660-3956e611adc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Archive('FIRE.7z').extractall(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08f320-69a8-4a80-931f-adae8d3e5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.rmtree(os.path.join(os.getcwd(),'FIRE_Image_Registration_Results'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9321b34-4953-4f1d-9900-be2bd396823a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),'FIRE_Image_Registration_Results/Stage1') \n",
    "os.makedirs(path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1df1ad-a4fe-4a28-8a27-311c1021dfd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),'FIRE_Image_Registration_Results/Stage2') \n",
    "os.makedirs(path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5cfb19-d74e-4e51-9c51-75c9c937c56f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(),'FIRE_Image_Registration_Results/Final_Registration_Results') \n",
    "os.makedirs(path,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7246c00-eeb6-4cb1-aef3-b68ff9830065",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Some of the code cells were referenced from the paper titled \"Emergent Correspondence from Image Diffusion.\" Please cite their paper as follows:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{tang2023emergent,\n",
    "  title={Emergent Correspondence from Image Diffusion},\n",
    "  author={Luming Tang and Menglin Jia and Qianqian Wang and Cheng Perng Phoo and Bharath Hariharan},\n",
    "  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n",
    "  year={2023},\n",
    "  url={https://openreview.net/forum?id=ypOiXjdfnU}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec4032-76c5-4fef-8bf3-c8b91630b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyUNet2DConditionModel(UNet2DConditionModel):\n",
    "    \"\"\"\n",
    "    Customized 2D U-Net conditioned model inherited from `UNet2DConditionModel`.\n",
    "\n",
    "    This model extends the original `UNet2DConditionModel` to incorporate additional conditioning mechanisms\n",
    "    such as encoder hidden states, attention mask, and cross-attention keyword arguments.\n",
    "    \"\"\"\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        up_ft_indices,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        class_labels: Optional[torch.Tensor] = None,\n",
    "        timestep_cond: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Forward method for `MyUNet2DConditionModel`.\n",
    "\n",
    "        Args:\n",
    "            sample (torch.FloatTensor): Noisy inputs tensor with shape (batch, channel, height, width).\n",
    "            timestep (torch.FloatTensor or float or int): Timesteps for each batch.\n",
    "            up_ft_indices (list): List of upsampling indices.\n",
    "            encoder_hidden_states (torch.FloatTensor): Encoder hidden states with shape (batch, sequence_length, feature_dim).\n",
    "            class_labels (Optional[torch.Tensor], default=None): Class labels tensor.\n",
    "            timestep_cond (Optional[torch.Tensor], default=None): Timestep condition tensor.\n",
    "            attention_mask (Optional[torch.Tensor], default=None): Mask to avoid attention to certain positions.\n",
    "            cross_attention_kwargs (Optional[dict], default=None): Keyword arguments passed along to the `AttnProcessor`.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing upsampled features (`up_ft`).\n",
    "        \"\"\"\n",
    "\n",
    "        # By default samples have to be AT least a multiple of the overall upsampling factor.\n",
    "        # The overall upsampling factor is equal to 2 ** (# num of upsampling layears).\n",
    "        # However, the upsampling interpolation output size can be forced to fit any upsampling size\n",
    "        # on the fly if necessary.\n",
    "        default_overall_up_factor = 2**self.num_upsamplers\n",
    "\n",
    "        # upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`\n",
    "        forward_upsample_size = False\n",
    "        upsample_size = None\n",
    "\n",
    "        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n",
    "            # logger.info(\"Forward upsample size to force interpolation output size.\")\n",
    "            forward_upsample_size = True\n",
    "\n",
    "        # prepare attention_mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1 - attention_mask.to(sample.dtype)) * -10000.0\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb, timestep_cond)\n",
    "\n",
    "        if self.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\"class_labels should be provided when num_class_embeds > 0\")\n",
    "\n",
    "            if self.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "            emb = emb + class_emb\n",
    "\n",
    "        # 2. pre-process\n",
    "        sample = self.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(\n",
    "                sample,\n",
    "                emb,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                cross_attention_kwargs=cross_attention_kwargs,\n",
    "            )\n",
    "\n",
    "        # 5. up\n",
    "        up_ft = {}\n",
    "        for i, upsample_block in enumerate(self.up_blocks):\n",
    "\n",
    "            if i > np.max(up_ft_indices):\n",
    "                break\n",
    "\n",
    "            is_final_block = i == len(self.up_blocks) - 1\n",
    "\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            # if we have not reached the final block and need to forward the\n",
    "            # upsample size, we do it here\n",
    "            if not is_final_block and forward_upsample_size:\n",
    "                upsample_size = down_block_res_samples[-1].shape[2:]\n",
    "\n",
    "            if hasattr(upsample_block, \"has_cross_attention\") and upsample_block.has_cross_attention:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    res_hidden_states_tuple=res_samples,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    upsample_size=upsample_size,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=upsample_size\n",
    "                )\n",
    "\n",
    "            if i in up_ft_indices:\n",
    "                up_ft[i] = sample.detach()\n",
    "\n",
    "        output = {}\n",
    "        output['up_ft'] = up_ft\n",
    "        return output\n",
    "\n",
    "class OneStepSDPipeline(StableDiffusionPipeline):\n",
    "    \"\"\"\n",
    "    One-step Stable Diffusion Pipeline.\n",
    "\n",
    "    Provides a one-step stable diffusion process, integrating the VAE encoding and U-Net based sampling.\n",
    "    \"\"\"\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        img_tensor,\n",
    "        t,\n",
    "        up_ft_indices,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Call method for `OneStepSDPipeline`.\n",
    "\n",
    "        Args:\n",
    "            img_tensor (torch.Tensor): Image tensor.\n",
    "            t (torch.Tensor or int): Timesteps tensor.\n",
    "            up_ft_indices (list): List of upsampling indices.\n",
    "            negative_prompt (Optional[str or list], default=None): Negative prompts.\n",
    "            generator (Optional[torch.Generator or list], default=None): Torch generator for random sampling.\n",
    "            prompt_embeds (Optional[torch.FloatTensor], default=None): Precomputed prompt embeddings.\n",
    "            callback (Optional[Callable], default=None): Callback function invoked during diffusion.\n",
    "            callback_steps (int, default=1): Frequency of invoking the callback.\n",
    "            cross_attention_kwargs (Optional[dict], default=None): Keyword arguments for cross-attention.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing output from U-Net.\n",
    "        \"\"\"\n",
    "        device = self._execution_device\n",
    "        latents = self.vae.encode(img_tensor).latent_dist.sample() * self.vae.config.scaling_factor\n",
    "        t = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        noise = torch.randn_like(latents).to(device)\n",
    "        latents_noisy = self.scheduler.add_noise(latents, noise, t)\n",
    "        unet_output = self.unet(latents_noisy,\n",
    "                               t,\n",
    "                               up_ft_indices,\n",
    "                               encoder_hidden_states=prompt_embeds,\n",
    "                               cross_attention_kwargs=cross_attention_kwargs)\n",
    "        return unet_output\n",
    "\n",
    "\n",
    "class SDFeaturizer:\n",
    "    \"\"\"\n",
    "    Stable Diffusion Featurizer.\n",
    "\n",
    "    Provides a mechanism to compute stable diffusion based features from an input image, conditioned on a given prompt.\n",
    "    \"\"\"\n",
    "    def __init__(self, sd_id='stabilityai/stable-diffusion-2-1'):\n",
    "        \"\"\"\n",
    "        Initializes `SDFeaturizer` with a given stable diffusion model ID.\n",
    "\n",
    "        Args:\n",
    "            sd_id (str, default='stabilityai/stable-diffusion-2-1'): Stable diffusion model ID to be used for featurization.\n",
    "        \"\"\"\n",
    "        unet = MyUNet2DConditionModel.from_pretrained(sd_id, subfolder=\"unet\")\n",
    "        onestep_pipe = OneStepSDPipeline.from_pretrained(sd_id, unet=unet, safety_checker=None)\n",
    "        onestep_pipe.vae.decoder = None\n",
    "        onestep_pipe.scheduler = DDIMScheduler.from_pretrained(sd_id, subfolder=\"scheduler\")\n",
    "        gc.collect()\n",
    "        onestep_pipe = onestep_pipe.to(\"cuda\")\n",
    "        onestep_pipe.enable_attention_slicing()\n",
    "        onestep_pipe.enable_xformers_memory_efficient_attention()\n",
    "        self.pipe = onestep_pipe\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self,\n",
    "                img_tensor, # single image, [1,c,h,w]\n",
    "                t,\n",
    "                up_ft_index,\n",
    "                prompt,\n",
    "                ensemble_size=8):\n",
    "        \"\"\"\n",
    "        Forward method for `SDFeaturizer`.\n",
    "\n",
    "        Args:\n",
    "            img_tensor (torch.Tensor): Single input image tensor with shape [1, c, h, w].\n",
    "            t (torch.Tensor or int): Timesteps tensor.\n",
    "            up_ft_index (int): Index for upsampling.\n",
    "            prompt (str): Textual prompt for conditioning.\n",
    "            ensemble_size (int, default=8): Size of the ensemble for feature averaging.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Stable diffusion based features with shape [1, c, h, w].\n",
    "        \"\"\"\n",
    "        img_tensor = img_tensor.repeat(ensemble_size, 1, 1, 1).cuda() # ensem, c, h, w\n",
    "        prompt_embeds = self.pipe._encode_prompt(\n",
    "            prompt=prompt,\n",
    "            device='cuda',\n",
    "            num_images_per_prompt=1,\n",
    "            do_classifier_free_guidance=False) # [1, 77, dim]\n",
    "        prompt_embeds = prompt_embeds.repeat(ensemble_size, 1, 1)\n",
    "        unet_ft_all = self.pipe(\n",
    "            img_tensor=img_tensor,\n",
    "            t=t,\n",
    "            up_ft_indices=[up_ft_index],\n",
    "            prompt_embeds=prompt_embeds)\n",
    "        unet_ft = unet_ft_all['up_ft'][up_ft_index] # ensem, c, h, w\n",
    "        unet_ft = unet_ft.mean(0, keepdim=True) # 1,c,h,w\n",
    "        return unet_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b9ec8-12b2-41c1-ba67-52cbcbbd9633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFT:\n",
    "    \"\"\"\n",
    "    RetinaRegNet (RetinaRegNetwork) utilizes DFT (Diffusion Features) for identifying vital key feature correlations\n",
    "    and locations between images.\n",
    "    \"\"\"\n",
    "    def __init__(self, imgs,img_size,pts):\n",
    "        \"\"\"\n",
    "        Initialize the DFT object.\n",
    "\n",
    "        Parameters:\n",
    "        - imgs (list): List of input image tensors.\n",
    "        - img_size (int): Expected size of the image for processing.\n",
    "        - pts (list): List of point tuples specifying coordinates.\n",
    "        \"\"\"\n",
    "        self.pts = pts\n",
    "        self.imgs = imgs\n",
    "        self.num_imgs = len(imgs)\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def unravel_index(self,index, shape):\n",
    "        \"\"\"\n",
    "        Converts a flat index into a tuple of coordinate indices in a tensor of the specified shape.\n",
    "\n",
    "        This function mimics numpy's `unravel_index` functionality, which is used to convert a flat index\n",
    "        into a tuple of coordinate indices for an array of given shape. This is useful for finding the original\n",
    "        multi-dimensional indices of a position in a flattened array.\n",
    "\n",
    "        Parameters:\n",
    "        - index (int): The flat index into the array.\n",
    "        - shape (tuple of ints): The shape of the array from which the index is derived.\n",
    "\n",
    "        Returns:\n",
    "        - tuple of ints: A tuple representing the coordinates of the index in an array of the specified shape.\n",
    "\n",
    "        Note:\n",
    "            This function operates under the assumption that indexing starts from 0, which is standard in Python.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for dim in reversed(shape):\n",
    "            out.append(index % dim)\n",
    "            index = index // dim\n",
    "        return tuple(reversed(out))\n",
    "\n",
    "    def compute_pooled_and_combining_feature_maps(self,feature_map, hierarchy_range=1, stride=1):\n",
    "        \"\"\"\n",
    "        Compute pooled and stacked feature maps.\n",
    "\n",
    "        Parameters:\n",
    "        - feature_map (torch.Tensor): Input feature map.\n",
    "        - hierarchy_range (int, optional): Depth of hierarchical pooling. Defaults to 3.\n",
    "        - stride (int, optional): Stride for pooling. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Pooled and stacked feature map.\n",
    "        \"\"\"\n",
    "        # List to store the pooled feature maps\n",
    "        pooled_feature_maps = feature_map\n",
    "        # Loop through the specified hierarchy range\n",
    "        for hierarchy in range(1,hierarchy_range):\n",
    "            # Average pooling with kernel size 3^k x 3^k\n",
    "            win_size = 3 ** hierarchy\n",
    "            avg_pool = torch.nn.AvgPool2d(win_size, stride=1, padding=win_size // 2, count_include_pad=False)\n",
    "            pooled_map = avg_pool(feature_map)\n",
    "            # Append the pooled feature map to the list\n",
    "            pooled_feature_maps=+pooled_map\n",
    "        return pooled_feature_maps\n",
    "\n",
    "    def compute_batched_2d_correlation_maps(self, pts_list, feature_map1, feature_map2):\n",
    "        \"\"\"\n",
    "        Computes 2D correlation maps between selected points in one feature map and another feature map.\n",
    "\n",
    "        This method takes two feature maps and a list of points. It extracts features from the first feature map\n",
    "        at specified points, normalizes them, and then computes a batched 2D correlation with the second feature map.\n",
    "        The output is a set of correlation maps, each corresponding to a point in `pts_list`, showing how that point's\n",
    "        feature vector correlates across the spatial dimensions of the second feature map.\n",
    "\n",
    "        Parameters:\n",
    "        - pts_list (list of tuples): List of points (y, x) for which the correlation map is to be computed.\n",
    "        - feature_map1 (torch.Tensor): The first feature map tensor of shape (1, C, H1, W1) where C is the number of channels.\n",
    "        - feature_map2 (torch.Tensor): The second feature map tensor of shape (1, C, H2, W2) where C is the number of channels\n",
    "                                     and H2, W2 do not necessarily need to be equal to H1, W1.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: A tensor of shape (NumPoints, H2, W2) where each slice corresponds to the correlation map\n",
    "                          for each point in `pts_list`.\n",
    "        Notes:\n",
    "            The function assumes that the first dimension of feature_map1 and feature_map2 is 1 (batch size of 1).\n",
    "            This method uses batch matrix multiplication and vector normalization for efficient computation.\n",
    "            Running this method on a GPU is recommended due to its computational and memory intensity.\n",
    "        \"\"\"\n",
    "        # Convert the input tensors to float16\n",
    "        feature_map1 = feature_map1.to(dtype=torch.float16)\n",
    "        feature_map2 = feature_map2.to(dtype=torch.float16)\n",
    "        _, C, H, W = feature_map2.shape\n",
    "\n",
    "        # Flatten feature_map2 for batch matrix multiplication\n",
    "        feature_map2_flat = feature_map2.view(C, H*W)\n",
    "\n",
    "        # Prepare a batch of point features\n",
    "        points_indices = torch.tensor(pts_list)\n",
    "        point_features = feature_map1[0, :, points_indices[:, 0], points_indices[:, 1]].transpose(0, 1)  # Shape: (NumPoints, Channels)  # Shape: (NumPoints, Channels)\n",
    "\n",
    "        # Normalize the point features and feature_map2_flat\n",
    "        point_features_norm = torch.norm(point_features, dim=1, keepdim=True)\n",
    "        normalized_point_features = point_features / point_features_norm\n",
    "\n",
    "        feature_map2_norm = torch.norm(feature_map2_flat, dim=0, keepdim=True)\n",
    "        normalized_feature_map2 = feature_map2_flat / feature_map2_norm\n",
    "\n",
    "        # Compute the correlation map for each point\n",
    "        correlation_maps = torch.mm(normalized_point_features, normalized_feature_map2)\n",
    "\n",
    "        # Reshape the correlation maps to the desired output shape (NumPoints, H, W)\n",
    "        correlation_maps = correlation_maps.view(-1, H, W)\n",
    "\n",
    "        # Cleanup if needed\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return correlation_maps\n",
    "\n",
    "    def compute_correlation_map_max_locations(self, pts_list, feature_map1, feature_map2): # heirachy range - hpo\n",
    "        \"\"\"\n",
    "        Compute the maximum locations in the batched correlation maps between two feature maps.\n",
    "\n",
    "        Parameters:\n",
    "        - pts_list (list of tuples): List of points for which the correlation maps were computed.\n",
    "        - feature_map1, feature_map2 (torch.Tensor): The input feature maps.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Tensor of maximum locations for each point.\n",
    "        - torch.Tensor: Tensor of maximum values for each point.\n",
    "        \"\"\"\n",
    "        enhanced_feature_map1 = self.compute_pooled_and_combining_feature_maps(feature_map1, hierarchy_range=1)\n",
    "        enhanced_feature_map2 = self.compute_pooled_and_combining_feature_maps(feature_map2, hierarchy_range=1)\n",
    "        # Compute the batched correlation maps\n",
    "        batched_correlation_maps = self.compute_batched_2d_correlation_maps(pts_list, enhanced_feature_map1, enhanced_feature_map2)\n",
    "\n",
    "        M,H2, W2 = batched_correlation_maps.shape\n",
    "        #print(batched_correlation_maps.shape)\n",
    "\n",
    "        # Find the maximum values and their locations along the last two dimensions for each map\n",
    "        max_values, max_indices_flat = torch.max(batched_correlation_maps.view(len(pts_list), -1), dim=-1)\n",
    "\n",
    "        x, y = zip(*[self.unravel_index(idx.item(), (H2, W2)) for idx in max_indices_flat.view(-1)])\n",
    "        x = torch.tensor(x, device = 'cuda').view(M)\n",
    "        y = torch.tensor(y, device = 'cuda').view(M)\n",
    "\n",
    "        # Stack the coordinates to get a 2xHxW tensor\n",
    "        max_locations = torch.stack((x, y)).t()\n",
    "\n",
    "        return max_locations, max_values\n",
    "\n",
    "    def feature_upsampling(self,ft):\n",
    "        \"\"\"\n",
    "        Upsample the feature to match the specified image size.\n",
    "\n",
    "        Parameters:\n",
    "        - ft (torch.Tensor): Feature tensor to be upsampled.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Upsampled source and target feature maps.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            num_channel = ft.size(1)\n",
    "            src_ft = ft[0].unsqueeze(0)\n",
    "            src_ft = nn.Upsample(size=(self.img_size, self.img_size), mode='bilinear')(src_ft)  # (1, C, H, W)\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            trg_ft = nn.Upsample(size=(self.img_size, self.img_size), mode='bilinear')(ft[1:])  # (1, C, H, W)\n",
    "        return src_ft,trg_ft\n",
    "\n",
    "    def feature_maps(self,feature_map1,feature_map2,iccl):\n",
    "        \"\"\"\n",
    "        Processes feature maps to extract points that meet the inverse consistency criteria between two images.\n",
    "\n",
    "        This method computes the maximum locations of correlation between feature maps of two images and \n",
    "        checks for inverse consistency between the mapped points. It filters these points based on the \n",
    "        specified inverse consistency criteria limit (iccl), keeping only those pairs where the \n",
    "        distance between the original point and its double-mapped location is within the threshold.\n",
    "\n",
    "        Parameters:\n",
    "        - feature_map1 (torch.Tensor): The first feature map, used as the base for initial correlations.\n",
    "        - feature_map2 (torch.Tensor): The second feature map, used for reverse correlations to check consistency.\n",
    "        - iccl (float): The maximum allowed distance (inverse consistency criteria limit) for a point and \n",
    "                      its double-mapped location to be considered consistent.\n",
    "\n",
    "        Returns:\n",
    "        tuple of (list, list, list):\n",
    "        - pnts (list of tuples): The points from the original feature map that meet the inverse consistency criteria.\n",
    "        - rmaxs (list of floats): The maximum correlation values at these points.\n",
    "        - rspts (list of tuples): The corresponding points in the second feature map that have the highest correlation \n",
    "                                  with the points in `pnts`.\n",
    "        \"\"\"\n",
    "        pnts,rmaxs,rspts=[],[],[]\n",
    "        pts = [(int(y), int(x)) for x, y in self.pts]\n",
    "        max_indices_ST, max_values_ST = self.compute_correlation_map_max_locations(pts,feature_map1,feature_map2)\n",
    "        x_prime_y_prime = max_indices_ST\n",
    "        max_indices_TS, max_values_TS = self.compute_correlation_map_max_locations(max_indices_ST,feature_map2,feature_map1)\n",
    "        x_prime_prime_y_prime_prime = max_indices_TS\n",
    "        for i, (pt, max_idx) in enumerate(zip(self.pts, x_prime_prime_y_prime_prime)):\n",
    "            # Calculate the distance between the point and the max correlation index\n",
    "            if np.sqrt((pt[1] - max_idx.cpu()[0]) ** 2 + (pt[0] - max_idx.cpu()[1]) ** 2) <=iccl: ### inverse consistency criteria\n",
    "                pnts.append((int(pt[0]), int(pt[1])))\n",
    "                rmaxs.append(max_values_ST[i].cpu().item())  # Assuming max_values_ST is a tensor with corresponding max values\n",
    "                rspts.append((x_prime_y_prime[i][1].cpu().item(), x_prime_y_prime[i][0].cpu().item()))  # Assuming x_prime_y_prime has corresponding max index locations\n",
    "        return pnts, rmaxs, rspts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3a49f7-f12d-41c4-a5c0-bfd706959604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_boundary(image, mean_intensity):\n",
    "    \"\"\"\n",
    "    Compute the boundary of an image based on its mean intensity.\n",
    "\n",
    "    Parameters:\n",
    "    - image (numpy.array): The input grayscale image.\n",
    "    - mean_intensity (float): Average intensity of the image to define boundaries.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: upper, lower, left, and right boundaries of the image region with intensities above mean_intensity.\n",
    "    \"\"\"\n",
    "    # Compute the upper, lower, left, and right boundary\n",
    "    upper_boundary = next((i for i, row in enumerate(image) if np.mean(row) > mean_intensity), 0)\n",
    "    lower_boundary = next((i for i, row in enumerate(image[::-1]) if np.mean(row) > mean_intensity), 0)\n",
    "\n",
    "    left_boundary = next((i for i, col in enumerate(image.T) if np.mean(col) > mean_intensity), 0)\n",
    "    right_boundary = next((i for i, col in enumerate(image.T[::-1]) if np.mean(col) > mean_intensity), 0)\n",
    "\n",
    "    return upper_boundary, image.shape[0]-lower_boundary, left_boundary, image.shape[1]-right_boundary\n",
    "\n",
    "def is_within_boundary(kp, boundaries):\n",
    "    \"\"\"\n",
    "    Check if a keypoint is within the specified boundaries.\n",
    "\n",
    "    Parameters:\n",
    "    - kp (cv2.KeyPoint): The keypoint to check.\n",
    "    - boundaries (tuple): Tuple of (upper, lower, left, right) boundaries.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the keypoint is within the boundaries, False otherwise.\n",
    "    \"\"\"\n",
    "    upper, lower, left, right = boundaries\n",
    "    return left <= kp.pt[0] <= right and upper <= kp.pt[1] <= lower\n",
    "\n",
    "def SIFT_top_n_keypoints(image_path, N=250, img_shape=256, max_dist=25):\n",
    "    \"\"\"\n",
    "    Detect top N keypoints in the given image using SIFT, considering constraints on distance, boundary, and collinearity.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (str): Path to the input image.\n",
    "    - N (int): Number of keypoints to select. Defaults to 250.\n",
    "    - img_shape (int): The size to which the image should be resized. Defaults to 256.\n",
    "    - max_dist (int): Minimum distance between selected keypoints. Defaults to 25.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of keypoints' positions in the form (x, y).\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (img_shape, img_shape))\n",
    "\n",
    "    # Initialize SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "\n",
    "    # Sort keypoints based on response (strength of the keypoint)\n",
    "    keypoints = sorted(keypoints, key=lambda x: -x.response)\n",
    "\n",
    "    # Determine the intensity threshold\n",
    "    mean_intensity = np.mean(image)\n",
    "    boundaries = compute_boundary(image, mean_intensity)\n",
    "\n",
    "    # Select top N keypoints\n",
    "    selected_keypoints = []\n",
    "    for keypoint in keypoints:\n",
    "        # Check if the keypoint is within the boundary\n",
    "        if is_within_boundary(keypoint, boundaries):\n",
    "            # Check if the pixel intensity at the keypoint is greater than the threshold (not black)\n",
    "            if image[int(keypoint.pt[1]), int(keypoint.pt[0])] > mean_intensity:\n",
    "                # Check if the keypoint is far from existing selected keypoints\n",
    "                if all(cv2.norm(np.array(keypoint.pt) - np.array(kp.pt)) > max_dist for kp in selected_keypoints):\n",
    "                    selected_keypoints.append(keypoint)\n",
    "\n",
    "            # Break if N keypoints are selected\n",
    "            if len(selected_keypoints) == N:\n",
    "                break\n",
    "\n",
    "    return [kp.pt for kp in selected_keypoints]\n",
    "\n",
    "def select_random_points(img, num_points=100, img_size=1200,offset=0.01,window_size = 51,max_attempts_per_point=50):\n",
    "    \"\"\"\n",
    "    Selects a specified number of random points from an image, ensuring that each point is centered in a region \n",
    "    meeting a defined intensity threshold within the image. The image is resized to a specified size, and points \n",
    "    are chosen randomly, with each potential point undergoing validation against criteria before being accepted.\n",
    "\n",
    "    Parameters:\n",
    "    - img (str): Path to the image file.\n",
    "    - num_points (int, optional): The number of random points to select. Defaults to 100.\n",
    "    - img_size (int, optional): The size to which the image is resized (assumed square). Defaults to 1200.\n",
    "    - offset (float, optional): Proportional offset to exclude points near the edges, represented as a fraction of \n",
    "                              the image dimensions. Defaults to 0.01.\n",
    "    - window_size (int, optional): Size of the square window used to check pixel intensity around each point. \n",
    "                                 Defaults to 51.\n",
    "    - max_attempts_per_point (int, optional): The maximum number of attempts allowed to find a suitable point \n",
    "                                            that meets the criteria. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: A list where each tuple represents the (y, x) coordinates of a selected point.\n",
    "\n",
    "    Notes:\n",
    "        The function converts the image to grayscale and resizes it to img_size x img_size. It avoids selecting \n",
    "        points near the image boundary by applying a boundary offset calculated from the 'offset' parameter. \n",
    "        Each point must be centered in a window (defined by 'window_size') where all pixels have an intensity \n",
    "        greater than or equal to 5. If the function fails to find a suitable point after 'max_attempts_per_point' \n",
    "        for any location, it stops and returns the points found up to that moment.\n",
    "    \"\"\"\n",
    "\n",
    "    image = cv2.resize(cv2.imread(img, cv2.IMREAD_GRAYSCALE), (img_size, img_size))\n",
    "    h, w = image.shape\n",
    "    boundary_offset = int(offset * h)\n",
    "    pts = []\n",
    "    window_offset = window_size // 2  # Calculate the offset from the center of the window\n",
    "\n",
    "    while len(pts) < num_points:\n",
    "        attempts = 0\n",
    "        while attempts < max_attempts_per_point:\n",
    "            x = random.randint(boundary_offset + window_offset, h - boundary_offset - window_offset - 1)\n",
    "            y = random.randint(boundary_offset + window_offset, w - boundary_offset - window_offset - 1)\n",
    "\n",
    "            # Define the window boundaries\n",
    "            x_lower = x - window_offset\n",
    "            x_upper = x + window_offset + 1\n",
    "            y_lower = y - window_offset\n",
    "            y_upper = y + window_offset + 1\n",
    "\n",
    "            # Check that no pixel in the window has an intensity less than 10\n",
    "            if np.all(image[x_lower:x_upper, y_lower:y_upper] >= 5):\n",
    "                pts.append((y, x))\n",
    "                break  # Successfully found a point, break the inner loop\n",
    "            attempts += 1  # Increment attempts\n",
    "\n",
    "        if attempts == max_attempts_per_point:\n",
    "            print(\"Maximum attempts reached, unable to find sufficient points with the specified criteria.\")\n",
    "            break  # Break outer loop if max attempts is reached without finding a point\n",
    "\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fe3b0-4bd5-4e44-b325-07ed4f7b45d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLAHE_plot_cond(image,disp_clip):\n",
    "    \"\"\"\n",
    "    Conditionally applies CLAHE to an image based on the provided clipping limit.\n",
    "\n",
    "    Parameters:\n",
    "        image (np.array): The input image as a NumPy array, typically grayscale.\n",
    "        disp_clip (float or str): The clip limit for CLAHE. If it is '0.0' (as a string or float), CLAHE is not applied.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The image after applying CLAHE if `disp_clip` is not '0.0'; otherwise, returns the original image.\n",
    "    \"\"\"\n",
    "    if float(disp_clip)!=0.0:\n",
    "        image = clahe(image,float(disp_clip))\n",
    "    return image\n",
    "        \n",
    "def outliers_plot_condition(landmark_errors,cond):\n",
    "    \"\"\"\n",
    "    Filters out specific outlier values from a list of landmark errors based on a condition.\n",
    "\n",
    "    This function examines each error in the list of landmark errors and removes specific outlier values,\n",
    "    in this case, the value 10000, if the condition specified by the 'cond' parameter is True. If 'cond' \n",
    "    is False, the list is returned unchanged. This functionality can be useful for cleaning or preparing\n",
    "    data before further analysis or visualization.\n",
    "\n",
    "    Parameters:\n",
    "    - landmark_errors (list of int or float): A list containing numerical values that represent the errors\n",
    "                                            in landmarks detection.\n",
    "    - cond (bool): A condition that determines whether the filtering of outliers should be performed. If\n",
    "                 True, outliers are removed; if False, the list is returned as is.\n",
    "\n",
    "    Returns:\n",
    "    - list of int or float: A list of landmark errors with specified outliers removed if the condition is met.\n",
    "    \"\"\"\n",
    "    if cond:\n",
    "        landmark_errors =[x for x in landmark_errors if x!=10000]\n",
    "    return landmark_errors\n",
    "\n",
    "def clahe(imag, clip):\n",
    "    \"\"\"\n",
    "    Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) to an image.\n",
    "\n",
    "    This function converts an image to grayscale, applies CLAHE to enhance the image contrast,\n",
    "    and then converts it back to RGB. It uses OpenCV for the CLAHE operation and PIL for image\n",
    "    conversions.\n",
    "\n",
    "    Parameters:\n",
    "    - imag (np.array): The input image array. Expected to be in format suitable for OpenCV.\n",
    "    - clip (float): The clipping limit for the CLAHE algorithm, which controls the contrast limit.\n",
    "                  Higher values increase contrast.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The contrast-enhanced image in RGB format.\n",
    "\n",
    "    Notes:\n",
    "          The tile grid size for CLAHE is set to (8, 8). Adjustments to this parameter may affect\n",
    "          the granularity of the histogram equalization.\n",
    "    \"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(8, 8))\n",
    "    imag = Image.fromarray(np.uint8(imag))\n",
    "    imag = imag.convert('L')\n",
    "    img = np.asarray(imag)\n",
    "    image_equalized = clahe.apply(img)\n",
    "    image_equalized_img = Image.fromarray(np.uint8(image_equalized))\n",
    "    image_equalized = image_equalized_img.convert('RGB')\n",
    "    image_equalized = np.asarray(image_equalized)\n",
    "    return image_equalized\n",
    "\n",
    "def compute_plot_FIRE_AUC(landmark_errors):\n",
    "    \"\"\"\n",
    "    Function to compute and plot the success rate curve and calculate the AUC for the dataset titled FIRE.\n",
    "    \n",
    "    Parameters:\n",
    "    - landmark_errors: List of landmark errors including outliers.\n",
    "    \"\"\"\n",
    "    landmark_errors_sorted = sorted(landmark_errors)\n",
    "    # Initialize lists for thresholds and success rates\n",
    "    thresholds = list(range(26)) # 0 to 100\n",
    "    success_rates = []\n",
    "    # Calculate success rate for each threshold\n",
    "    for threshold in thresholds:\n",
    "        successful_count = sum([1 for error in landmark_errors_sorted if error <= threshold])\n",
    "        success_rate = successful_count / len(landmark_errors_sorted)\n",
    "        success_rates.append(success_rate * 100) # convert to percentage\n",
    "\n",
    "    # Plot the curve\n",
    "    plt.plot(thresholds, success_rates, label=\"Success Rate Curve\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Success Rate (%)\")\n",
    "    plt.title(\"Success Rate vs. Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # Compute AUC\n",
    "    auc = np.sum(success_rates)/ 2500 # normalize to 0-1\n",
    "    print(\"AUC:\", auc)\n",
    "    \n",
    "def plot_landmark_errors(landmark_errors,rpth,chr='All',disable_outliers=False):\n",
    "    \"\"\"\n",
    "    Plots a graph of landmark errors over successive iterations to provide a visual analysis of registration accuracy\n",
    "    across samples. This function is designed to help in the assessment of registration processes in image processing\n",
    "    or computer vision tasks by plotting each landmark error against its iteration number. It also calculates and \n",
    "    displays the average landmark error across all iterations.\n",
    "\n",
    "    Parameters:\n",
    "    - landmark_errors (list of float): A list containing numerical errors for each landmark across multiple iterations.\n",
    "                                     Outliers (e.g., errors set to 10000) are automatically excluded from the plot.\n",
    "    - rpth (str): Path where the resulting plot image will be saved.\n",
    "    - chr (str, optional): Characteristic or description to include in the plot title, indicating the dataset or model\n",
    "                         used. Defaults to 'All'.\n",
    "    - disable_outliers (bool, optional): If set to True, disables the automatic exclusion of outlier values in the error\n",
    "                                       data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function does not return any value but saves the plot to the specified path and displays it.\n",
    "\n",
    "    Notes:\n",
    "        This plot is useful for tracking improvements or deteriorations in landmark detection algorithms over time.\n",
    "        It automatically filters out error values set to 10000, considering them as outliers, unless disable_outliers\n",
    "        is set to True.\n",
    "        The function saves the plot in the directory specified by `rpth` and names it 'Landmark_Error_Plot.png'.\n",
    "    \"\"\"\n",
    "    landmark_errors=outliers_plot_condition(landmark_errors,disable_outliers)\n",
    "    samples = list(range(0, len(landmark_errors)))\n",
    "    avg_error = sum(landmark_errors) / len(landmark_errors)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(samples, landmark_errors, marker='o', linestyle='-', color='#2C3E50', label=\"Landmark Error\")\n",
    "    plt.axhline(y=avg_error, color='#E74C3C', linestyle='--', label=f\"Average Error: {avg_error:.3f}\")\n",
    "    plt.title(\"Mean Landmark Error for the entire Database Housing {} images\".format(chr), fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Iteration Number\", fontsize=14)\n",
    "    plt.ylabel(\"Landmark Error\", fontsize=14)\n",
    "    plt.xticks(samples, [f\"Case {i}\" for i in samples], rotation=45)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(rpth,'Landmark_Error_Plot.png'))\n",
    "    plt.show();\n",
    "\n",
    "def image_point_correspondences(images,img_size,landmarks1,landmarks2,rpth,num,snum,disp_size=256,disp_clip=0.0):\n",
    "    \"\"\"\n",
    "    Displays and compares point correspondences between two images using given landmarks.\n",
    "\n",
    "    This function visualizes two images side-by-side with their respective landmarks. Each pair of corresponding\n",
    "    landmarks across the images is marked with the same color for easy identification of correspondences. The function\n",
    "    is designed to handle visualization for studies involving image registration or similar tasks where landmark\n",
    "    matching is crucial.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list of str): File paths to the two images (source and target images).\n",
    "    - img_size (int): The size to which images should be resized, specified as width and height (assumed square).\n",
    "    - landmarks1 (list of tuples): Landmark points on the first image (source image).\n",
    "    - landmarks2 (list of tuples): Corresponding landmark points on the second image (target image).\n",
    "    - rpth (str): Path where the resultant visualization should be saved.\n",
    "    - num (int): An identifier number used to differentiate the output file name.\n",
    "    - snum (str): Stage number or identifier to categorize the process stage.\n",
    "    - disp_size (int, optional): The display size to which the image will be resized for visualization. Defaults to 256.\n",
    "    - disp_clip (float, optional): Enabling the image to be enhanced using CLAHE based on the coefficient assigned here for diaply purposes. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "    - None: This function directly displays the image using matplotlib and saves the output visualization to disk.\n",
    "\n",
    "    Notes:\n",
    "        The function uses OpenCV for reading and resizing images. It employs a CLAHE function to enhance image contrast.\n",
    "        Matplotlib is used for visualizing the images and landmarks. The color map switches based on the number\n",
    "        of landmarks; if there are more than 15 landmarks, a cyclic colormap is used to differentiate them.\n",
    "        This function is particularly useful for visualizing transformations and registrations in medical imaging or\n",
    "        similar fields where point correspondence is critical.\n",
    "    \"\"\"\n",
    "    image1 = CLAHE_plot_cond(cv2.cvtColor(cv2.resize(cv2.imread(images[0]),(disp_size,disp_size)), cv2.COLOR_BGR2RGB),disp_clip)\n",
    "    image2 = CLAHE_plot_cond(cv2.cvtColor(cv2.resize(cv2.imread(images[1]),(disp_size,disp_size)), cv2.COLOR_BGR2RGB),disp_clip)\n",
    "    landmarks1 = coordinates_rescaling(landmarks1,img_size,img_size,disp_size)\n",
    "    landmarks2 = coordinates_rescaling(landmarks2,img_size,img_size,disp_size)\n",
    "    assert len(landmarks1) == len(landmarks2), f\"points lengths are incompatible: {len(landmarks1)} != {len(landmarks2)}.\"\n",
    "    num_points = len(landmarks1)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(\"Stage-{} Point Correspondences\".format(snum), fontsize=14, fontweight='bold', y=0.925)\n",
    "    ax1.set_title('Fixed Image')\n",
    "    ax2.set_title('Moving Image')\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    ax1.imshow(image1)\n",
    "    ax2.imshow(image2)\n",
    "    if num_points > 15:\n",
    "        cmap = plt.get_cmap('tab20')\n",
    "    else:\n",
    "        cmap = ListedColormap([\"red\", \"yellow\", \"blue\", \"lime\", \"magenta\", \"indigo\", \"orange\", \"cyan\", \"darkgreen\",\n",
    "                               \"maroon\", \"black\", \"white\", \"chocolate\", \"gray\", \"blueviolet\"])\n",
    "    colors = np.array([cmap(x) for x in range(len(landmarks1))])\n",
    "    radius1, radius2 = 4, 1\n",
    "    for point1, point2, color in zip(landmarks1, landmarks2, colors):\n",
    "        x1, y1 = point1\n",
    "        circ1_1 = plt.Circle((x1, y1), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ1_2 = plt.Circle((x1, y1), radius2, facecolor=color, edgecolor='white')\n",
    "        ax1.add_patch(circ1_1)\n",
    "        ax1.add_patch(circ1_2)\n",
    "        x2, y2 = point2\n",
    "        circ2_1 = plt.Circle((x2, y2), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ2_2 = plt.Circle((x2, y2), radius2, facecolor=color, edgecolor='white')\n",
    "        ax2.add_patch(circ2_1)\n",
    "        ax2.add_patch(circ2_2)\n",
    "    plt.figtext(0.5, 0.115, \"Note: {0} point correspondences were identified by the model for stage-{1}\".format(len(landmarks1), snum), ha='center', fontweight='bold', fontsize=8.5)\n",
    "    plt.savefig(os.path.join(rpth,'Stage'+str(snum)+'Point_Correspondences'+str(num)+'.png'))\n",
    "    plt.show();\n",
    "\n",
    "def original_image_point_correspondences(images,orig_moving_image_pth,img_size, landmarks1, landmarks2, landmarks3, rpth, num, disp_size=256,disp_clip=0.0):\n",
    "    \"\"\"\n",
    "    Visualizes and saves point correspondences across three images (fixed, moving, and transformed)\n",
    "    to aid in assessing the effectiveness of image registration processes. The function adjusts image\n",
    "    contrast using CLAHE for enhanced visualization and overlays landmark points on each image.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list of np.array): List containing three images representing fixed, moving, and transformed states.\n",
    "    - orig_moving_image_pth (str): Path to the original moving image, used to update the second image in the list.\n",
    "    - img_size (tuple): Original dimensions (width, height) of the images prior to any processing.\n",
    "    - landmarks1 (list of tuples): Coordinates of landmarks in the fixed image.\n",
    "    - landmarks2 (list of tuples): Coordinates of landmarks in the original moving image.\n",
    "    - landmarks3 (list of tuples): Coordinates of landmarks in the transformed image.\n",
    "    - rpth (str): Directory path where the result images will be saved.\n",
    "    - num (int): Identifier to differentiate output filenames.\n",
    "    - disp_size (int, optional): Target size (one dimension) for scaling images for display. Defaults to 256.\n",
    "    - disp_clip (float, optional): Enabling the image to be enhanced using CLAHE based on the coefficient assigned here for diaply purposes. Defaults to 0.0.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: If the number of landmarks in any list does not match the others.\n",
    "\n",
    "    Notes:\n",
    "        The images are resized to `disp_size` for display.\n",
    "        Landmarks are also rescaled to match the display size.\n",
    "        A colormap is applied to distinguish between different landmarks; a larger colormap is used if landmarks exceed 15.\n",
    "    \"\"\"\n",
    "    assert len(landmarks1) == len(landmarks2) == len(landmarks3), \"All landmarks lists must have the same length.\"\n",
    "    images[1]=cv2.imread(orig_moving_image_pth) # replacing the deformed image with the original moving image for displaying final results\n",
    "    num_points = len(landmarks1)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(\"Final Registration Results by Composing Transformations Estimated in Two Stages\", fontsize=14, fontweight='bold',y=0.925) \n",
    "    \n",
    "    ax1.set_title('Fixed Image')\n",
    "    ax2.set_title('Moving Image')\n",
    "    ax3.set_title('Deformed Image')\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    ax1.imshow(CLAHE_plot_cond(cv2.cvtColor(cv2.resize(images[0],(disp_size,disp_size)), cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    ax2.imshow(CLAHE_plot_cond(cv2.cvtColor(cv2.resize(images[1],(disp_size,disp_size)), cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    ax3.imshow(CLAHE_plot_cond(cv2.cvtColor(cv2.resize(images[2].astype(np.uint8),(disp_size,disp_size)), cv2.COLOR_BGR2RGB),disp_clip))\n",
    "\n",
    "    landmarks1 = coordinates_rescaling(landmarks1,img_size,img_size,disp_size)\n",
    "    landmarks2 = coordinates_rescaling(landmarks2,img_size,img_size,disp_size)\n",
    "    landmarks3 = coordinates_rescaling(landmarks3,img_size,img_size,disp_size)\n",
    "\n",
    "    if num_points > 15:\n",
    "        cmap = plt.get_cmap('tab20')\n",
    "    else:\n",
    "        cmap = ListedColormap([\"red\", \"yellow\", \"blue\", \"lime\", \"magenta\", \"indigo\", \"orange\", \"cyan\", \"darkgreen\",\n",
    "                               \"maroon\", \"black\", \"white\", \"chocolate\", \"gray\", \"blueviolet\"])\n",
    "\n",
    "    colors = np.array([cmap(x) for x in range(num_points)])\n",
    "    radius1, radius2 = 4, 1\n",
    "\n",
    "    for point1, point2, point3, color in zip(landmarks1, landmarks2, landmarks3, colors):\n",
    "        # Landmarks for Image 1\n",
    "        x1, y1 = point1\n",
    "        circ1_1 = plt.Circle((x1, y1), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ1_2 = plt.Circle((x1, y1), radius2, facecolor=color, edgecolor='white')\n",
    "        ax1.add_patch(circ1_1)\n",
    "        ax1.add_patch(circ1_2)\n",
    "\n",
    "        # Landmarks for Image 2\n",
    "        x2, y2 = point2\n",
    "        circ2_1 = plt.Circle((x2, y2), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ2_2 = plt.Circle((x2, y2), radius2, facecolor=color, edgecolor='white')\n",
    "        ax2.add_patch(circ2_1)\n",
    "        ax2.add_patch(circ2_2)\n",
    "\n",
    "        # Landmarks for Image 3\n",
    "        x3, y3 = point3\n",
    "        circ3_1 = plt.Circle((x3, y3), radius1, facecolor=color, edgecolor='white', alpha=0.5)\n",
    "        circ3_2 = plt.Circle((x3, y3), radius2, facecolor=color, edgecolor='white')\n",
    "        ax3.add_patch(circ3_1)\n",
    "        ax3.add_patch(circ3_2)\n",
    "        \n",
    "    plt.savefig(os.path.join(rpth, 'Final_Registration_Results_for_case' + str(num) + '.png'))\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537fc0f-ffd1-4a1e-862e-c0d1da725bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_points_affine(moving_points, affine_matrix):\n",
    "    \"\"\"\n",
    "    Transform the moving points using the given affine matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - moving_points: List of (x, y) tuples\n",
    "    - affine_matrix: (3x3) affine matrix\n",
    "    \n",
    "    Returns:\n",
    "    - List of (x, y) tuples representing transformed points\n",
    "    \"\"\"\n",
    "    points_array = np.array(moving_points)\n",
    "    homogeneous_points = np.hstack([points_array, np.ones((len(moving_points), 1))])\n",
    "    transformed_points = np.dot(homogeneous_points, affine_matrix.T)\n",
    "    return [tuple(point) for point in transformed_points[:, :2]]\n",
    "\n",
    "def transform_points_homography(moving_points, homography_matrix):\n",
    "    \"\"\"\n",
    "    Transform the moving points using the given homography matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - moving_points: List of (x, y) tuples\n",
    "    - homography_matrix: (3x3) homography matrix\n",
    "    \n",
    "    Returns:\n",
    "    - List of (x, y) tuples representing transformed points\n",
    "    \"\"\"\n",
    "    points_array = np.array(moving_points)\n",
    "    homogeneous_points = np.hstack([points_array, np.ones((len(moving_points), 1))])\n",
    "    transformed_points = np.dot(homogeneous_points, homography_matrix.T)\n",
    "    transformed_points /= transformed_points[:, 2][:, np.newaxis]  # Normalize by z-coordinate\n",
    "    return [tuple(point[:2]) for point in transformed_points]\n",
    "\n",
    "def transform_points_third_order_polynomial(moving_points, coefficients):\n",
    "    \"\"\"\n",
    "    Transform the moving points using the given third-order polynomial coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "    - moving_points: List of (x, y) tuples\n",
    "    - coefficients: Array of 20 coefficients for the third-order polynomial transformation\n",
    "    \n",
    "    Returns:\n",
    "    - List of (x, y) tuples representing transformed points\n",
    "    \"\"\"\n",
    "    if len(coefficients) != 20:\n",
    "        raise ValueError(\"Coefficients should have a shape of (20,).\")\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, \\\n",
    "    a11, a12, a13, a14, a15, a16, a17, a18, a19, a20 = coefficients\n",
    "\n",
    "    transformed_points = []\n",
    "    for x, y in moving_points:\n",
    "        # Compute new x' and y' for each point using third-order polynomial\n",
    "        x_prime = (a1*x**3 + a2*x**2*y + a3*x*y**2 + a4*y**3 + \n",
    "                   a5*x**2 + a6*x*y + a7*y**2 + a8*x + a9*y + a10)\n",
    "        y_prime = (a11*x**3 + a12*x**2*y + a13*x*y**2 + a14*y**3 + \n",
    "                   a15*x**2 + a16*x*y + a17*y**2 + a18*x + a19*y + a20)\n",
    "        transformed_points.append((x_prime, y_prime))\n",
    "\n",
    "    return transformed_points\n",
    "\n",
    "def transform_points_quadratic(points, coefficients):\n",
    "    \"\"\"\n",
    "    Applies a quadratic transformation to a set of 2D points based on the provided coefficients. This function is\n",
    "    typically used in image processing and computer vision tasks to deform points according to a quadratic model.\n",
    "\n",
    "    Parameters:\n",
    "    - points (list of tuples): A list of points, where each point is represented as a tuple (x, y).\n",
    "    - coefficients (list): A list of 12 coefficients for the quadratic transformation model.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of tuples representing the deformed points.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the number of coefficients is not equal to 12, as the quadratic model requires exactly 12 coefficients.\n",
    "\n",
    "    Notes:\n",
    "        This function uses a quadratic transformation defined as:\n",
    "        x' = a1*x + a2*y + a3*x*y + a4*x^2 + a5*y^2 + a6\n",
    "        y' = a7*x + a8*y + a9*x*y + a10*x^2 + a11*y^2 + a12\n",
    "        where `x, y` are the original coordinates and `x', y'` are the transformed coordinates.\n",
    "        The coefficients must be specified in the order [a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12].\n",
    "    \"\"\"\n",
    "    if len(coefficients) != 12:\n",
    "        raise ValueError(\"Coefficients should have a shape of (12,).\")\n",
    "    \n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12 = coefficients\n",
    "    \n",
    "    deformed = []\n",
    "    for x, y in points:\n",
    "        x_prime = a1*x + a2*y + a3*x*y + a4*x**2 + a5*y**2 + a6\n",
    "        y_prime = a7*x + a8*y + a9*x*y + a10*x**2 + a11*y**2 + a12\n",
    "        deformed.append((x_prime, y_prime))\n",
    "    \n",
    "    return deformed\n",
    "\n",
    "def compute_landmark_error_fixed_space(polynomial_matrix,fixed_points,moving_points,new_image_size,image_size):\n",
    "    \"\"\"\n",
    "    Compute the landmark error between fixed points and transformed moving points.\n",
    "    \n",
    "    Parameters:\n",
    "    - fixed_points: List of (x, y) tuples in the fixed image.\n",
    "    - moving_points: List of (x, y) tuples in the moving image.\n",
    "    - polynomial_matrix: (3x3) matrix used to transform points using a third-order polynomial.\n",
    "    - image_size: The original size of the images.\n",
    "    - new_image_size: The size of the images after rescaling.\n",
    "    \n",
    "    Returns:\n",
    "    - mle: Mean Landmark Error.\n",
    "    \"\"\"\n",
    "    transformed_points = transform_points_third_order_polynomial(moving_points, polynomial_matrix)\n",
    "    transformed_points = coordinates_rescaling_high_scale(transformed_points,new_image_size,new_image_size, image_size)\n",
    "    errors = np.linalg.norm(np.array(fixed_points) - transformed_points, axis=1)\n",
    "    mle = np.mean(errors)\n",
    "    return mle\n",
    "\n",
    "def compute_landmark_error(fixed_points,fixed_image_size,moving_points,moving_image_size,new_image_size):\n",
    "    \"\"\"\n",
    "    Calculates the mean landmark error between fixed points and transformed moving points\n",
    "    after rescaling to a new image size. This function is primarily used in image processing\n",
    "    to measure the accuracy of image registration by quantifying the displacement of landmark points.\n",
    "\n",
    "    Parameters:\n",
    "    - fixed_points (list of tuples): Coordinates of landmark points in the fixed image as (x, y) tuples.\n",
    "    - fixed_image_size (tuple): The original size (width, height) of the fixed image.\n",
    "    - moving_points (list of tuples): Coordinates of landmark points in the moving image as (x, y) tuples.\n",
    "    - moving_image_size (tuple): The original size (width, height) of the moving image.\n",
    "    - new_image_size (int): The size to which both sets of points will be resized.\n",
    "\n",
    "    Returns:\n",
    "    - float: The mean landmark error calculated as the average Euclidean distance between corresponding\n",
    "          landmarks after rescaling to the new image size.\n",
    "\n",
    "    Notes:\n",
    "        The function first rescales the coordinates of both fixed and moving points to a new size.\n",
    "        It then calculates the Euclidean distance between the corresponding rescaled points.\n",
    "        This metric is useful for evaluating the precision of image registration methods, particularly in medical imaging.\n",
    "    \"\"\"\n",
    "    rescaled_fixed_points = coordinates_rescaling_high_scale(fixed_points,new_image_size,new_image_size,fixed_image_size)\n",
    "    rescaled_moving_points = coordinates_rescaling_high_scale(moving_points,new_image_size,new_image_size,moving_image_size)\n",
    "    errors = np.linalg.norm(np.array(rescaled_fixed_points) - rescaled_moving_points, axis=1)\n",
    "    mle = np.mean(errors)\n",
    "    return mle\n",
    "    \n",
    "def compute_third_order_polynomial_matrix(landmarks1, landmarks2):\n",
    "    \"\"\"\n",
    "    Compute coefficients for the third-order polynomial transformation.\n",
    "\n",
    "    Parameters:\n",
    "    - landmarks1 (list): List of (x, y) tuples of landmarks in the first image.\n",
    "    - landmarks2 (list): List of (x, y) tuples of landmarks in the second image.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Coefficients of the third-order polynomial transformation.\n",
    "    \"\"\"\n",
    "    if len(landmarks1) != len(landmarks2) or len(landmarks1) < 10:\n",
    "        raise ValueError(\"Both landmarks should have the same number of points, and at least 10 points are required.\")\n",
    "\n",
    "    A = []\n",
    "    B = []\n",
    "\n",
    "    for (x, y), (x_prime, y_prime) in zip(landmarks1, landmarks2):\n",
    "        # For x'\n",
    "        A.append([x**3, x**2 * y, x * y**2, y**3, x**2, x * y, y**2, x, y, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "        # For y'\n",
    "        A.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, x**3, x**2 * y, x * y**2, y**3, x**2, x * y, y**2, x, y, 1])\n",
    "        \n",
    "        B.extend([x_prime, y_prime])\n",
    "\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "\n",
    "    # Solve the linear system\n",
    "    coefficients, _, _, _ = np.linalg.lstsq(A, B, rcond=None)\n",
    "\n",
    "    return coefficients  # The shape of coefficients is (20,)\n",
    "    \n",
    "def compute_quadratic_matrix(landmarks1, landmarks2):\n",
    "    \"\"\"\n",
    "    Compute the quadratic matrix using provided landmarks.\n",
    "    \n",
    "    Parameters:\n",
    "    - landmarks1: List of (x, y) tuples from the source image.\n",
    "    - landmarks2: List of (x, y) tuples from the target image.\n",
    "\n",
    "    Returns:\n",
    "    - 3x3 homography matrix.\n",
    "    \"\"\"\n",
    "    if len(landmarks1) != len(landmarks2) or len(landmarks1) < 6:\n",
    "        raise ValueError(\"Both landmarks should have the same number of points, and at least 6 points are required.\")\n",
    "\n",
    "    A = []\n",
    "    B = []\n",
    "\n",
    "    for (x, y), (x_prime, y_prime) in zip(landmarks1, landmarks2):\n",
    "        A.append([x, y, x*y, x*x, y*y, 1, 0, 0, 0, 0, 0, 0])\n",
    "        A.append([0, 0, 0, 0, 0, 0, x, y, x*y, x*x, y*y, 1])\n",
    "        \n",
    "        B.append(x_prime)\n",
    "        B.append(y_prime)\n",
    "\n",
    "    A = np.array(A)\n",
    "    B = np.array(B)\n",
    "\n",
    "    # Solve the linear system\n",
    "    coefficients, _, _, _ = np.linalg.lstsq(A, B, rcond=None)\n",
    "\n",
    "    return coefficients\n",
    "    \n",
    "def compute_homography_matrix(landmarks1, landmarks2):\n",
    "    \"\"\"\n",
    "    Compute the homography matrix using provided landmarks.\n",
    "    \n",
    "    Parameters:\n",
    "    - landmarks1: List of (x, y) tuples from the source image.\n",
    "    - landmarks2: List of (x, y) tuples from the target image.\n",
    "\n",
    "    Returns:\n",
    "    - 3x3 homography matrix.\n",
    "    \"\"\"\n",
    "    homography_matrix, _ = cv2.findHomography(np.array(landmarks1), np.array(landmarks2))\n",
    "    return homography_matrix\n",
    "\n",
    "def transform_points_third_order_polynomial_matrix(landmarks1, landmarks2,img_size,new_img_size):\n",
    "    \"\"\"\n",
    "    Computes a third-order polynomial transformation matrix based on rescaled landmark points from one image space\n",
    "    to another. This transformation is typically used for tasks like geometric transformation of images where precise \n",
    "    alignment or registration of image features is necessary.\n",
    "\n",
    "    Parameters:\n",
    "    - landmarks1 (list of tuples): List of original landmark points in the source image given as (x, y) tuples.\n",
    "    - landmarks2 (list of tuples): List of corresponding landmark points in the target image given as (x, y) tuples.\n",
    "                                 The points in landmarks2 should correspond one-to-one with those in landmarks1.\n",
    "    - img_size (int): Original size of the images from which the landmarks were extracted. This is used to help\n",
    "                    rescale points for accurate computation of the transformation matrix.\n",
    "    - new_img_size (int): New size to which the points will be rescaled before computing the transformation matrix.\n",
    "                        This should reflect the size of the image space into which the points will be transformed.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A transformation matrix which can be used to map the points from the space defined by landmarks1\n",
    "                       to the space defined by landmarks2. The matrix is represented as a 10x1 array of coefficients,\n",
    "                       corresponding to the terms of a third-order polynomial.\n",
    "\n",
    "    Notes:\n",
    "        Ensure that the number of points in landmarks1 and landmarks2 are equal and that they correspond to each other in order.\n",
    "        This function involves rescaling coordinates, calculating a transformation matrix, and is typically used in image processing\n",
    "        tasks where geometric transformations are necessary for alignment and registration.\n",
    "    \"\"\"\n",
    "    landmarks1 = coordinates_rescaling(landmarks1,img_size,img_size,new_img_size)\n",
    "    landmarks2 = coordinates_rescaling(landmarks2,img_size,img_size,new_img_size)\n",
    "    third_order_polynomial_matrix  = compute_third_order_polynomial_matrix(landmarks1, landmarks2)\n",
    "    return third_order_polynomial_matrix \n",
    "\n",
    "def transform_points_quadratic_matrix(landmarks1, landmarks2,img_size,new_img_size):\n",
    "    \"\"\"\n",
    "    Computes a quadratic transformation matrix based on rescaled landmarks from one set of image coordinates to another.\n",
    "\n",
    "    This function rescales the input landmarks from their original dimensions (img_size) to new dimensions (new_img_size).\n",
    "    It then calculates a quadratic transformation matrix that describes how points from the first set of landmarks (landmarks1)\n",
    "    can be transformed to align with the second set (landmarks2). This matrix could be used to apply geometric transformations\n",
    "    to images or coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - landmarks1 (list of tuples): List of (x, y) tuples representing original landmarks in the source image.\n",
    "    - landmarks2 (list of tuples): List of (x, y) tuples representing target landmarks in the target image, \n",
    "                                 corresponding to landmarks1.\n",
    "    - img_size (int): The original size (width and height, assumed square) of the images from which the landmarks were extracted.\n",
    "    - new_img_size (int): The new size (width and height, assumed square) to which the images and landmarks are rescaled\n",
    "                        before computing the transformation matrix.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A matrix that contains the coefficients of the quadratic transformation. This matrix is used\n",
    "                       to transform points from the source image to the target image based on the calculated polynomial.\n",
    "\n",
    "    Notes:\n",
    "        Ensure that the number of points in landmarks1 and landmarks2 are equal and that they correspond to each other in order.\n",
    "        This function is essential in image processing tasks where precise transformations are necessary for image alignment and registration.\n",
    "    \"\"\"\n",
    "    landmarks1 = coordinates_rescaling(landmarks1,img_size,img_size,new_img_size)\n",
    "    landmarks2 = coordinates_rescaling(landmarks2,img_size,img_size,new_img_size)\n",
    "    quadratic_matrix = compute_quadratic_matrix(landmarks1, landmarks2)\n",
    "    return quadratic_matrix \n",
    "\n",
    "\n",
    "def warp_image_third_order_polynomial(image, coefficients):\n",
    "    \"\"\"\n",
    "    Applies a third-order polynomial transformation to an image using provided coefficients, effectively deforming the image.\n",
    "\n",
    "    Parameters:\n",
    "    - image (numpy.ndarray): The image to deform, provided as a numpy array. The array can be either\n",
    "                           two-dimensional (grayscale image) or three-dimensional (color image).\n",
    "    - coefficients (list or array): An array of 20 coefficients for the third-order polynomial transformation.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the number of coefficients provided is not 20, an error is raised due to the requirement\n",
    "                of exactly 20 coefficients to perform the transformation.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The deformed image as a numpy array of the same shape as the input image.\n",
    "\n",
    "    Notes:\n",
    "        The deformation is defined by a polynomial transformation that adjusts the coordinates of each pixel\n",
    "        based on the polynomial defined by the coefficients.\n",
    "        This function supports both grayscale and color images. For color images, the transformation is applied\n",
    "        to each color channel independently.\n",
    "        The transformation involves calculating new pixel positions and mapping the original pixel values\n",
    "        to these new positions using spline interpolation of order 1.\n",
    "    \"\"\"\n",
    "    if len(coefficients) != 20:\n",
    "        raise ValueError(\"Coefficients should have a shape of (20,).\")\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, \\\n",
    "    a11, a12, a13, a14, a15, a16, a17, a18, a19, a20 = coefficients\n",
    "    \n",
    "    # Check if the image is grayscale or colored\n",
    "    if len(image.shape) == 2:\n",
    "        height, width = image.shape\n",
    "        output = np.zeros((height, width))\n",
    "        channels = 1\n",
    "        image = image[:, :, np.newaxis]  # add an additional dimension for consistency\n",
    "    else:\n",
    "        height, width, channels = image.shape\n",
    "        output = np.zeros((height, width, channels))\n",
    "    \n",
    "    # Generate the coordinates\n",
    "    coordinates = np.indices((height, width))\n",
    "    x_coords = coordinates[1]\n",
    "    y_coords = coordinates[0]\n",
    "\n",
    "    # Compute new x' and y' for every x and y using third-order polynomial\n",
    "    x_prime = (a1*x_coords**3 + a2*x_coords**2*y_coords + a3*x_coords*y_coords**2 + a4*y_coords**3 + \n",
    "               a5*x_coords**2 + a6*x_coords*y_coords + a7*y_coords**2 + a8*x_coords + a9*y_coords + a10)\n",
    "    y_prime = (a11*x_coords**3 + a12*x_coords**2*y_coords + a13*x_coords*y_coords**2 + a14*y_coords**3 + \n",
    "               a15*x_coords**2 + a16*x_coords*y_coords + a17*y_coords**2 + a18*x_coords + a19*y_coords + a20)\n",
    "\n",
    "    # Map the old image pixels to the new deformed positions\n",
    "    for c in range(channels):  # for each channel\n",
    "        output[:, :, c] = map_coordinates(image[:, :, c], [y_prime, x_prime], order=1, mode='constant', cval=0.0)\n",
    "\n",
    "    if channels == 1:\n",
    "        return output[:, :, 0]  # return as 2D grayscale image\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "def warp_image_quadratic_matrix(image, coefficients):\n",
    "    \"\"\"\n",
    "    Applies a quadratic transformation to deform an image using provided coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "    - image (numpy.ndarray): The image to deform, represented as a numpy array. This array can be \n",
    "                           either two-dimensional (grayscale image) or three-dimensional (color image).\n",
    "    - coefficients (list or array): A list or array of 12 coefficients defining the quadratic transformation.\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError: If the number of coefficients provided is not equal to 12, raises an error indicating\n",
    "                that exactly 12 coefficients are required for the transformation.\n",
    "    \n",
    "    Returns:\n",
    "    - numpy.ndarray: The deformed image as a numpy array of the same shape as the input image.\n",
    "    \n",
    "    Notes:\n",
    "        The deformation involves calculating new pixel coordinates using the quadratic equation defined\n",
    "        by the coefficients and then mapping the original pixel values to these new coordinates.\n",
    "        The function checks if the image is in grayscale or color and processes each channel independently.\n",
    "        The mapping of pixels uses spline interpolation of order 1 for accuracy and fills any areas outside\n",
    "        the transformed coordinates with zeros.\n",
    "    \"\"\"\n",
    "    if len(coefficients) != 12:\n",
    "        raise ValueError(\"Coefficients should have a shape of (12,).\")\n",
    "    \n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12 = coefficients\n",
    "    \n",
    "    # Check if the image is grayscale or colored\n",
    "    if len(image.shape) == 2:\n",
    "        height, width = image.shape\n",
    "        output = np.zeros((height, width))\n",
    "        channels = 1\n",
    "        image = image[:, :, np.newaxis]  # add an additional dimension for consistency\n",
    "    else:\n",
    "        height, width, channels = image.shape\n",
    "        output = np.zeros((height, width, channels))\n",
    "    \n",
    "    # Generate the coordinates\n",
    "    coordinates = np.indices((height, width))\n",
    "    x_coords = coordinates[1]\n",
    "    y_coords = coordinates[0]\n",
    "\n",
    "    # Compute new x' and y' for every x and y\n",
    "    x_prime = a1*x_coords + a2*y_coords + a3*x_coords*y_coords + a4*x_coords**2 + a5*y_coords**2 + a6\n",
    "    y_prime = a7*x_coords + a8*y_coords + a9*x_coords*y_coords + a10*x_coords**2 + a11*y_coords**2 + a12\n",
    "\n",
    "    # Map the old image pixels to the new deformed positions\n",
    "    for c in range(channels):  # for each channel\n",
    "        output[:, :, c] = map_coordinates(image[:, :, c], [y_prime, x_prime], order=1, mode='constant', cval=0.0)\n",
    "\n",
    "    if channels == 1:\n",
    "        return output[:, :, 0]  # return as 2D grayscale image\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "\n",
    "def compute_third_order_polynomial_matrix_and_plot(images, img_size, landmarks1, landmarks2, rpth, num,snum,disp_clip=0.0):\n",
    "    \"\"\"\n",
    "    Computes a third-order polynomial transformation matrix based on landmark correspondences\n",
    "    between two images and applies this transformation to align one image with another. This function\n",
    "    also displays and saves the original and transformed images, enhancing their contrast for better\n",
    "    visibility.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list of str): Paths to the source and target images.\n",
    "    - img_size (int): The dimensions (height and width) to which the images should be resized.\n",
    "    - landmarks1 (list of tuples): Coordinates of landmarks in the source image.\n",
    "    - landmarks2 (list of tuples): Corresponding coordinates of landmarks in the target image.\n",
    "    - rpth (str): Path to the directory where the resultant images will be saved.\n",
    "    - num (int): An identifier number for differentiating the output file names.\n",
    "    - snum (int): Stage number for referencing in output.\n",
    "    - disp_clip (float, optional): Clipping limit for the CLAHE algorithm, used for contrast enhancement of the image, for display purposes. Default is 0.0.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the list of landmarks from the source image is empty.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Contains three elements:\n",
    "        - imags (list of np.array): The original fixed and moving images along with the transformed image.\n",
    "        - imgs (list of str): Paths to the saved output images.\n",
    "        - coefficients (np.array): Coefficients of the third-order polynomial used for the transformation.\n",
    "\n",
    "    Notes:\n",
    "        This function is suited for complex registration tasks where finer control over the transformation is required.\n",
    "        The transformation matrix is applied to the target image to align it with the source image, effectively warping it.\n",
    "        The images are displayed and saved with enhanced contrast to aid in visual assessment of the registration quality.\n",
    "    \"\"\"\n",
    "    imags,imgs = [],[]\n",
    "    img1 = cv2.resize(cv2.imread(images[0]), (img_size, img_size))\n",
    "    img2 = cv2.resize(cv2.imread(images[1]), (img_size, img_size))\n",
    "    \n",
    "    imags.append(img1)\n",
    "    imags.append(img2)\n",
    "\n",
    "     # Check if the list is not empty\n",
    "    if not landmarks1:  raise ValueError(\"Input list cannot be empty\")\n",
    "    \n",
    "    # Check and delte the temporary folder if it exists\n",
    "    if os.path.exists(os.path.join(os.getcwd(),'temp_dir')): shutil.rmtree(os.path.join(os.getcwd(),'temp_dir'))\n",
    "\n",
    "    # Compute the third-order polynomial transformation matrix\n",
    "    coefficients = compute_third_order_polynomial_matrix(landmarks1, landmarks2)\n",
    "    coefficients_for_transform = compute_third_order_polynomial_matrix(landmarks2, landmarks1)\n",
    "\n",
    "    # Apply the transformation using third-order polynomial\n",
    "    transformed_image = warp_image_third_order_polynomial(img2, coefficients_for_transform.flatten())\n",
    "    imags.append(transformed_image)\n",
    "\n",
    "    # Display and save the images\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(\"Stage-{} Results: Registration Using Third Order Polynomial Transformation\".format(snum), fontsize=14, fontweight='bold', y=0.93)\n",
    "    \n",
    "    axs[0].imshow(CLAHE_plot_cond(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[0].set_title('Fixed Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(CLAHE_plot_cond(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[1].set_title('Moving Image')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(CLAHE_plot_cond(cv2.cvtColor(transformed_image.astype(np.uint8), cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[2].set_title('Deformed Image')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.show();\n",
    "    \n",
    "    # saving intermediary results for better visualization\n",
    "    imgs.append(os.path.join(rpth, 'Deformed_Image_' + str(num) + '_.png'))\n",
    "    imgs.append(os.path.join(rpth, 'Fixed_' + str(num) + '_.png'))\n",
    "    cv2.imwrite(os.path.join(rpth, 'Fixed_' + str(num) + '_.png'), img1)\n",
    "    cv2.imwrite(os.path.join(rpth, 'Moving_' + str(num) + '_.png'), img2)\n",
    "    cv2.imwrite(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'),transformed_image);\n",
    "    return imags,imgs,coefficients\n",
    "\n",
    "def compute_affine_matrix_and_plot(images,img_size,landmarks1, landmarks2,rpth,num,snum,disp_clip=0.0):\n",
    "    \"\"\"\n",
    "    Computes an affine transformation matrix based on provided landmarks from two images and applies \n",
    "    this transformation to visually compare the source, target, and transformed images.\n",
    "\n",
    "    This function computes the affine transformation matrix that best maps the source image to align \n",
    "    with the target image using landmark correspondences. It then applies this transformation to \n",
    "    the source image and displays the original (source and target) and transformed images side-by-side.\n",
    "    The images are enhanced using CLAHE for better visibility and are saved to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list of str): File paths for the source and target images.\n",
    "    - img_size (int): The size (width and height) to which the images will be resized.\n",
    "    - landmarks1 (list of tuples): Landmark points (x, y) on the source image.\n",
    "    - landmarks2 (list of tuples): Corresponding landmark points (x, y) on the target image.\n",
    "    - rpth (str): Directory path where the resultant images will be saved.\n",
    "    - num (int): Identifier number used to differentiate the output file names.\n",
    "    - snum (int): Stage number for referencing in output.\n",
    "    - disp_clip (float, optional): Clipping limit for the CLAHE algorithm, used for contrast enhancement of the image, for display purposes. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Contains two items:\n",
    "        - imgs (list of str): Paths to the saved images.\n",
    "        - affine_matrix (numpy.ndarray): The computed affine transformation matrix.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the landmarks list is empty, indicating insufficient data to compute the matrix.\n",
    "\n",
    "    Notes:\n",
    "        The affine transformation matrix is computed using a least squares method based on provided landmarks.\n",
    "        This function is useful for tasks in image registration where visual comparison of alignment is required.\n",
    "        Enhanced contrast is used to aid in the visual assessment of image registration quality.\n",
    "    \"\"\"\n",
    "    imgs,imags=[],[]\n",
    "    img1 = cv2.resize(cv2.imread(images[0]),(img_size,img_size))\n",
    "    img2 = cv2.resize(cv2.imread(images[1]),(img_size,img_size))\n",
    "    \n",
    "    imags.append(img1)\n",
    "    imags.append(img2)\n",
    "    \n",
    "    # Check if the list is not empty\n",
    "    if not landmarks1:  raise ValueError(\"Input list cannot be empty\")\n",
    "    \n",
    "    # Check and delte the temporary folder if it exists\n",
    "    if os.path.exists(os.path.join(os.getcwd(),'temp_dir')): shutil.rmtree(os.path.join(os.getcwd(),'temp_dir'))\n",
    "\n",
    "    # Create the array with the specified format\n",
    "    A = np.array([[xs, ys, 1] for xs, ys in landmarks1])\n",
    "    \n",
    "    X = np.array([xt for xt, yt in landmarks2])\n",
    "    Y = np.array([yt for xt, yt in landmarks2])\n",
    "\n",
    "    # Solve for the variables x1, y1, and z1\n",
    "    sol1 = np.dot(np.dot(np.linalg.inv(np.dot(A.T,A)),A.T),X)\n",
    "    sol2 = np.dot(np.dot(np.linalg.inv(np.dot(A.T,A)),A.T),Y)\n",
    "    # Extract the variables\n",
    "    x1, y1, z1 = sol1\n",
    "    x2, y2, z2 = sol2\n",
    "    affine_matrix = np.array([[x1,y1,z1],\n",
    "                           [x2,y2,z2],\n",
    "                           [0, 0, 1]])\n",
    "    print(\"Affine Matrix:\")\n",
    "    print(affine_matrix)\n",
    "    \n",
    "    # Ensure the affine matrix is of type float32\n",
    "    affine_matrix = affine_matrix.astype(np.float32)\n",
    "\n",
    "    # Use only the top two rows for cv2.warpAffine\n",
    "    affine_for_warp = affine_matrix[:2]\n",
    "    \n",
    "    # Apply the affine transformation using cv2.warpAffine\n",
    "    transformed_image = cv2.warpAffine(img2, affine_for_warp, (img2.shape[1], img2.shape[0]))\n",
    "    imags.append(transformed_image)\n",
    "\n",
    "    # Display and save the images\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(\"Stage-{} Results: Registration Using Affine Transformation\".format(snum), fontsize=14, fontweight='bold', y=0.93)\n",
    "\n",
    "    axs[0].imshow(CLAHE_plot_cond(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[0].set_title('Fixed Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(CLAHE_plot_cond(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[1].set_title('Moving Image')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(CLAHE_plot_cond(cv2.cvtColor(transformed_image.astype(np.uint8), cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[2].set_title('Deformed Image')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.show();\n",
    "    \n",
    "    # saving intermediary results for better visualization\n",
    "    imgs.append(os.path.join(rpth, 'Deformed_Image_' + str(num) + '_.png'))\n",
    "    imgs.append(os.path.join(rpth, 'Fixed_' + str(num) + '_.png'))\n",
    "    cv2.imwrite(os.path.join(rpth, 'Fixed_' + str(num) + '_.png'), img1)\n",
    "    cv2.imwrite(os.path.join(rpth, 'Moving_' + str(num) + '_.png'), img2)\n",
    "    cv2.imwrite(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'),transformed_image);\n",
    "    return imgs,imags,affine_matrix\n",
    "    \n",
    "def compute_quadratic_matrix_and_plot(images,img_size,landmarks1, landmarks2,rpth,num,snum,disp_clip=0.0):\n",
    "    \"\"\"\n",
    "    Computes a quadratic transformation matrix from source to target landmarks and applies this transformation\n",
    "    to the source image. The transformed source image is displayed alongside the original source and target images,\n",
    "    and all images are saved to disk.\n",
    "\n",
    "    This function takes pairs of corresponding landmarks from the source and target images to compute a quadratic\n",
    "    transformation matrix. This matrix is then used to warp the source image to match the target image. The\n",
    "    result, along with the original images, is displayed and saved for comparison.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list of str): File paths for the source and target images.\n",
    "    - img_size (int): The size (width and height) to which the images will be resized.\n",
    "    - landmarks1 (list of tuples): Landmark points (x, y) on the source image.\n",
    "    - landmarks2 (list of tuples): Corresponding landmark points (x, y) on the target image.\n",
    "    - rpth (str): Directory path where the resultant images will be saved.\n",
    "    - num (int): Identifier number used to differentiate the output file names.\n",
    "    - cll (float, optional): Clipping limit for the CLAHE algorithm used in contrast enhancement. Default is 1.5.\n",
    "    - snum (int): Stage number used for displaying in the title of the plot.\n",
    "    - disp_clip (float, optional): Clipping limit for the CLAHE algorithm, used for contrast enhancement of the image, for display purposes. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Contains three items:\n",
    "        - imgs (list of str): File paths where the output images are saved.\n",
    "        - imags (list of np.array): List containing the numpy arrays of the original and transformed images.\n",
    "        - quadratic_matrix (numpy.ndarray): The computed quadratic transformation matrix.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: If the number of points in `landmarks1` and `landmarks2` are not equal, since a matching\n",
    "                    number of points is required for matrix computation.\n",
    "\n",
    "    Notes:\n",
    "        The function uses OpenCV for image processing tasks such as reading, resizing, transforming, and saving images.\n",
    "        The quadratic transformation matrix is computed using a least squares method based on provided landmarks.\n",
    "        Matplotlib is used for visualizing the before and after effects of the transformation.\n",
    "        This function is particularly useful in applications such as image registration and geometric transformations.\n",
    "    \"\"\"\n",
    "    imgs,imags=[],[]\n",
    "    img1 = cv2.resize(cv2.imread(images[0]),(img_size,img_size))\n",
    "    img2 = cv2.resize(cv2.imread(images[1]),(img_size,img_size))\n",
    "    \n",
    "    imags.append(img1)\n",
    "    imags.append(img2)\n",
    "    \n",
    "    # Check if the list is not empty\n",
    "    if not landmarks1:  raise ValueError(\"Input list cannot be empty\")\n",
    "    \n",
    "    # Check and delte the temporary folder if it exists\n",
    "    if os.path.exists(os.path.join(os.getcwd(),'temp_dir')): shutil.rmtree(os.path.join(os.getcwd(),'temp_dir'))\n",
    "\n",
    "    # Ensure the quadratic matrix is of type float32\n",
    "    quadratic_matrix = compute_quadratic_matrix(landmarks1, landmarks2)\n",
    "    \n",
    "    quadratic_matrix_for_image_deformed = compute_quadratic_matrix(landmarks2, landmarks1)\n",
    "    \n",
    "    print(\"quadratic Matrix:\")\n",
    "    print(quadratic_matrix)\n",
    "    \n",
    "    # Apply the quadratic transformation using cv2.warpquadratic\n",
    "    transformed_image =  warp_image_quadratic_matrix(img2, quadratic_matrix_for_image_deformed)\n",
    "    transformed_image = cv2.resize(transformed_image,  (img2.shape[1], img2.shape[0]))\n",
    "    imags.append(transformed_image)\n",
    "    \n",
    "    # Display and save the images\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(\"Stage-{} Results: Registration Using Quadratic Transformation\".format(snum), fontsize=14, fontweight='bold', y=0.93)\n",
    "\n",
    "    axs[0].imshow(CLAHE_plot_cond(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[0].set_title('Fixed Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(CLAHE_plot_cond(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[1].set_title('Moving Image')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(CLAHE_plot_cond(cv2.cvtColor(transformed_image.astype(np.uint8), cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[2].set_title('Deformed Image')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.show();\n",
    "    \n",
    "    # saving intermediary results for better visualization\n",
    "    imgs.append(os.path.join(rpth, 'Deformed_Image_' + str(num) + '_.png'))\n",
    "    imgs.append(os.path.join(rpth, 'Fixed_' + str(num) + '_.png'))\n",
    "    cv2.imwrite(os.path.join(rpth, 'Fixed_' + str(num) + '_.png'), img1)\n",
    "    cv2.imwrite(os.path.join(rpth, 'Moving_' + str(num) + '_.png'), img2)\n",
    "    cv2.imwrite(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'),transformed_image);\n",
    "    return imgs,imags,quadratic_matrix\n",
    "\n",
    "def compute_homography_matrix_and_plot(images, img_size, landmarks1, landmarks2, rpth, num,snum,disp_clip=0.0): \n",
    "    \"\"\"\n",
    "    Computes the homography transformation matrix based on landmark correspondences between two images \n",
    "    and applies this transformation to the source image. The function displays the original source and \n",
    "    target images along with the transformed source image. It also saves these images to disk.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list of str): Paths to the source and target images.\n",
    "    - img_size (int): The size to which both images will be resized.\n",
    "    - landmarks1 (list of tuples): Landmark points (x, y) from the source image.\n",
    "    - landmarks2 (list of tuples): Corresponding landmark points (x, y) from the target image.\n",
    "    - rpth (str): The directory path where the resultant images will be saved.\n",
    "    - num (int): An identifier number used to differentiate the output file names.\n",
    "    - snum (int): Stage number used for displaying in the title of the plot.\n",
    "    - disp_clip (float, optional): Clipping limit for the CLAHE algorithm, used for contrast enhancement of the image, for display purposes. Default is 0.0.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing the paths to the saved images, a list of image arrays including the transformed image,\n",
    "           and the computed homography matrix.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the list of landmarks is empty, indicating that there are not enough data points to compute the homography.\n",
    "\n",
    "    Notes:\n",
    "        The function uses OpenCV for image reading, resizing, and applying the homography transformation.\n",
    "        Matplotlib is used for displaying the images.\n",
    "        Ensure the landmarks are accurately defined as their correspondence directly affects the quality of the transformation.\n",
    "        Homography transformations are particularly useful for applications in image registration, computer vision, and photogrammetry.\n",
    "    \"\"\"\n",
    "    imgs,imags=[],[]\n",
    "    img1 = cv2.resize(cv2.imread(images[0]),(img_size,img_size))\n",
    "    img2 = cv2.resize(cv2.imread(images[1]),(img_size,img_size))\n",
    "    \n",
    "    imags.append(img1)\n",
    "    imags.append(img2)\n",
    "    \n",
    "    # Check if the list is not empty\n",
    "    if not landmarks1:  raise ValueError(\"Input list cannot be empty\")\n",
    "    \n",
    "    # Check and delte the temporary folder if it exists\n",
    "    if os.path.exists(os.path.join(os.getcwd(),'temp_dir')): shutil.rmtree(os.path.join(os.getcwd(),'temp_dir'))\n",
    "    \n",
    "    # Compute homography matrix\n",
    "    homography_matrix = compute_homography_matrix(landmarks1, landmarks2)\n",
    "    \n",
    "    print(\"Homography Matrix:\")\n",
    "    print(homography_matrix)\n",
    "    \n",
    "    # Ensure the affine matrix is of type float32\n",
    "    homography_matrix = homography_matrix.astype(np.float32)\n",
    "\n",
    "    # Apply the homography transformation using cv2.warpPerspective\n",
    "    transformed_image=cv2.warpPerspective(img2, homography_matrix, (img2.shape[1], img2.shape[0]))\n",
    "    imags.append(transformed_image)\n",
    "    \n",
    "    # Display and save the images\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(\"Stage-{} Results: Registration Using Homography Transformation\".format(snum), fontsize=14, fontweight='bold', y=0.93)\n",
    "\n",
    "    axs[0].imshow(CLAHE_plot_cond(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[0].set_title('Fixed Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(CLAHE_plot_cond(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[1].set_title('Moving Image')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(CLAHE_plot_cond(cv2.cvtColor(transformed_image.astype(np.uint8), cv2.COLOR_BGR2RGB),disp_clip))\n",
    "    axs[2].set_title('Deformed Image')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.show();\n",
    "    \n",
    "    # saving intermediary results for better visualization\n",
    "    imgs.append(os.path.join(rpth, 'Deformed_Image_' + str(num) + '_.png'))\n",
    "    imgs.append(os.path.join(rpth, 'Fixed_' + str(num) + '_.png'))\n",
    "    cv2.imwrite(os.path.join(rpth, 'Fixed_' + str(num) + '_.png'), img1)\n",
    "    cv2.imwrite(os.path.join(rpth, 'Moving_' + str(num) + '_.png'), img2)\n",
    "    cv2.imwrite(os.path.join(rpth,'Deformed_Image_'+str(num)+'_.png'),transformed_image);\n",
    "    return imgs,imags,homography_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c38a6-6548-46c9-bdc7-75f72ce9bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_error(point, transformed_point):\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between the original point and the transformed point.\n",
    "\n",
    "    Parameters:\n",
    "    - point (tuple): Original point (x, y).\n",
    "    - transformed_point (tuple): Transformed point (x, y).\n",
    "\n",
    "    Returns:\n",
    "    - float: Euclidean distance.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(np.array(point) - np.array(transformed_point))\n",
    "\n",
    "def estimate_affine_transformation(points):\n",
    "    \"\"\"\n",
    "    Estimates the affine transformation matrix using point correspondences.\n",
    "\n",
    "    Parameters:\n",
    "    - points (np.array): Array of point correspondences.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Affine transformation matrix.\n",
    "    \"\"\"\n",
    "    src_pts = np.float32([point[0] for point in points])\n",
    "    dst_pts = np.float32([point[1] for point in points])\n",
    "    affine_matrix, _ = cv2.estimateAffinePartial2D(src_pts, dst_pts)\n",
    "    return affine_matrix\n",
    "\n",
    "def estimate_homography_matrix(points):\n",
    "    \"\"\"\n",
    "    Estimates the homography matrix given a set of point correspondences.\n",
    "\n",
    "    Parameters:\n",
    "    - points: A list of tuples, where each tuple contains two (x, y) tuples.\n",
    "              The first tuple in each pair is from the first set of points (set1),\n",
    "              and the second tuple is the corresponding point in the second set (set2).\n",
    "\n",
    "    Returns:\n",
    "    - homography_matrix: The estimated (3x3) homography matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate the points into two sets\n",
    "    set1 = [point[0] for point in points]\n",
    "    set2 = [point[1] for point in points]\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    set1 = np.array(set1, dtype=np.float32)\n",
    "    set2 = np.array(set2, dtype=np.float32)\n",
    "\n",
    "    # Estimate the homography matrix\n",
    "    homography_matrix, _ = cv2.findHomography(set1, set2, cv2.RANSAC)\n",
    "\n",
    "    return homography_matrix\n",
    "\n",
    "def remove_outliers_based_on_error_affine(set1, set2, threshold=20):\n",
    "    \"\"\"\n",
    "    Filters out outlier point pairs from two sets of points by applying an affine transformation and \n",
    "    removing pairs that have an error greater than a specified threshold. The function first estimates\n",
    "    an affine transformation matrix based on all given point pairs. Each point in the first set is then \n",
    "    transformed using this matrix, and the error is calculated as the Euclidean distance between the \n",
    "    transformed point and the corresponding point in the second set. Points with an error exceeding the \n",
    "    threshold are considered outliers and are excluded from the results.\n",
    "\n",
    "    Parameters:\n",
    "    - set1 (list of tuples): A list of (x, y) tuples representing coordinates of points in the first image.\n",
    "    - set2 (list of tuples): A list of (x, y) tuples representing corresponding coordinates of points in the \n",
    "                           second image. The indices in `set1` and `set2` must correspond.\n",
    "    - threshold (float, optional): The maximum allowed error distance between the original and transformed \n",
    "                                 points for them to be considered inliers. Default value is 20.\n",
    "\n",
    "    Returns:\n",
    "    - tuple of lists: Returns two lists (updated_set1, updated_set2) containing the inlier points from\n",
    "                    `set1` and `set2` respectively.\n",
    "    Notes:\n",
    "        It is critical that `set1` and `set2` are of equal length and that the points correspond correctly, \n",
    "        as any misalignment could result in incorrect calculations and poor results.\n",
    "        This function is typically used in image processing and computer vision tasks where alignment and \n",
    "        transformation of point sets between images is required, particularly in stereo vision and motion tracking.\n",
    "    \"\"\"\n",
    "    points = list(zip(set1, set2))\n",
    "    affine_matrix = estimate_affine_transformation(points)\n",
    "    updated_set1 = []\n",
    "    updated_set2 = []\n",
    "\n",
    "    for point1, point2 in zip(set1, set2):\n",
    "        transformed_point = transform_points_affine([point1], affine_matrix)[0]\n",
    "        error = landmark_error(point2, transformed_point)\n",
    "\n",
    "        if error <= threshold:\n",
    "            updated_set1.append(point1)\n",
    "            updated_set2.append(point2)\n",
    "\n",
    "    return updated_set1, updated_set2\n",
    "\n",
    "def remove_outliers_based_on_error_homography(set1, set2, threshold=20):\n",
    "    \"\"\"\n",
    "    Filters out outlier point pairs from two sets of points by applying a homography transformation\n",
    "    and removing pairs that have an error greater than a specified threshold. The function first estimates\n",
    "    a homography transformation matrix based on all given point pairs. Each point in the first set is then\n",
    "    transformed using this matrix, and the error is calculated as the Euclidean distance between the\n",
    "    transformed point and the corresponding point in the second set. Points with an error exceeding the\n",
    "    threshold are considered outliers and are excluded from the results.\n",
    "\n",
    "    Parameters:\n",
    "    - set1 (list of tuples): A list of (x, y) tuples representing coordinates of points in the first image.\n",
    "    - set2 (list of tuples): A list of (x, y) tuples representing corresponding coordinates of points in the\n",
    "                           second image. The indices in `set1` and `set2` must correspond.\n",
    "    - threshold (float, optional): The maximum allowed error distance between the original and transformed \n",
    "                                 points for them to be considered inliers. Default value is 20.\n",
    "\n",
    "    Returns:\n",
    "    - tuple of lists: Returns two lists (updated_set1, updated_set2) containing the inlier points from\n",
    "                        `set1` and `set2` respectively.\n",
    "    Notes:\n",
    "        Ensure that `set1` and `set2` are of equal length and that the points correspond correctly, \n",
    "        as any misalignment could result in incorrect calculations and poor results.\n",
    "        This function is typically used in image processing and computer vision tasks where precise\n",
    "        alignment and transformation of point sets between images are required, especially in applications\n",
    "        like panorama stitching and object tracking.\n",
    "    \"\"\"\n",
    "    points = list(zip(set1, set2))\n",
    "    homography_matrix = estimate_homography_matrix(points)\n",
    "    updated_set1 = []\n",
    "    updated_set2 = []\n",
    "\n",
    "    for point1, point2 in zip(set1, set2):\n",
    "        transformed_point = transform_points_homography([point1], homography_matrix)[0]\n",
    "        error = landmark_error(point2, transformed_point)\n",
    "\n",
    "        if error <= threshold:\n",
    "            updated_set1.append(point1)\n",
    "            updated_set2.append(point2)\n",
    "\n",
    "    return updated_set1, updated_set2\n",
    "\n",
    "def filter_outlier_cond(computed,original,criteria='affine', thresh=20):\n",
    "    \"\"\"\n",
    "    Filters out outliers based on a specified condition.\n",
    "\n",
    "    This function processes two sets of points (computed and original) and filters out outliers based on a specified criteria (either 'affine' or 'homography'). The function uses either homography matrix estimation or affine error-based methods to identify and remove outliers.\n",
    "\n",
    "    Parameters:\n",
    "    - computed (list of tuples): List of computed points as (x, y) coordinates.\n",
    "    - original (list of tuples): List of original points as (x, y) coordinates to compare against.\n",
    "    - criteria (str, optional): The criteria to use for filtering outliers. Options are 'affine' or 'homography'. Defaults to 'affine'.\n",
    "    - thresh (int, optional): Threshold value used in the outlier removal process. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list containing the filtered computed points after outlier removal.\n",
    "    - list: A list containing the filtered original points after outlier removal.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: If the length of the computed points is not 3.\n",
    "\n",
    "    Notes:\n",
    "        If 'homography' is chosen as the criteria, the function estimates a homography matrix between the computed and original points and removes outliers based on the threshold.\n",
    "        If 'affine' is chosen, it removes outliers based on affine transformation error exceeding the threshold.\n",
    "    \"\"\"\n",
    "    assert len(computed) >= 3\n",
    "    if criteria=='homography':\n",
    "        computed,original = estimate_homography_matrix(computed,original,thresh)\n",
    "    else:\n",
    "        computed,original = remove_outliers_based_on_error_affine(computed,original,thresh)\n",
    "    return computed,original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ca666-ea48-4a66-a95d-67724c9cc676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_initialization(images,N,img_size,max_dist,offset,window_size,clip):\n",
    "    \"\"\"\n",
    "    Initializes image processing by applying CLAHE if specified, extracting keypoints using SIFT,\n",
    "    and computing the Discrete Fourier Transform (DFT) for the given images.\n",
    "\n",
    "    Parameters:\n",
    "        - images (list of str): List of image file paths that need processing.\n",
    "        - N (int): Number of keypoints to detect or random points to select.\n",
    "        - img_size (tuple of int): The dimensions (width, height) to which images should be resized.\n",
    "        - max_dist (float): Maximum distance between keypoints for the SIFT algorithm.\n",
    "        - offset (float): Offset used in the selection of random points.\n",
    "        - window_size (int): Size of the window used in random point selection.\n",
    "        - clip (float): Clipping limit for the CLAHE algorithm; if greater than 0, CLAHE is applied.\n",
    "\n",
    "    Returns:\n",
    "        - tuple:\n",
    "            - images (list of np.array): The list of images after processing, possibly enhanced if CLAHE was applied.\n",
    "            - pts(list of tuples): the list of detected points after applying SIFT and Random point sampling on the image.\n",
    "            - dft (np.array): The result of the Discrete Fourier Transform applied on the images.\n",
    "\n",
    "    Notes:\n",
    "        The function begins by extracting SIFT keypoints from the first image and augmenting these with randomly selected points.\n",
    "        It then applies CLAHE if the clipping limit is specified and computes the DFT based on the keypoints and random points.\n",
    "    \"\"\"\n",
    "    pts = SIFT_top_n_keypoints(images[0],N,img_size,max_dist)\n",
    "    pts = pts+select_random_points(images[0],N,img_size,offset,window_size)\n",
    "    if clip > 0: \n",
    "        images = CLAHE_Images(images, clip = clip)\n",
    "    dft = DFT(images,img_size,pts)\n",
    "    return images,pts,dft\n",
    "\n",
    "def CLAHE_Images(imags,clip):\n",
    "    \"\"\"\n",
    "    Applies Contrast Limited Adaptive Histogram Equalization (CLAHE) to a list of image files to enhance \n",
    "    their contrast. This method is particularly useful for improving the visibility of features in images \n",
    "    that suffer from poor contrast.\n",
    "\n",
    "    Parameters:\n",
    "    - imags (list of str): List of paths to the image files that need contrast enhancement.\n",
    "    - clip (float): Clip limit for the CLAHE algorithm, which sets the threshold for contrast limiting. \n",
    "                  The higher the clip limit, the more aggressive the contrast enhancement.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: Returns a list of paths to the saved CLAHE-processed images. Each processed image is \n",
    "                 saved with a \"CLAHE_\" prefix in its filename to distinguish it from the original.\n",
    "\n",
    "    Notes:\n",
    "        This function uses OpenCV's `createCLAHE` method to apply the CLAHE algorithm. Each image is \n",
    "        first converted to grayscale as CLAHE is typically applied to single-channel images for better \n",
    "        visualization of detail.\n",
    "        The images are processed in-place and saved in the same directory as the original, with 'CLAHE_' \n",
    "        prefixed to their original filenames.\n",
    "        It is recommended to adjust the `clip` parameter based on the specific requirements of the image \n",
    "        content and the desired level of contrast enhancement.\n",
    "    \"\"\"\n",
    "    imgs=[]\n",
    "    img_dir = os.path.join(os.getcwd(),'temp_dir')\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip, tileGridSize=(8, 8))\n",
    "    os.makedirs(os.path.join(os.getcwd(),'temp_dir') ,exist_ok=True)\n",
    "    for img in imags:\n",
    "      fn,_ = os.path.splitext(os.path.basename(img))\n",
    "      ifn = os.path.join(img_dir,'CLAHE'+'_'+str(fn)+'.png')\n",
    "      imag = cv2.imread(img)\n",
    "      imag = Image.fromarray(np.uint8(imag))\n",
    "      imag = imag.convert('L')\n",
    "      img = np.asarray(imag)\n",
    "      image_equalized = clahe.apply(img)\n",
    "      image_equalized_img = Image.fromarray(np.uint8(image_equalized))\n",
    "      image_equalized = image_equalized_img.convert('RGB')\n",
    "      image_equalized = np.asarray(image_equalized)\n",
    "      cv2.imwrite(ifn,image_equalized);\n",
    "      imgs.append(ifn)\n",
    "    return imgs\n",
    "\n",
    "def Feature_padding(feature_maps, size):\n",
    "    \"\"\"\n",
    "    Pad feature maps to a uniform size using bilinear interpolation.\n",
    "\n",
    "    This function adjusts the size of each feature map in the input list to a specified uniform size using bilinear interpolation. This is typically used to standardize the size of feature maps obtained from different sources or processes.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_maps (list of tensors): A list of feature map tensors to be resized.\n",
    "    - size (tuple): The target size for the feature maps as (height, width).\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of uniformly sized feature maps.\n",
    "    \"\"\"\n",
    "    uniform_feature_maps=[]\n",
    "    for feature in feature_maps:\n",
    "        uniform_feature_maps.append(F.interpolate(feature, size=size, mode='bilinear', align_corners=False))\n",
    "    return uniform_feature_maps\n",
    "\n",
    "def multi_resolution_features(orig_images,img_size,N,clip,offset,window_size,max_dist,timestep,up_ft_indices,multi_ch,multi_img_size,multi_iter):\n",
    "    \"\"\"\n",
    "    Generate multi-resolution features from images using SIFT, and Random Points.\n",
    "\n",
    "    This function processes images to generate feature maps at multiple resolutions. It combines techniques like SIFT,Random Points Sampler, and CLAHE to enhance and extract features from the images. The function can operate in either multi-channel or single-channel mode.\n",
    "\n",
    "    Parameters:\n",
    "    - orig_images (list of str): List of paths to the images to be processed.\n",
    "    - img_size (int): The size of the images for processing.\n",
    "    - N (int): The number of keypoints to be used in SIFT.\n",
    "    - clip (float): The clip limit for CLAHE.\n",
    "    - max_dist (float): Maximum distance for keypoint selection in SIFT.\n",
    "    - timestep (float): Timestep parameter for Diffusion Model initialization.\n",
    "    - up_ft_indices (list): Indices for feature upsampling in the Diffusion Model.\n",
    "    - multi_ch (bool): Flag to indicate multi-channel mode.\n",
    "    - multi_img_size (int): The size of the images for multi-resolution processing.\n",
    "    - multi_iter (int): Number of iterations for multi-resolution processing.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple of source and target feature tensors.\n",
    "    \"\"\"\n",
    "    if multi_ch:\n",
    "        src_fts,trg_fts =[],[]\n",
    "        for i in range(multi_iter):\n",
    "            images,pts,dft = main_initialization(orig_images,N,multi_img_size*(i+1),max_dist,offset,window_size,clip)\n",
    "            src_ft1,trg_ft1 = dft.feature_upsampling(RetinaRegNet_Intialization(images,multi_img_size*(i+1),timestep,up_ft_indices))\n",
    "            src_fts.append(src_ft1)\n",
    "            trg_fts.append(trg_ft1)\n",
    "        src_fts = Feature_padding(src_fts,(img_size,img_size))\n",
    "        trg_fts = Feature_padding(trg_fts,(img_size,img_size))\n",
    "        src_ft = torch.cat(src_fts, dim=1)\n",
    "        trg_ft = torch.cat(trg_fts, dim=1)\n",
    "    else:\n",
    "        images,pts,dft = main_initialization(orig_images,N,img_size,max_dist,offset,window_size,clip)\n",
    "        src_ft,trg_ft = dft.feature_upsampling(RetinaRegNet_Intialization(images,img_size,timestep,up_ft_indices))\n",
    "    return src_ft,trg_ft\n",
    "\n",
    "def landmarks_condition_check(orig_images, img_size, pts, t, uft, landmarks1, landmarks2, max_tries=2, num=100, iccl=3, outlier_cond='affine', thresh=20):\n",
    "    \"\"\"\n",
    "    Iteratively attempts to improve image registration quality by enhancing image contrast and adjusting landmarks\n",
    "    until certain quality conditions are met or a maximum number of attempts is reached. This function applies CLAHE\n",
    "    for image contrast enhancement and uses various feature transformation and scaling techniques to improve the accuracy\n",
    "    of landmark correspondences between two images.\n",
    "\n",
    "    Parameters:\n",
    "    - orig_images (list of str): Paths to the original images to be processed.\n",
    "    - img_size (int): Size of the images to be processed, assumed to be square.\n",
    "    - pts (list): List of all sampled feature keypoints in the image\n",
    "    - t (float): Threshold parameter for initializing the Diffusion Model.\n",
    "    - uft (float): Parameter for extracting diffusion features from the diffusion model.\n",
    "    - landmarks1 (list of tuples): Initial landmarks as (x, y) coordinates in the first image.\n",
    "    - landmarks2 (list of tuples): Target landmarks as (x, y) coordinates in the second image.\n",
    "    - max_tries (int, optional): Maximum number of attempts to improve image registration. Defaults to 2.\n",
    "    - num (int, optional): Minimum required number of landmarks. Defaults to 100.\n",
    "    - iccl (float, optional): Inverse consistency criteria limit used in landmark filtering. Defaults to 3.\n",
    "    - outlier_cond (str, optional): Condition used to determine outliers. Defaults to 'affine'.\n",
    "    - thresh (float, optional): Threshold used for filtering outliers. Defaults to 20.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Depending on the success of the registration process, this function returns:\n",
    "        The original images and the best set of landmarks found, or\n",
    "        The original images and a set of default landmarks if conditions are not met.\n",
    "\n",
    "    Raises:\n",
    "    - AssertionError: If the number of initial and target landmarks do not match.\n",
    "\n",
    "    Notes:\n",
    "        This function is particularly useful in medical imaging or computer vision tasks where accurate image\n",
    "        registration is crucial for further analysis.\n",
    "        The effectiveness of the registration process depends heavily on the quality and accuracy of the input landmarks.\n",
    "        CLAHE and other image processing techniques may not always produce the desired results if the input images\n",
    "        are of poor quality or the initial landmarks are inaccurately defined.\n",
    "    \"\"\"\n",
    "    imgs,lim,land_marks1, land_marks2,list_landmarks_2, list_sim_scores,list_landmarks_1,temp = [], [], [], [], [],[],[],[]\n",
    "    tries, ch, = 0, 0\n",
    "    assert len(landmarks1) == len(landmarks2), f\"Points lengths are incompatible: {len(landmarks1)} != {len(landmarks2)}.\"\n",
    "    landmarks2,landmarks1 = filter_outlier_cond(landmarks2,landmarks1,outlier_cond,thresh)\n",
    "    list_landmarks_1.append(landmarks1)\n",
    "    imgs.append(orig_images)\n",
    "    list_landmarks_2.append(landmarks2)\n",
    "    if len(landmarks2) < num:\n",
    "        print(\"Image Registration Unsuccessful for Original Set of Images\")\n",
    "        while len(land_marks2) < num and tries< max_tries:\n",
    "            print(\"Executing Trial\", tries + 1)\n",
    "            dft = DFT(orig_images, img_size, pts)\n",
    "            src_ft,trg_ft = dft.feature_upsampling(RetinaRegNet_Intialization(orig_images,img_size,t + 75*tries,uft))\n",
    "            land_marks1,sim_score, land_marks2 = dft.feature_maps(src_ft,trg_ft,iccl)\n",
    "            del src_ft\n",
    "            del trg_ft\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            land_marks2,land_marks1 = filter_outlier_cond(land_marks2,land_marks1,outlier_cond,thresh)\n",
    "            list_landmarks_1.append(land_marks1)\n",
    "            imgs.append(images)\n",
    "            list_landmarks_2.append(land_marks2)\n",
    "            list_sim_scores.append(np.mean(sim_score))\n",
    "            tries += 1\n",
    "        for i in range(len(list_landmarks_2)):\n",
    "            lim.append(len(list_landmarks_2[i]))\n",
    "        idx = np.argmax(np.array(lim))\n",
    "        return orig_images,list_landmarks_1[idx],list_landmarks_2[idx]\n",
    "    else:\n",
    "        return orig_images, landmarks1, landmarks2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70358021-0f5b-46b5-9cf6-84310ef5d006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def folder_structure(path,nfn):\n",
    "    \"\"\"\n",
    "    Creates a hierarchical folder structure for saving image registration results across different stages.\n",
    "\n",
    "    The function constructs directories for three stages of image registration results for each name provided in the list.\n",
    "    It ensures that the directory for each stage exists, or creates it if it does not exist. This setup is intended\n",
    "    to organize output from multiple stages of processing in separate folders under a common root directory specified by `path`.\n",
    "\n",
    "    Parameters:\n",
    "        - path (str): The base path where the 'Image_Registration_Results' directory will be created.\n",
    "        - nfn (list of str): A list of sub-folder names to create under each stage directory.\n",
    "\n",
    "    Returns:\n",
    "        - None: This function only creates directories and does not return any value.\n",
    "\n",
    "    Notes:\n",
    "        This function uses `os.makedirs()` with `exist_ok=True` to ensure that no error is raised if the directories already exist.\n",
    "        It is useful for setting up an organized structure for storing outputs from different stages of image processing tasks.\n",
    "    \"\"\"\n",
    "    for i in range(len(nfn)):\n",
    "        os.makedirs(os.path.join(path+'_'+'Image_Registration_Results','Stage1', nfn[i]), exist_ok=True)\n",
    "        os.makedirs(os.path.join(path+'_'+'Image_Registration_Results','Stage2', nfn[i]), exist_ok=True)\n",
    "        os.makedirs(os.path.join(path+'_'+'Image_Registration_Results','Final_Registration_Results', nfn[i]), exist_ok=True)\n",
    "    print(\"Created {} Sub Folders for saving Registration Results\".format(len(nfn)))\n",
    "\n",
    "def images_oganization(images):\n",
    "    \"\"\"\n",
    "    Organizes images into pairs for processing.\n",
    "\n",
    "    This function takes a list of images and rearranges them into pairs. If the number of images\n",
    "    is even, it swaps each pair (e.g., [img1, img2] becomes [img2, img1]). If the number is odd,\n",
    "    it prints a warning indicating that some images do not have a pair.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list of str): A list of image file paths.\n",
    "\n",
    "    Returns:\n",
    "    - imags (list of list of str): A list containing reordered pairs of image file paths.\n",
    "\n",
    "    Raises:\n",
    "    - UserWarning: Raises a warning if the number of images is odd, indicating not all images can be paired.\n",
    "    \"\"\"\n",
    "    imags = []\n",
    "    if len(images) % 2 == 0:\n",
    "        for i in range(0, len(images), 2):  # Loop with a step of 2\n",
    "            imags.append([images[i+1], images[i]])\n",
    "    else:\n",
    "        print(\"Some Images do not have a pair\") \n",
    "    return imags\n",
    "\n",
    "def sub_files_oganization(images,pnts):\n",
    "    \"\"\"\n",
    "    Organizes images and corresponding points into three categories based on their naming conventions.\n",
    "\n",
    "    This function categorizes images and their corresponding point sets into three lists based on the\n",
    "    first letter of the file names ('A', 'P', 'S'). It helps in sorting images for different processing\n",
    "    pipelines or data handling strategies.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list of tuple): List of tuples, each containing the file paths of images.\n",
    "    - pnts (list): List of corresponding point data associated with each image.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Contains six lists organized into three groups:\n",
    "        - imags_A, pnts_A: Lists containing images and points starting with 'A'.\n",
    "        - imags_P, pnts_P: Lists containing images and points starting with 'P'.\n",
    "        - imags_S, pnts_S: Lists containing images and points starting with 'S'.\n",
    "\n",
    "    Note:\n",
    "      It is assumed that the image filenames are structured in a way that their categorization can be discerned\n",
    "      from the first letter of the basename of the file path.\n",
    "    \"\"\"\n",
    "    imags_A ,imags_P,imags_S = [],[],[]\n",
    "    pnts_A ,pnts_P,pnts_S = [],[],[]\n",
    "    for i in range(len(images)):\n",
    "        if os.path.splitext(os.path.basename(images[i][0]))[0][:1] =='A':\n",
    "            imags_A.append(images[i])\n",
    "            pnts_A.append(pnts[i])\n",
    "        elif os.path.splitext(os.path.basename(images[i][0]))[0][:1] =='P':\n",
    "            imags_P.append(images[i])\n",
    "            pnts_P.append(pnts[i])\n",
    "        else:\n",
    "            imags_S.append(images[i])\n",
    "            pnts_S.append(pnts[i])\n",
    "    return imags_A,pnts_A,imags_P,pnts_P,imags_S,pnts_S\n",
    "\n",
    "\n",
    "def text_points_parser(pnts):\n",
    "    \"\"\"\n",
    "    Extract point coordinates from a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - pnts (str): Path to the text file containing point coordinates.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Lists of fixed points and moving points.\n",
    "    \"\"\"\n",
    "    fixed_pnts = []\n",
    "    moving_pnts = []\n",
    "    with open(pnts, 'r') as file:\n",
    "        for line in file:\n",
    "            points = [float(coord) for coord in line.strip().split()]\n",
    "            fps = tuple(points[:2])\n",
    "            lps = tuple(points[2:])\n",
    "            fixed_pnts.append(fps)\n",
    "            moving_pnts.append(lps)\n",
    "    return fixed_pnts,moving_pnts\n",
    "\n",
    "def coordinates_rescaling_high_scale(pnts,H,W,img_shape):\n",
    "    \"\"\"\n",
    "    Rescale a list of coordinates based on given height and width ratios.\n",
    "\n",
    "    Parameters:\n",
    "    - pnts (list of tuples): List of (x, y) coordinates to be rescaled.\n",
    "    - H (int): Original height.\n",
    "    - W (int): Original width.\n",
    "    - img_shape (int): Desired image dimension (assumes square shape).\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: List of rescaled (x, y) coordinates.\n",
    "    \"\"\"\n",
    "    scaled_points=[]\n",
    "    for row in pnts:\n",
    "        a = (row[0]/W)*img_shape[1]\n",
    "        b = (row[1]/H)*img_shape[0]\n",
    "        scaled_points.append((a,b))\n",
    "    return scaled_points\n",
    "\n",
    "\n",
    "def coordinates_rescaling(pnts,H,W,img_shape):\n",
    "    \"\"\"\n",
    "    Rescale a list of coordinates based on given height and width ratios.\n",
    "\n",
    "    Parameters:\n",
    "    - pnts (list of tuples): List of (x, y) coordinates to be rescaled.\n",
    "    - H (int): Original height.\n",
    "    - W (int): Original width.\n",
    "    - img_shape (int): Desired image dimension (assumes square shape).\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: List of rescaled (x, y) coordinates.\n",
    "    \"\"\"\n",
    "    scaled_points=[]\n",
    "    for row in pnts:\n",
    "        a = (row[0]/W)*img_shape\n",
    "        b = (row[1]/H)*img_shape\n",
    "        scaled_points.append((a,b))\n",
    "    return scaled_points\n",
    "    \n",
    "def coordinates_processing(image1,image2,fpnts,mpnts,img_shape=256):\n",
    "    \"\"\"\n",
    "    Process and rescale coordinates for two images.\n",
    "\n",
    "    Parameters:\n",
    "    - image1 (str): Path to the first image.\n",
    "    - image2 (str): Path to the second image.\n",
    "    - fpnts (list of tuples): List of (x, y) coordinates related to the first image.\n",
    "    - mpnts (list of tuples): List of (x, y) coordinates related to the second image.\n",
    "    - img_shape (int, optional): Desired image dimension for rescaling. Default is 256.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing:\n",
    "        - Tuple: Height and Width of the second image.\n",
    "        - Tuple: Height and Width of the first image.\n",
    "        - int: Maximum of the heights and widths of both images.\n",
    "        - list of tuples: Scaled coordinates for the first image.\n",
    "        - list of tuples: Scaled coordinates for the second image.\n",
    "        - list of tuples: Scaled original coordinates for the second image.\n",
    "    \"\"\"\n",
    "    H1,W1,C1 = cv2.imread(image1).shape\n",
    "    H2,W2,C2 = cv2.imread(image2).shape\n",
    "    scaled_moving_points = coordinates_rescaling(mpnts,H1,W1,img_shape)\n",
    "    scaled_fixed_points = coordinates_rescaling(fpnts,H2,W2,img_shape)\n",
    "    scaled_original_moving_points = coordinates_rescaling(mpnts,H1,W1,max(max(H1,W1),max(H2,W2))) #4000*4000\n",
    "    return (H2,W2),(H1,W1),max(max(H1,W1),max(H2,W2)),scaled_fixed_points,scaled_moving_points,scaled_original_moving_points\n",
    "\n",
    "def feature_scaling(images,fixed_points,moving_points,img_shape):\n",
    "    \"\"\"\n",
    "    Apply feature scaling to given images and their associated points.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list): List of tuples containing image paths for fixed and moving images.\n",
    "    - fixed_points (list): List of fixed points corresponding to each image.\n",
    "    - moving_points (list): List of moving points corresponding to each image.\n",
    "    - img_shape (int): Desired image dimension for rescaling.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing:\n",
    "        - list: Sizes of fixed images.\n",
    "        - list: Sizes of moving images.\n",
    "        - list: Maximum of the heights and widths of the images.\n",
    "        - list: Fixed points after scaling.\n",
    "        - list: Moving points after scaling.\n",
    "        - list: Scaled moving points.\n",
    "    \"\"\"\n",
    "    fixed_image_size,moving_image_size,max_image_size,fixed_pointss,moving_pointss,scaled_moving_points =[],[],[],[],[],[]\n",
    "    for i in range(len(images)):\n",
    "        fhs,mhss,mhs,fpnts,mpnts,scmpnts = coordinates_processing(images[i][0],images[i][1],fixed_points[i],moving_points[i],img_shape)\n",
    "        fixed_image_size.append(fhs)\n",
    "        moving_image_size.append(mhss)\n",
    "        max_image_size.append(mhs)\n",
    "        fixed_pointss.append(fpnts)\n",
    "        moving_pointss.append(mpnts)\n",
    "        scaled_moving_points.append(scmpnts)\n",
    "    return fixed_image_size,moving_image_size,max_image_size,fixed_pointss,moving_pointss,scaled_moving_points\n",
    "\n",
    "def text_file_processing(cnts):\n",
    "    \"\"\"\n",
    "    Processes a list of text files containing point data to extract fixed and moving points.\n",
    "\n",
    "    Parameters:\n",
    "    - cnts (list of str): A list of file paths where each file contains coordinate data.\n",
    "\n",
    "    Returns:\n",
    "    - tuple of lists: A tuple containing two lists:\n",
    "        - fixed_points (list): A list of fixed points extracted from the text files.\n",
    "        - moving_points (list): A list of moving points extracted from the text files.\n",
    "\n",
    "    Notes:\n",
    "      This function skips files named '.ipynb_checkpoints' and only processes valid text files.\n",
    "      Each file is expected to have fixed and moving points in a specific format, parsed by `text_points_parser`.\n",
    "    \"\"\"\n",
    "    fixed_points,moving_points =[],[]\n",
    "    for i in cnts:\n",
    "        if os.path.isfile(i) and i != '.ipynb_checkpoints':\n",
    "            fps,mps = text_points_parser(i)\n",
    "            fixed_points.append(fps)\n",
    "            moving_points.append(mps)\n",
    "        else:\n",
    "            continue\n",
    "    return fixed_points,moving_points\n",
    "\n",
    "def data_organization(pth,img_shape=256,files =['Images','Ground Truth']):\n",
    "    \"\"\"\n",
    "    Organizes and processes image and point data from specified directories.\n",
    "\n",
    "    Parameters:\n",
    "    - pth (str): The base directory path that contains the 'Images' and 'Ground Truth' directories.\n",
    "    - img_shape (int, optional): The target size for resizing the images. Defaults to 256.\n",
    "    - files (list of str, optional): The list of directory names to process. Defaults to ['Images', 'Ground Truth'].\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Contains a large number of elements, including organized lists of image file paths, point data,\n",
    "             and metadata about image and point processing.\n",
    "\n",
    "    Notes:\n",
    "      This function segregates images and point data based on their filename initials into different categories\n",
    "      such as 'A', 'P', 'S' for different processing tasks.\n",
    "      Each category of files is further processed to extract and scale point data relevant to image registration tasks.\n",
    "      The function uses several other functions such as `folder_structure`, `images_organization`, and\n",
    "      `sub_files_organization` to structure and organize data.\n",
    "    \"\"\"\n",
    "    images , cnts ,fns,fnn  =[], [], [] ,[]\n",
    "    for i in files:\n",
    "        for j in sorted(os.listdir(os.path.join(pth,str(i)))):\n",
    "            if i == 'Images':\n",
    "                images.append(os.path.join(pth,str(i),j))\n",
    "                fns.append(j[0])\n",
    "            else:\n",
    "                cnts.append(os.path.join(pth,str(i),j))\n",
    "    images = [img for img in images if not img.startswith('.')]\n",
    "    cnts = [pnt for pnt in cnts if not pnt.startswith('.')]\n",
    "    folder_structure(pth,np.unique(fns))\n",
    "    images = images_oganization(images)\n",
    "    images_A,cnts_A,images_P,cnts_P,images_S,cnts_S = sub_files_oganization(images,cnts)\n",
    "    fixed_points , moving_points = text_file_processing(cnts)\n",
    "    fixed_points_A , moving_points_A = text_file_processing(cnts_A)\n",
    "    fixed_points_P , moving_points_P = text_file_processing(cnts_P)\n",
    "    fixed_points_S , moving_points_S = text_file_processing(cnts_S)\n",
    "    fixed_image_size,moving_image_size,max_image_size,fixed_pointss,moving_pointss,scaled_moving_points = feature_scaling(images,fixed_points,moving_points,img_shape)\n",
    "    fixed_image_size_A,moving_image_size_A,max_image_size_A,fixed_pointss_A,moving_pointss_A,scaled_moving_points_A = feature_scaling(images_A,fixed_points_A , moving_points_A,img_shape)\n",
    "    fixed_image_size_P,moving_image_size_P,max_image_size_P,fixed_pointss_P,moving_pointss_P,scaled_moving_points_P = feature_scaling(images_P,fixed_points_P , moving_points_P,img_shape)\n",
    "    fixed_image_size_S,moving_image_size_S,max_image_size_S,fixed_pointss_S,moving_pointss_S,scaled_moving_points_S = feature_scaling(images_S,fixed_points_S , moving_points_S,img_shape)\n",
    "    return images,images_A,images_P,images_S,fixed_image_size,fixed_image_size_A,fixed_image_size_P,fixed_image_size_S,moving_image_size,moving_image_size_A,moving_image_size_P,moving_image_size_S,max_image_size,max_image_size_A,max_image_size_P,max_image_size_S,fixed_points,fixed_points_A,fixed_points_P,fixed_points_S,moving_points_A,moving_points_P,moving_points_S,fixed_pointss,fixed_pointss_A,fixed_pointss_P,fixed_pointss_S,moving_pointss,moving_pointss_A,moving_pointss_P,moving_pointss_S,scaled_moving_points,scaled_moving_points_A,scaled_moving_points_P,scaled_moving_points_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5586c68-cd9d-40c4-9d78-5def613ab482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RetinaRegNet_Intialization(filelist,img_size = 256,timestep = 75,up_ft_index = 2):\n",
    "    \"\"\"\n",
    "    Initialize RetinaRegNet by processing a list of image files.\n",
    "\n",
    "    Parameters:\n",
    "    - filelist (list of str): List of paths to image files for feature extraction.\n",
    "    - img_size (int, optional): Desired size for resizing images. Default is 256.\n",
    "    - timestep (int, optional): Time step for the intializing the diffusion model. Default is 75.\n",
    "    - up_ft_index (int, optional): Index for the extracting diffusion features from the diffusion model . Default is 2\n",
    "\n",
    "    Returns:\n",
    "    - ft (torch.Tensor): A tensor containing the Diffusion features of the images in the list.\n",
    "\n",
    "    Notes:\n",
    "        The function uses the SDFeaturizer from the 'stabilityai/stable-diffusion-2-1' model to extract stable diffusion features\n",
    "        from each image. After processing all images, the extracted features are concatenated into a single tensor.\n",
    "        To avoid memory issues, the function cleans up resources after processing.\n",
    "    \"\"\"\n",
    "    ft = []\n",
    "    imglist = []\n",
    "    dfm = SDFeaturizer(sd_id='stabilityai/stable-diffusion-2-1')\n",
    "    for filename in filelist:\n",
    "        img = Image.open(filename).convert('RGB')\n",
    "        img = img.resize((img_size, img_size))\n",
    "        imglist.append(img)\n",
    "        img_tensor = (PILToTensor()(img) / 255.0 - 0.5) * 2\n",
    "        ft.append(dfm.forward(img_tensor,\n",
    "                               timestep,\n",
    "                               up_ft_index,\n",
    "                               prompt='FIRE',\n",
    "                               ensemble_size=8))\n",
    "    ft = torch.cat(ft, dim=0)\n",
    "\n",
    "    del dfm\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0922c74-90c7-4f86-a35a-35071920b335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(orig_images,rpth,ifn,stage_num,img_size=256,up_ft_indices = 1,timestep = 75,N=50,offset=0.01,window_size=51,max_dist =5,iccl=3,outlier_cond='affine',thresh=20,max_tries=3,num=50,clip = 1.0, disp_clip=0.0, multi_ch=True,multi_iter=3, multi_img_size=256):\n",
    "    \"\"\"\n",
    "    Perform image registration and point correspondence using a series of processing steps.\n",
    "\n",
    "    Parameters:\n",
    "    - images (list): A list of input images for registration.\n",
    "    - rpth (str): Path to save the resulting registered images.\n",
    "    - ifn (str): File name prefix for the saved images.\n",
    "    - stage_num (int): Stage number for referencing in plots and outputs.\n",
    "    - img_size (int, optional): Size of the input images (default is 256).\n",
    "    - up_ft_indices (int, optional): Up-sampling factor for feature indices (default is 1).\n",
    "    - timestep (int, optional): Time step for feature extraction (default is 75).\n",
    "    - N (int, optional): Number of keypoints to extract (default is 50).\n",
    "    - offset (float, optional): Offset parameter for feature extraction (default is 0.01).\n",
    "    - window_size (int, optional): Size of the window for feature extraction (default is 51).\n",
    "    - max_dist (int, optional): Maximum distance for feature matching (default is 5).\n",
    "    - iccl (int, optional): ICC level for feature matching (default is 3).\n",
    "    - outlier_cond (str, optional): Condition for outlier removal (default is 'affine').\n",
    "    - thresh (int, optional): Threshold value for outlier removal (default is 20).\n",
    "    - max_tries (int, optional): Maximum number of attempts for matching features (default is 3).\n",
    "    - num (int, optional): Number of iterations for matching features (default is 50).\n",
    "    - clip (float, optional): Clip parameter for image enhancement (default is 1.0).\n",
    "    - disp_clip (float, optional): Clip parameter for enhancing quality of images in plots (default is 0.0).\n",
    "    - multi_ch (bool, optional): Flag indicating whether to use multi-channel processing (default is True).\n",
    "    - multi_iter (int, optional): Number of iterations for multi-channel processing (default is 3).\n",
    "    - multi_img_size (int, optional): Size of images for multi-channel processing (default is 256).\n",
    "\n",
    "    Returns:\n",
    "    - original (list): List of original image points.\n",
    "    - computed (list): List of computed image points after registration.\n",
    "\n",
    "    Note:\n",
    "        This function performs various processing steps including feature extraction, feature matching,\n",
    "        outlier removal, and image registration.\n",
    "        It saves the resulting registered images in the specified directory.\n",
    "        If the image registration is unsuccessful, empty lists are returned for both original and computed points.\n",
    "    \"\"\"\n",
    "    images,pts,dft = main_initialization(orig_images,N,img_size,max_dist,offset,window_size,clip)\n",
    "    src_ft,trg_ft = multi_resolution_features(orig_images,img_size,N,clip,offset,window_size,max_dist,timestep,up_ft_indices,multi_ch,multi_img_size,multi_iter)\n",
    "    pnts,rmaxs, rspts = dft.feature_maps(src_ft,trg_ft,iccl)\n",
    "    del src_ft\n",
    "    del trg_ft\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    images,original,computed = landmarks_condition_check(images, img_size, pts, timestep, up_ft_indices, pnts, rspts, max_tries, num, iccl, outlier_cond, thresh)\n",
    "    if len(computed)!=0:\n",
    "        image_point_correspondences(images[::-1],img_size,computed,original,rpth,ifn,stage_num,disp_clip=disp_clip)\n",
    "        return original,computed\n",
    "    else:\n",
    "        print(\"Image Registration is Unsuccessful for the presented Images due to unsufficent Matching Features\")\n",
    "        return [],[]\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd2475-bf7d-4815-b85c-7ef5d589a867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_size = 920\n",
    "images,images_A,images_P,images_S,fixed_image_size,fixed_image_size_A,fixed_image_size_P,fixed_image_size_S,moving_image_size,moving_image_size_A,moving_image_size_P,moving_image_size_S,max_image_size,max_image_size_A,max_image_size_P,max_image_size_S,fixed_points,fixed_points_A,fixed_points_P,fixed_points_S,moving_points_A,moving_points_P,moving_points_S,scaled_fixed_points,scaled_fixed_points_A,scaled_fixed_points_P,scaled_fixed_points_S,scaled_moving_points,scaled_moving_points_A,scaled_moving_points_P,scaled_moving_points_S,scaled_original_moving_points,scaled_original_moving_points_A,scaled_original_moving_points_P,scaled_original_moving_points_S = data_organization(os.path.join(os.getcwd(),'FIRE'),img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8a90fb-1acf-4bb7-8ade-ee0a5521b987",
   "metadata": {},
   "source": [
    "#### Class-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93141c7f-8df8-4100-a430-51da4755152f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "landmark_errors1=[]\n",
    "for i in range(len(images_A)):\n",
    "    print(\"Case {}\".format(i))\n",
    "    print(\"Loading Fixed Images {0} Moving Image{1} to the framework\".format(images_A[i][1],images_A[i][0]))\n",
    "    original_low_res,computed_low_res = main(images_A[i],os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage1','A'),str(i),str(1),img_size,up_ft_indices = 2,timestep = 1,N=1000,offset=0.01,window_size=51,max_dist = 10,iccl=3,outlier_cond='affine',thresh=25, max_tries=2,num=100,clip = 0.0,disp_clip=0.0,multi_ch=False,multi_iter=4, multi_img_size=230)\n",
    "    imags,imgs,homography_matrix_low_res = compute_homography_matrix_and_plot(images_A[i][::-1], img_size,original_low_res,computed_low_res,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage1','A'),str(i),str(1),disp_clip=0.0)\n",
    "    if len(homography_matrix_low_res) !=0:    \n",
    "        transformed_points_hom = transform_points_homography(scaled_moving_points_A[i],homography_matrix_low_res)\n",
    "        transformed_points_high_res_hom =  coordinates_rescaling(transformed_points_hom,img_size,img_size,max_image_size_A[i])\n",
    "        original_low_res,computed_low_res = main(imags,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage2','A'),str(i),str(2),img_size,up_ft_indices = 2,timestep = 1,N=1000,offset=0.01,window_size=51,max_dist = 10,iccl=3,outlier_cond='affine',thresh=15, max_tries=2,num=100,clip = 0.0,disp_clip=0.0,multi_ch=False,multi_iter=4, multi_img_size=230)\n",
    "        imag,imgs,polynomial_matrix_low_res = compute_third_order_polynomial_matrix_and_plot(imags[::-1], img_size,original_low_res,computed_low_res,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage2','A'),str(i),str(2),disp_clip=0.0)\n",
    "        if len(polynomial_matrix_low_res) !=0:\n",
    "            ## rescaled version for dispaly purposes\n",
    "            transformed_points_poly = transform_points_third_order_polynomial(transformed_points_hom, polynomial_matrix_low_res)\n",
    "            original_image_point_correspondences(imag,images_A[i][0],img_size, scaled_fixed_points_A[i], scaled_moving_points_A[i], transformed_points_poly,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Final_Registration_Results','A'), str(i),disp_clip=0.0)\n",
    "            ### Original Version for computation of errors\n",
    "            polynomial_matrix = transform_points_third_order_polynomial_matrix(original_low_res,computed_low_res,img_size,max_image_size_A[i])\n",
    "            bef_error = compute_landmark_error(fixed_points_A[i],fixed_image_size_A[i],moving_points_A[i],moving_image_size_A[i],max_image_size_A[i])\n",
    "            aft_error = compute_landmark_error_fixed_space(polynomial_matrix,fixed_points_A[i],transformed_points_high_res_hom,max_image_size_A[i],fixed_image_size_A[i])\n",
    "            print(\"Mean Landmark Error for Case {0} Before Registration is {1} pixels\".format(i,bef_error))\n",
    "            print(\"Mean Landmark Error for Case {0} After Registration is {1} pixels\".format(i,aft_error))\n",
    "            landmark_errors1.append(aft_error)\n",
    "        else:\n",
    "            landmark_errors1.append(10000)\n",
    "    else:\n",
    "        landmark_errors1.append(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52884528-f4b1-4d59-8637-b44e781fdac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_landmark_errors(landmark_errors1,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Final_Registration_Results','A'),'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d38b4-9a19-49aa-a76e-e4ee372c2f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_plot_FIRE_AUC(landmark_errors1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cffd96-572c-4fae-b721-b14ed43d4074",
   "metadata": {},
   "source": [
    "#### Class-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ebb96-405d-4eab-a06a-e1351b3d27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_errors2=[]\n",
    "for i in range(len(images_P)):\n",
    "    print(\"Case {}\".format(i))\n",
    "    print(\"Loading Fixed Images {0} Moving Image{1} to the framework\".format(images_P[i][1],images_P[i][0]))\n",
    "    original_low_res,computed_low_res = main(images_P[i],os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage1','P'),str(i),str(1),img_size,up_ft_indices = 2,timestep = 1,N=1000,offset=0.01,window_size=51,max_dist = 10,iccl=3,outlier_cond='affine',thresh=25, max_tries=2,num=100,clip = 0.0,disp_clip=0.0,multi_ch=False,multi_iter=4, multi_img_size=230)\n",
    "    imags,imgs,homography_matrix_low_res = compute_homography_matrix_and_plot(images_P[i][::-1], img_size,original_low_res,computed_low_res,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage1','P'),str(i),str(1),disp_clip=0.0)\n",
    "    if len(homography_matrix_low_res) !=0:    \n",
    "        transformed_points_hom = transform_points_homography(scaled_moving_points_P[i],homography_matrix_low_res)\n",
    "        transformed_points_high_res_hom =  coordinates_rescaling(transformed_points_hom,img_size,img_size,max_image_size_P[i])\n",
    "        original_low_res,computed_low_res = main(imags,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage2','P'),str(i),str(2),img_size,up_ft_indices = 2,timestep = 1,N=1000,offset=0.01,window_size=51,max_dist = 10,iccl=3,outlier_cond='affine',thresh=15, max_tries=2,num=100,clip = 0.0,disp_clip=0.0,multi_ch=False,multi_iter=4, multi_img_size=230)\n",
    "        imag,imgs,polynomial_matrix_low_res = compute_third_order_polynomial_matrix_and_plot(imags[::-1], img_size,original_low_res,computed_low_res,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage2','P'),str(i),str(2),disp_clip=0.0)\n",
    "        if len(polynomial_matrix_low_res) !=0:\n",
    "            ## rescaled version for dispaly purposes\n",
    "            transformed_points_poly = transform_points_third_order_polynomial(transformed_points_hom, polynomial_matrix_low_res)\n",
    "            original_image_point_correspondences(imag,images_P[i][0],img_size, scaled_fixed_points_P[i], scaled_moving_points_P[i], transformed_points_poly,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Final_Registration_Results','P'), str(i),disp_clip=0.0)\n",
    "            ### Original Version for computation of errors\n",
    "            polynomial_matrix = transform_points_third_order_polynomial_matrix(original_low_res,computed_low_res,img_size,max_image_size_P[i])\n",
    "            bef_error = compute_landmark_error(fixed_points_P[i],fixed_image_size_P[i],moving_points_P[i],moving_image_size_P[i],max_image_size_P[i])\n",
    "            aft_error = compute_landmark_error_fixed_space(polynomial_matrix,fixed_points_P[i],transformed_points_high_res_hom,max_image_size_P[i],fixed_image_size_P[i])\n",
    "            print(\"Mean Landmark Error for Case {0} Before Registration is {1} pixels\".format(i,bef_error))\n",
    "            print(\"Mean Landmark Error for Case {0} After Registration is {1} pixels\".format(i,aft_error))\n",
    "            landmark_errors2.append(aft_error)\n",
    "        else:\n",
    "            landmark_errors2.append(10000)\n",
    "    else:\n",
    "        landmark_errors2.append(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075587d-ddcf-4470-a193-12b6dba0cc18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_landmark_errors(landmark_errors2,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Final_Registration_Results','P'),'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e00c1-365a-480c-b569-d66d3afffe7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_plot_FIRE_AUC(landmark_errors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29e28a-c58d-436b-98eb-4890394096da",
   "metadata": {},
   "source": [
    "#### Class-S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae467d-47f1-4c40-82c8-9923ededd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_errors3=[]\n",
    "for i in range(len(images_S)):\n",
    "    print(\"Case {}\".format(i))\n",
    "    print(\"Loading Fixed Images {0} Moving Image{1} to the framework\".format(images_S[i][1],images_S[i][0]))\n",
    "    original_low_res,computed_low_res = main(images_S[i],os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage1','S'),str(i),str(1),img_size,up_ft_indices = 2,timestep = 1,N=1000,offset=0.01,window_size=51,max_dist = 10,iccl=3,outlier_cond='affine',thresh=25, max_tries=2,num=100,clip = 0.0,disp_clip=0.0,multi_ch=False,multi_iter=4, multi_img_size=230)\n",
    "    imags,imgs,homography_matrix_low_res = compute_homography_matrix_and_plot(images_S[i][::-1], img_size,original_low_res,computed_low_res,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage1','S'),str(i),str(1),disp_clip=0.0)\n",
    "    if len(homography_matrix_low_res) !=0:    \n",
    "        transformed_points_hom = transform_points_homography(scaled_moving_points_S[i],homography_matrix_low_res)\n",
    "        transformed_points_high_res_hom =  coordinates_rescaling(transformed_points_hom,img_size,img_size,max_image_size_S[i])\n",
    "        original_low_res,computed_low_res = main(imags,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage2','S'),str(i),str(2),img_size,up_ft_indices = 2,timestep = 1,N=1000,offset=0.01,window_size=51,max_dist = 10,iccl=3,outlier_cond='affine',thresh=15, max_tries=2,num=100,clip = 0.0,disp_clip=0.0,multi_ch=False,multi_iter=4, multi_img_size=230)\n",
    "        imag,imgs,polynomial_matrix_low_res = compute_third_order_polynomial_matrix_and_plot(imags[::-1], img_size,original_low_res,computed_low_res,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Stage2','S'),str(i),str(2),disp_clip=0.0)\n",
    "        if len(polynomial_matrix_low_res) !=0:\n",
    "            ## rescaled version for dispaly purposes\n",
    "            transformed_points_poly = transform_points_third_order_polynomial(transformed_points_hom, polynomial_matrix_low_res)\n",
    "            original_image_point_correspondences(imag,images_S[i][0],img_size, scaled_fixed_points_S[i], scaled_moving_points_S[i], transformed_points_poly,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Final_Registration_Results','S'), str(i),disp_clip=0.0)\n",
    "            ### Original Version for computation of errors\n",
    "            polynomial_matrix = transform_points_third_order_polynomial_matrix(original_low_res,computed_low_res,img_size,max_image_size_S[i])\n",
    "            bef_error = compute_landmark_error(fixed_points_S[i],fixed_image_size_S[i],moving_points_S[i],moving_image_size_S[i],max_image_size_S[i])\n",
    "            aft_error = compute_landmark_error_fixed_space(polynomial_matrix,fixed_points_S[i],transformed_points_high_res_hom,max_image_size_S[i],fixed_image_size_S[i])\n",
    "            print(\"Mean Landmark Error for Case {0} Before Registration is {1} pixels\".format(i,bef_error))\n",
    "            print(\"Mean Landmark Error for Case {0} After Registration is {1} pixels\".format(i,aft_error))\n",
    "            landmark_errors3.append(aft_error)\n",
    "        else:\n",
    "            landmark_errors3.append(10000)\n",
    "    else:\n",
    "        landmark_errors3.append(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa577a-ff97-497e-8d05-a5521ee64d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_landmark_errors(landmark_errors3,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results','Final_Registration_Results','S'),'S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053d908-2fca-4ca0-93dc-e7c6c7540958",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_plot_FIRE_AUC(landmark_errors3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898cd9f-d023-4ecc-a802-06572aa5bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_errors = landmark_errors1 + landmark_errors2 + landmark_errors3\n",
    "plot_landmark_errors(landmark_errors,os.path.join(os.getcwd(),'FIRE_Image_Registration_Results'),'All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126442e3-f874-4a0a-8998-f5df47f59469",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_plot_FIRE_AUC(landmark_errors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VBS_HRC",
   "language": "python",
   "name": "vbs_hrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
